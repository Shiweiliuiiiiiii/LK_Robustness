Not using distributed mode
Namespace(Decom=False, aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=64, bn=True, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/sliu/project_space/datasets/sketch', data_set='IMNET', data_type='imagenet-s', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.1, enable_wandb=False, epochs=300, eval=True, eval_data_path=None, finetune='', fix=False, growth='random', head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, kernel_size=[7, 7, 7, 7, 100], layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=-1, log_dir=None, lr=0.004, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='swin', model_ema=False, model_ema_decay=0.9999, model_ema_eval=False, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_workers=10, only_L=False, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', parallel=True, pin_mem=True, project='SLaK', prune='magnitude', prune_rate=0.3, recount=1, redistribution='none', remode='pixel', reprob=0.25, resplit=False, resume='/gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_VitTeacher_STRN50_NKD/checkpoint-best.pth', save_ckpt=True, save_ckpt_freq=10, save_ckpt_num=3, seed=0, smoothing=0.1, sparse=False, sparse_init='snip', sparsity=0.4, start_epoch=0, train_interpolation='bicubic', update_freq=1, update_frequency=100, use_amp=False, verbose=False, wandb_ckpt=False, warmup_epochs=20, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, width_factor=1.0, world_size=1)
Transform = 
Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /home/sliu/project_space/datasets/sketch
Number of the class = 1000
Mixup is activated!
pretrained url https://github.com/SwinTransformer/storage/releases/download/v1.0.0/swin_tiny_patch4_window7_224.pth
Model = SwinTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
    (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (layers): ModuleList(
    (0): BasicLayer(
      dim=96, input_resolution=(56, 56), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): Identity()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          (norm1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=96, out_features=288, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=96, out_features=96, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((96,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=96, out_features=384, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=384, out_features=96, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(56, 56), dim=96
        (reduction): Linear(in_features=384, out_features=192, bias=False)
        (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
      )
    )
    (1): BasicLayer(
      dim=192, input_resolution=(28, 28), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=192, out_features=576, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=192, out_features=192, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=192, out_features=768, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=768, out_features=192, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(28, 28), dim=192
        (reduction): Linear(in_features=768, out_features=384, bias=False)
        (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      )
    )
    (2): BasicLayer(
      dim=384, input_resolution=(14, 14), depth=6
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (2): SwinTransformerBlock(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (3): SwinTransformerBlock(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (4): SwinTransformerBlock(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (5): SwinTransformerBlock(
          (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=384, out_features=1152, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=384, out_features=384, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=384, out_features=1536, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=1536, out_features=384, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
      (downsample): PatchMerging(
        input_resolution=(14, 14), dim=384
        (reduction): Linear(in_features=1536, out_features=768, bias=False)
        (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)
      )
    )
    (3): BasicLayer(
      dim=768, input_resolution=(7, 7), depth=2
      (blocks): ModuleList(
        (0): SwinTransformerBlock(
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
        (1): SwinTransformerBlock(
          (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (attn): WindowAttention(
            (qkv): Linear(in_features=768, out_features=2304, bias=True)
            (attn_drop): Dropout(p=0.0, inplace=False)
            (proj): Linear(in_features=768, out_features=768, bias=True)
            (proj_drop): Dropout(p=0.0, inplace=False)
            (softmax): Softmax(dim=-1)
          )
          (drop_path): DropPath()
          (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
          (mlp): Mlp(
            (fc1): Linear(in_features=768, out_features=3072, bias=True)
            (act): GELU()
            (fc2): Linear(in_features=3072, out_features=768, bias=True)
            (drop): Dropout(p=0.0, inplace=False)
          )
        )
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (avgpool): AdaptiveAvgPool1d(output_size=1)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
number of params: 28288354
LR = 0.00400000
Batch size = 64
Update frequent = 1
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight",
      "layers.0.blocks.0.attn.relative_position_bias_table",
      "layers.0.blocks.0.attn.qkv.weight",
      "layers.0.blocks.0.attn.proj.weight",
      "layers.0.blocks.0.mlp.fc1.weight",
      "layers.0.blocks.0.mlp.fc2.weight",
      "layers.0.blocks.1.attn.relative_position_bias_table",
      "layers.0.blocks.1.attn.qkv.weight",
      "layers.0.blocks.1.attn.proj.weight",
      "layers.0.blocks.1.mlp.fc1.weight",
      "layers.0.blocks.1.mlp.fc2.weight",
      "layers.0.downsample.reduction.weight",
      "layers.1.blocks.0.attn.relative_position_bias_table",
      "layers.1.blocks.0.attn.qkv.weight",
      "layers.1.blocks.0.attn.proj.weight",
      "layers.1.blocks.0.mlp.fc1.weight",
      "layers.1.blocks.0.mlp.fc2.weight",
      "layers.1.blocks.1.attn.relative_position_bias_table",
      "layers.1.blocks.1.attn.qkv.weight",
      "layers.1.blocks.1.attn.proj.weight",
      "layers.1.blocks.1.mlp.fc1.weight",
      "layers.1.blocks.1.mlp.fc2.weight",
      "layers.1.downsample.reduction.weight",
      "layers.2.blocks.0.attn.relative_position_bias_table",
      "layers.2.blocks.0.attn.qkv.weight",
      "layers.2.blocks.0.attn.proj.weight",
      "layers.2.blocks.0.mlp.fc1.weight",
      "layers.2.blocks.0.mlp.fc2.weight",
      "layers.2.blocks.1.attn.relative_position_bias_table",
      "layers.2.blocks.1.attn.qkv.weight",
      "layers.2.blocks.1.attn.proj.weight",
      "layers.2.blocks.1.mlp.fc1.weight",
      "layers.2.blocks.1.mlp.fc2.weight",
      "layers.2.blocks.2.attn.relative_position_bias_table",
      "layers.2.blocks.2.attn.qkv.weight",
      "layers.2.blocks.2.attn.proj.weight",
      "layers.2.blocks.2.mlp.fc1.weight",
      "layers.2.blocks.2.mlp.fc2.weight",
      "layers.2.blocks.3.attn.relative_position_bias_table",
      "layers.2.blocks.3.attn.qkv.weight",
      "layers.2.blocks.3.attn.proj.weight",
      "layers.2.blocks.3.mlp.fc1.weight",
      "layers.2.blocks.3.mlp.fc2.weight",
      "layers.2.blocks.4.attn.relative_position_bias_table",
      "layers.2.blocks.4.attn.qkv.weight",
      "layers.2.blocks.4.attn.proj.weight",
      "layers.2.blocks.4.mlp.fc1.weight",
      "layers.2.blocks.4.mlp.fc2.weight",
      "layers.2.blocks.5.attn.relative_position_bias_table",
      "layers.2.blocks.5.attn.qkv.weight",
      "layers.2.blocks.5.attn.proj.weight",
      "layers.2.blocks.5.mlp.fc1.weight",
      "layers.2.blocks.5.mlp.fc2.weight",
      "layers.2.downsample.reduction.weight",
      "layers.3.blocks.0.attn.relative_position_bias_table",
      "layers.3.blocks.0.attn.qkv.weight",
      "layers.3.blocks.0.attn.proj.weight",
      "layers.3.blocks.0.mlp.fc1.weight",
      "layers.3.blocks.0.mlp.fc2.weight",
      "layers.3.blocks.1.attn.relative_position_bias_table",
      "layers.3.blocks.1.attn.qkv.weight",
      "layers.3.blocks.1.attn.proj.weight",
      "layers.3.blocks.1.mlp.fc1.weight",
      "layers.3.blocks.1.mlp.fc2.weight",
      "head.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "patch_embed.proj.bias",
      "patch_embed.norm.weight",
      "patch_embed.norm.bias",
      "layers.0.blocks.0.norm1.weight",
      "layers.0.blocks.0.norm1.bias",
      "layers.0.blocks.0.attn.qkv.bias",
      "layers.0.blocks.0.attn.proj.bias",
      "layers.0.blocks.0.norm2.weight",
      "layers.0.blocks.0.norm2.bias",
      "layers.0.blocks.0.mlp.fc1.bias",
      "layers.0.blocks.0.mlp.fc2.bias",
      "layers.0.blocks.1.norm1.weight",
      "layers.0.blocks.1.norm1.bias",
      "layers.0.blocks.1.attn.qkv.bias",
      "layers.0.blocks.1.attn.proj.bias",
      "layers.0.blocks.1.norm2.weight",
      "layers.0.blocks.1.norm2.bias",
      "layers.0.blocks.1.mlp.fc1.bias",
      "layers.0.blocks.1.mlp.fc2.bias",
      "layers.0.downsample.norm.weight",
      "layers.0.downsample.norm.bias",
      "layers.1.blocks.0.norm1.weight",
      "layers.1.blocks.0.norm1.bias",
      "layers.1.blocks.0.attn.qkv.bias",
      "layers.1.blocks.0.attn.proj.bias",
      "layers.1.blocks.0.norm2.weight",
      "layers.1.blocks.0.norm2.bias",
      "layers.1.blocks.0.mlp.fc1.bias",
      "layers.1.blocks.0.mlp.fc2.bias",
      "layers.1.blocks.1.norm1.weight",
      "layers.1.blocks.1.norm1.bias",
      "layers.1.blocks.1.attn.qkv.bias",
      "layers.1.blocks.1.attn.proj.bias",
      "layers.1.blocks.1.norm2.weight",
      "layers.1.blocks.1.norm2.bias",
      "layers.1.blocks.1.mlp.fc1.bias",
      "layers.1.blocks.1.mlp.fc2.bias",
      "layers.1.downsample.norm.weight",
      "layers.1.downsample.norm.bias",
      "layers.2.blocks.0.norm1.weight",
      "layers.2.blocks.0.norm1.bias",
      "layers.2.blocks.0.attn.qkv.bias",
      "layers.2.blocks.0.attn.proj.bias",
      "layers.2.blocks.0.norm2.weight",
      "layers.2.blocks.0.norm2.bias",
      "layers.2.blocks.0.mlp.fc1.bias",
      "layers.2.blocks.0.mlp.fc2.bias",
      "layers.2.blocks.1.norm1.weight",
      "layers.2.blocks.1.norm1.bias",
      "layers.2.blocks.1.attn.qkv.bias",
      "layers.2.blocks.1.attn.proj.bias",
      "layers.2.blocks.1.norm2.weight",
      "layers.2.blocks.1.norm2.bias",
      "layers.2.blocks.1.mlp.fc1.bias",
      "layers.2.blocks.1.mlp.fc2.bias",
      "layers.2.blocks.2.norm1.weight",
      "layers.2.blocks.2.norm1.bias",
      "layers.2.blocks.2.attn.qkv.bias",
      "layers.2.blocks.2.attn.proj.bias",
      "layers.2.blocks.2.norm2.weight",
      "layers.2.blocks.2.norm2.bias",
      "layers.2.blocks.2.mlp.fc1.bias",
      "layers.2.blocks.2.mlp.fc2.bias",
      "layers.2.blocks.3.norm1.weight",
      "layers.2.blocks.3.norm1.bias",
      "layers.2.blocks.3.attn.qkv.bias",
      "layers.2.blocks.3.attn.proj.bias",
      "layers.2.blocks.3.norm2.weight",
      "layers.2.blocks.3.norm2.bias",
      "layers.2.blocks.3.mlp.fc1.bias",
      "layers.2.blocks.3.mlp.fc2.bias",
      "layers.2.blocks.4.norm1.weight",
      "layers.2.blocks.4.norm1.bias",
      "layers.2.blocks.4.attn.qkv.bias",
      "layers.2.blocks.4.attn.proj.bias",
      "layers.2.blocks.4.norm2.weight",
      "layers.2.blocks.4.norm2.bias",
      "layers.2.blocks.4.mlp.fc1.bias",
      "layers.2.blocks.4.mlp.fc2.bias",
      "layers.2.blocks.5.norm1.weight",
      "layers.2.blocks.5.norm1.bias",
      "layers.2.blocks.5.attn.qkv.bias",
      "layers.2.blocks.5.attn.proj.bias",
      "layers.2.blocks.5.norm2.weight",
      "layers.2.blocks.5.norm2.bias",
      "layers.2.blocks.5.mlp.fc1.bias",
      "layers.2.blocks.5.mlp.fc2.bias",
      "layers.2.downsample.norm.weight",
      "layers.2.downsample.norm.bias",
      "layers.3.blocks.0.norm1.weight",
      "layers.3.blocks.0.norm1.bias",
      "layers.3.blocks.0.attn.qkv.bias",
      "layers.3.blocks.0.attn.proj.bias",
      "layers.3.blocks.0.norm2.weight",
      "layers.3.blocks.0.norm2.bias",
      "layers.3.blocks.0.mlp.fc1.bias",
      "layers.3.blocks.0.mlp.fc2.bias",
      "layers.3.blocks.1.norm1.weight",
      "layers.3.blocks.1.norm1.bias",
      "layers.3.blocks.1.attn.qkv.bias",
      "layers.3.blocks.1.attn.proj.bias",
      "layers.3.blocks.1.norm2.weight",
      "layers.3.blocks.1.norm2.bias",
      "layers.3.blocks.1.mlp.fc1.bias",
      "layers.3.blocks.1.mlp.fc2.bias",
      "norm.weight",
      "norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
criterion = SoftTargetCrossEntropy()
patch_embed.proj.weight density is 1.0
patch_embed.proj.bias density is 1.0
patch_embed.norm.weight density is 1.0
patch_embed.norm.bias density is 1.0
layers.0.blocks.0.norm1.weight density is 1.0
layers.0.blocks.0.norm1.bias density is 1.0
layers.0.blocks.0.attn.relative_position_bias_table density is 1.0
layers.0.blocks.0.attn.qkv.weight density is 1.0
layers.0.blocks.0.attn.qkv.bias density is 1.0
layers.0.blocks.0.attn.proj.weight density is 1.0
layers.0.blocks.0.attn.proj.bias density is 1.0
layers.0.blocks.0.norm2.weight density is 1.0
layers.0.blocks.0.norm2.bias density is 1.0
layers.0.blocks.0.mlp.fc1.weight density is 1.0
layers.0.blocks.0.mlp.fc1.bias density is 1.0
layers.0.blocks.0.mlp.fc2.weight density is 1.0
layers.0.blocks.0.mlp.fc2.bias density is 1.0
layers.0.blocks.1.norm1.weight density is 1.0
layers.0.blocks.1.norm1.bias density is 1.0
layers.0.blocks.1.attn.relative_position_bias_table density is 1.0
layers.0.blocks.1.attn.qkv.weight density is 1.0
layers.0.blocks.1.attn.qkv.bias density is 1.0
layers.0.blocks.1.attn.proj.weight density is 1.0
layers.0.blocks.1.attn.proj.bias density is 1.0
layers.0.blocks.1.norm2.weight density is 1.0
layers.0.blocks.1.norm2.bias density is 1.0
layers.0.blocks.1.mlp.fc1.weight density is 1.0
layers.0.blocks.1.mlp.fc1.bias density is 1.0
layers.0.blocks.1.mlp.fc2.weight density is 1.0
layers.0.blocks.1.mlp.fc2.bias density is 1.0
layers.0.downsample.reduction.weight density is 1.0
layers.0.downsample.norm.weight density is 1.0
layers.0.downsample.norm.bias density is 1.0
layers.1.blocks.0.norm1.weight density is 1.0
layers.1.blocks.0.norm1.bias density is 1.0
layers.1.blocks.0.attn.relative_position_bias_table density is 1.0
layers.1.blocks.0.attn.qkv.weight density is 1.0
layers.1.blocks.0.attn.qkv.bias density is 1.0
layers.1.blocks.0.attn.proj.weight density is 1.0
layers.1.blocks.0.attn.proj.bias density is 1.0
layers.1.blocks.0.norm2.weight density is 1.0
layers.1.blocks.0.norm2.bias density is 1.0
layers.1.blocks.0.mlp.fc1.weight density is 1.0
layers.1.blocks.0.mlp.fc1.bias density is 1.0
layers.1.blocks.0.mlp.fc2.weight density is 1.0
layers.1.blocks.0.mlp.fc2.bias density is 1.0
layers.1.blocks.1.norm1.weight density is 1.0
layers.1.blocks.1.norm1.bias density is 1.0
layers.1.blocks.1.attn.relative_position_bias_table density is 1.0
layers.1.blocks.1.attn.qkv.weight density is 1.0
layers.1.blocks.1.attn.qkv.bias density is 1.0
layers.1.blocks.1.attn.proj.weight density is 1.0
layers.1.blocks.1.attn.proj.bias density is 1.0
layers.1.blocks.1.norm2.weight density is 1.0
layers.1.blocks.1.norm2.bias density is 1.0
layers.1.blocks.1.mlp.fc1.weight density is 1.0
layers.1.blocks.1.mlp.fc1.bias density is 1.0
layers.1.blocks.1.mlp.fc2.weight density is 1.0
layers.1.blocks.1.mlp.fc2.bias density is 1.0
layers.1.downsample.reduction.weight density is 1.0
layers.1.downsample.norm.weight density is 1.0
layers.1.downsample.norm.bias density is 1.0
layers.2.blocks.0.norm1.weight density is 1.0
layers.2.blocks.0.norm1.bias density is 1.0
layers.2.blocks.0.attn.relative_position_bias_table density is 1.0
layers.2.blocks.0.attn.qkv.weight density is 1.0
layers.2.blocks.0.attn.qkv.bias density is 1.0
layers.2.blocks.0.attn.proj.weight density is 1.0
layers.2.blocks.0.attn.proj.bias density is 1.0
layers.2.blocks.0.norm2.weight density is 1.0
layers.2.blocks.0.norm2.bias density is 1.0
layers.2.blocks.0.mlp.fc1.weight density is 1.0
layers.2.blocks.0.mlp.fc1.bias density is 1.0
layers.2.blocks.0.mlp.fc2.weight density is 1.0
layers.2.blocks.0.mlp.fc2.bias density is 1.0
layers.2.blocks.1.norm1.weight density is 1.0
layers.2.blocks.1.norm1.bias density is 1.0
layers.2.blocks.1.attn.relative_position_bias_table density is 1.0
layers.2.blocks.1.attn.qkv.weight density is 1.0
layers.2.blocks.1.attn.qkv.bias density is 1.0
layers.2.blocks.1.attn.proj.weight density is 1.0
layers.2.blocks.1.attn.proj.bias density is 1.0
layers.2.blocks.1.norm2.weight density is 1.0
layers.2.blocks.1.norm2.bias density is 1.0
layers.2.blocks.1.mlp.fc1.weight density is 1.0
layers.2.blocks.1.mlp.fc1.bias density is 1.0
layers.2.blocks.1.mlp.fc2.weight density is 1.0
layers.2.blocks.1.mlp.fc2.bias density is 1.0
layers.2.blocks.2.norm1.weight density is 1.0
layers.2.blocks.2.norm1.bias density is 1.0
layers.2.blocks.2.attn.relative_position_bias_table density is 1.0
layers.2.blocks.2.attn.qkv.weight density is 1.0
layers.2.blocks.2.attn.qkv.bias density is 1.0
layers.2.blocks.2.attn.proj.weight density is 1.0
layers.2.blocks.2.attn.proj.bias density is 1.0
layers.2.blocks.2.norm2.weight density is 1.0
layers.2.blocks.2.norm2.bias density is 1.0
layers.2.blocks.2.mlp.fc1.weight density is 1.0
layers.2.blocks.2.mlp.fc1.bias density is 1.0
layers.2.blocks.2.mlp.fc2.weight density is 1.0
layers.2.blocks.2.mlp.fc2.bias density is 1.0
layers.2.blocks.3.norm1.weight density is 1.0
layers.2.blocks.3.norm1.bias density is 1.0
layers.2.blocks.3.attn.relative_position_bias_table density is 1.0
layers.2.blocks.3.attn.qkv.weight density is 1.0
layers.2.blocks.3.attn.qkv.bias density is 1.0
layers.2.blocks.3.attn.proj.weight density is 1.0
layers.2.blocks.3.attn.proj.bias density is 1.0
layers.2.blocks.3.norm2.weight density is 1.0
layers.2.blocks.3.norm2.bias density is 1.0
layers.2.blocks.3.mlp.fc1.weight density is 1.0
layers.2.blocks.3.mlp.fc1.bias density is 1.0
layers.2.blocks.3.mlp.fc2.weight density is 1.0
layers.2.blocks.3.mlp.fc2.bias density is 1.0
layers.2.blocks.4.norm1.weight density is 1.0
layers.2.blocks.4.norm1.bias density is 1.0
layers.2.blocks.4.attn.relative_position_bias_table density is 1.0
layers.2.blocks.4.attn.qkv.weight density is 1.0
layers.2.blocks.4.attn.qkv.bias density is 1.0
layers.2.blocks.4.attn.proj.weight density is 1.0
layers.2.blocks.4.attn.proj.bias density is 1.0
layers.2.blocks.4.norm2.weight density is 1.0
layers.2.blocks.4.norm2.bias density is 1.0
layers.2.blocks.4.mlp.fc1.weight density is 1.0
layers.2.blocks.4.mlp.fc1.bias density is 1.0
layers.2.blocks.4.mlp.fc2.weight density is 1.0
layers.2.blocks.4.mlp.fc2.bias density is 1.0
layers.2.blocks.5.norm1.weight density is 1.0
layers.2.blocks.5.norm1.bias density is 1.0
layers.2.blocks.5.attn.relative_position_bias_table density is 1.0
layers.2.blocks.5.attn.qkv.weight density is 1.0
layers.2.blocks.5.attn.qkv.bias density is 1.0
layers.2.blocks.5.attn.proj.weight density is 1.0
layers.2.blocks.5.attn.proj.bias density is 1.0
layers.2.blocks.5.norm2.weight density is 1.0
layers.2.blocks.5.norm2.bias density is 1.0
layers.2.blocks.5.mlp.fc1.weight density is 1.0
layers.2.blocks.5.mlp.fc1.bias density is 1.0
layers.2.blocks.5.mlp.fc2.weight density is 1.0
layers.2.blocks.5.mlp.fc2.bias density is 1.0
layers.2.downsample.reduction.weight density is 1.0
layers.2.downsample.norm.weight density is 1.0
layers.2.downsample.norm.bias density is 1.0
layers.3.blocks.0.norm1.weight density is 1.0
layers.3.blocks.0.norm1.bias density is 1.0
layers.3.blocks.0.attn.relative_position_bias_table density is 1.0
layers.3.blocks.0.attn.qkv.weight density is 1.0
layers.3.blocks.0.attn.qkv.bias density is 1.0
layers.3.blocks.0.attn.proj.weight density is 1.0
layers.3.blocks.0.attn.proj.bias density is 1.0
layers.3.blocks.0.norm2.weight density is 1.0
layers.3.blocks.0.norm2.bias density is 1.0
layers.3.blocks.0.mlp.fc1.weight density is 1.0
layers.3.blocks.0.mlp.fc1.bias density is 1.0
layers.3.blocks.0.mlp.fc2.weight density is 1.0
layers.3.blocks.0.mlp.fc2.bias density is 1.0
layers.3.blocks.1.norm1.weight density is 1.0
layers.3.blocks.1.norm1.bias density is 1.0
layers.3.blocks.1.attn.relative_position_bias_table density is 1.0
layers.3.blocks.1.attn.qkv.weight density is 1.0
layers.3.blocks.1.attn.qkv.bias density is 1.0
layers.3.blocks.1.attn.proj.weight density is 1.0
layers.3.blocks.1.attn.proj.bias density is 1.0
layers.3.blocks.1.norm2.weight density is 1.0
layers.3.blocks.1.norm2.bias density is 1.0
layers.3.blocks.1.mlp.fc1.weight density is 1.0
layers.3.blocks.1.mlp.fc1.bias density is 1.0
layers.3.blocks.1.mlp.fc2.weight density is 1.0
layers.3.blocks.1.mlp.fc2.bias density is 1.0
norm.weight density is 1.0
norm.bias density is 1.0
head.weight density is 1.0
head.bias density is 1.0
/home/sliu/miniconda3/envs/slak/lib/python3.8/site-packages/torch/functional.py:445: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at  /opt/conda/conda-bld/pytorch_1634272068694/work/aten/src/ATen/native/TensorShape.cpp:2157.)
  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]
Test:  [  0/531]  eta: 0:59:55  loss: 4.5574 (4.5574)  acc1: 28.1250 (28.1250)  acc5: 42.7083 (42.7083)  time: 6.7718  data: 3.4467  max mem: 1821
Test:  [ 10/531]  eta: 0:06:05  loss: 4.5574 (4.2805)  acc1: 25.0000 (29.6402)  acc5: 42.7083 (46.3068)  time: 0.7021  data: 0.3135  max mem: 1945
Test:  [ 20/531]  eta: 0:03:27  loss: 4.2360 (4.3280)  acc1: 25.0000 (27.1329)  acc5: 43.7500 (46.8750)  time: 0.0879  data: 0.0002  max mem: 1945
Test:  [ 30/531]  eta: 0:03:00  loss: 4.2871 (4.7993)  acc1: 22.9167 (23.7567)  acc5: 40.6250 (41.7675)  time: 0.1725  data: 0.0536  max mem: 1945
Test:  [ 40/531]  eta: 0:02:49  loss: 4.6302 (4.7460)  acc1: 18.7500 (24.2632)  acc5: 37.5000 (42.6067)  time: 0.2814  data: 0.1240  max mem: 1945
Test:  [ 50/531]  eta: 0:02:40  loss: 4.4035 (4.6481)  acc1: 23.9583 (25.4085)  acc5: 46.8750 (44.3423)  time: 0.2931  data: 0.1309  max mem: 1945
Test:  [ 60/531]  eta: 0:02:29  loss: 4.5079 (4.6399)  acc1: 26.0417 (25.8880)  acc5: 40.6250 (43.9891)  time: 0.2599  data: 0.0877  max mem: 1945
Test:  [ 70/531]  eta: 0:02:30  loss: 4.4169 (4.5461)  acc1: 29.1667 (26.5698)  acc5: 43.7500 (45.0998)  time: 0.3061  data: 0.1310  max mem: 1945
Test:  [ 80/531]  eta: 0:02:23  loss: 4.2278 (4.5302)  acc1: 27.0833 (26.7361)  acc5: 43.7500 (44.9974)  time: 0.3216  data: 0.1550  max mem: 1945
Test:  [ 90/531]  eta: 0:02:17  loss: 3.7222 (4.4038)  acc1: 28.1250 (27.7930)  acc5: 52.0833 (46.7262)  time: 0.2631  data: 0.0692  max mem: 1945
Test:  [100/531]  eta: 0:02:12  loss: 3.1530 (4.2983)  acc1: 37.5000 (28.7129)  acc5: 64.5833 (48.2467)  time: 0.2602  data: 0.0200  max mem: 1945
Test:  [110/531]  eta: 0:02:08  loss: 3.1166 (4.1965)  acc1: 37.5000 (29.3637)  acc5: 65.6250 (49.8217)  time: 0.2665  data: 0.0024  max mem: 1945
Test:  [120/531]  eta: 0:02:05  loss: 3.1395 (4.1261)  acc1: 31.2500 (29.7693)  acc5: 66.6667 (51.0847)  time: 0.3037  data: 0.0004  max mem: 1945
Test:  [130/531]  eta: 0:02:03  loss: 3.4178 (4.0894)  acc1: 33.3333 (30.1288)  acc5: 65.6250 (51.6062)  time: 0.3294  data: 0.0002  max mem: 1945
Test:  [140/531]  eta: 0:01:58  loss: 2.6743 (4.0150)  acc1: 36.4583 (30.9914)  acc5: 62.5000 (52.7039)  time: 0.2897  data: 0.0005  max mem: 1945
Test:  [150/531]  eta: 0:01:54  loss: 3.3721 (4.0126)  acc1: 29.1667 (31.0293)  acc5: 61.4583 (52.8284)  time: 0.2524  data: 0.0005  max mem: 1945
Test:  [160/531]  eta: 0:01:50  loss: 3.6014 (3.9924)  acc1: 31.2500 (31.4118)  acc5: 55.2083 (53.0021)  time: 0.2513  data: 0.0001  max mem: 1945
Test:  [170/531]  eta: 0:01:45  loss: 3.6736 (3.9971)  acc1: 34.3750 (31.6094)  acc5: 53.1250 (53.0702)  time: 0.2183  data: 0.0008  max mem: 1945
Test:  [180/531]  eta: 0:01:41  loss: 3.9579 (3.9926)  acc1: 32.2917 (31.8888)  acc5: 51.0417 (53.0157)  time: 0.2171  data: 0.0121  max mem: 1945
Test:  [190/531]  eta: 0:01:37  loss: 3.9052 (3.9985)  acc1: 30.2083 (31.8990)  acc5: 51.0417 (52.9832)  time: 0.2531  data: 0.0369  max mem: 1945
Test:  [200/531]  eta: 0:01:35  loss: 3.7081 (3.9957)  acc1: 30.2083 (31.9082)  acc5: 52.0833 (52.8244)  time: 0.2829  data: 0.0542  max mem: 1945
Test:  [210/531]  eta: 0:01:34  loss: 4.6542 (4.0453)  acc1: 15.6250 (31.3784)  acc5: 40.6250 (52.0735)  time: 0.3655  data: 0.1184  max mem: 1945
Test:  [220/531]  eta: 0:01:31  loss: 4.6542 (4.0571)  acc1: 29.1667 (31.4810)  acc5: 40.6250 (51.8759)  time: 0.3455  data: 0.0969  max mem: 1945
Test:  [230/531]  eta: 0:01:27  loss: 4.0136 (4.0672)  acc1: 30.2083 (31.3943)  acc5: 50.0000 (51.6955)  time: 0.2626  data: 0.0075  max mem: 1945
Test:  [240/531]  eta: 0:01:24  loss: 3.9818 (4.0741)  acc1: 30.2083 (31.3321)  acc5: 52.0833 (51.5085)  time: 0.2581  data: 0.0005  max mem: 1945
Test:  [250/531]  eta: 0:01:20  loss: 4.3380 (4.1063)  acc1: 23.9583 (30.9346)  acc5: 41.6667 (50.9587)  time: 0.2449  data: 0.0006  max mem: 1945
Test:  [260/531]  eta: 0:01:16  loss: 4.6348 (4.1174)  acc1: 22.9167 (30.6753)  acc5: 40.6250 (50.7344)  time: 0.1950  data: 0.0010  max mem: 1945
Test:  [270/531]  eta: 0:01:13  loss: 4.0428 (4.1103)  acc1: 28.1250 (30.7887)  acc5: 51.0417 (50.8610)  time: 0.1898  data: 0.0351  max mem: 1945
Test:  [280/531]  eta: 0:01:11  loss: 3.7165 (4.0924)  acc1: 35.4167 (31.1017)  acc5: 55.2083 (51.0157)  time: 0.2942  data: 0.1281  max mem: 1945
Test:  [290/531]  eta: 0:01:08  loss: 3.7165 (4.0955)  acc1: 33.3333 (31.0495)  acc5: 51.0417 (50.9486)  time: 0.3031  data: 0.0966  max mem: 1945
Test:  [300/531]  eta: 0:01:05  loss: 3.6539 (4.0840)  acc1: 33.3333 (31.2258)  acc5: 51.0417 (51.0451)  time: 0.2727  data: 0.0464  max mem: 1945
Test:  [310/531]  eta: 0:01:02  loss: 3.7292 (4.0790)  acc1: 36.4583 (31.4007)  acc5: 51.0417 (51.1422)  time: 0.3128  data: 0.0774  max mem: 1945
Test:  [320/531]  eta: 0:01:00  loss: 3.7292 (4.0699)  acc1: 36.4583 (31.6199)  acc5: 51.0417 (51.1747)  time: 0.3110  data: 0.0568  max mem: 1945
Test:  [330/531]  eta: 0:00:57  loss: 3.3844 (4.0581)  acc1: 33.3333 (31.7032)  acc5: 54.1667 (51.3249)  time: 0.2806  data: 0.0233  max mem: 1945
Test:  [340/531]  eta: 0:00:54  loss: 4.0589 (4.0748)  acc1: 26.0417 (31.3875)  acc5: 47.9167 (51.0478)  time: 0.2566  data: 0.0008  max mem: 1945
Test:  [350/531]  eta: 0:00:50  loss: 4.6614 (4.0874)  acc1: 17.7083 (31.2322)  acc5: 42.7083 (50.8161)  time: 0.2275  data: 0.0014  max mem: 1945
Test:  [360/531]  eta: 0:00:47  loss: 4.7313 (4.1041)  acc1: 20.8333 (30.9441)  acc5: 39.5833 (50.5598)  time: 0.1823  data: 0.0021  max mem: 1945
Test:  [370/531]  eta: 0:00:44  loss: 4.7118 (4.1144)  acc1: 19.7917 (30.8288)  acc5: 41.6667 (50.3397)  time: 0.2314  data: 0.0366  max mem: 1945
Test:  [380/531]  eta: 0:00:42  loss: 4.5400 (4.1174)  acc1: 20.8333 (30.7825)  acc5: 45.8333 (50.3336)  time: 0.2911  data: 0.0586  max mem: 1945
Test:  [390/531]  eta: 0:00:39  loss: 4.7353 (4.1441)  acc1: 21.8750 (30.5573)  acc5: 44.7917 (49.8828)  time: 0.2890  data: 0.0582  max mem: 1945
Test:  [400/531]  eta: 0:00:36  loss: 4.7337 (4.1560)  acc1: 20.8333 (30.4759)  acc5: 36.4583 (49.6753)  time: 0.3174  data: 0.0944  max mem: 1945
Test:  [410/531]  eta: 0:00:33  loss: 4.2480 (4.1582)  acc1: 31.2500 (30.5885)  acc5: 44.7917 (49.6477)  time: 0.2929  data: 0.0609  max mem: 1945
Test:  [420/531]  eta: 0:00:30  loss: 4.3613 (4.1661)  acc1: 30.2083 (30.5028)  acc5: 44.7917 (49.5670)  time: 0.2513  data: 0.0100  max mem: 1945
Test:  [430/531]  eta: 0:00:28  loss: 4.3613 (4.1766)  acc1: 23.9583 (30.4162)  acc5: 43.7500 (49.3885)  time: 0.2448  data: 0.0079  max mem: 1945
Test:  [440/531]  eta: 0:00:25  loss: 4.0367 (4.1693)  acc1: 30.2083 (30.5036)  acc5: 46.8750 (49.4449)  time: 0.2657  data: 0.0394  max mem: 1945
Test:  [450/531]  eta: 0:00:22  loss: 3.8743 (4.1666)  acc1: 36.4583 (30.5063)  acc5: 53.1250 (49.4849)  time: 0.3903  data: 0.1690  max mem: 1945
Test:  [460/531]  eta: 0:00:20  loss: 3.9684 (4.1680)  acc1: 33.3333 (30.5089)  acc5: 45.8333 (49.3922)  time: 0.3795  data: 0.1502  max mem: 1945
Test:  [470/531]  eta: 0:00:17  loss: 3.9684 (4.1591)  acc1: 34.3750 (30.7259)  acc5: 48.9583 (49.4648)  time: 0.2787  data: 0.0206  max mem: 1945
Test:  [480/531]  eta: 0:00:14  loss: 3.7837 (4.1602)  acc1: 33.3333 (30.6393)  acc5: 48.9583 (49.4803)  time: 0.2911  data: 0.0001  max mem: 1945
Test:  [490/531]  eta: 0:00:11  loss: 4.3303 (4.1766)  acc1: 22.9167 (30.4863)  acc5: 40.6250 (49.2511)  time: 0.3041  data: 0.0001  max mem: 1945
Test:  [500/531]  eta: 0:00:08  loss: 5.5516 (4.2245)  acc1: 10.4167 (30.0961)  acc5: 23.9583 (48.6174)  time: 0.3127  data: 0.0001  max mem: 1945
Test:  [510/531]  eta: 0:00:05  loss: 6.4649 (4.2642)  acc1: 8.3333 (29.7395)  acc5: 16.6667 (48.0696)  time: 0.3203  data: 0.0030  max mem: 1945
Test:  [520/531]  eta: 0:00:03  loss: 5.9803 (4.2971)  acc1: 9.3750 (29.4246)  acc5: 20.8333 (47.6528)  time: 0.3191  data: 0.0030  max mem: 1945
Test:  [530/531]  eta: 0:00:00  loss: 5.7255 (4.3292)  acc1: 7.2917 (29.0475)  acc5: 22.9167 (47.2008)  time: 0.2930  data: 0.0001  max mem: 1945
Test: Total time: 0:02:31 (0.2857 s / it)
* Acc@1 29.048 Acc@5 47.201 loss 4.329
Accuracy of the network on 50889 test images: 29.04754%
Not using distributed mode
Namespace(Decom=False, aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=64, bn=True, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/sliu/project_space/datasets/sketch', data_set='IMNET', data_type='imagenet-s', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.1, enable_wandb=False, epochs=300, eval=True, eval_data_path=None, finetune='', fix=False, growth='random', head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, kernel_size=[7, 7, 7, 7, 100], layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=-1, log_dir=None, lr=0.004, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='vit', model_ema=False, model_ema_decay=0.9999, model_ema_eval=False, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_workers=10, only_L=False, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', parallel=True, pin_mem=True, project='SLaK', prune='magnitude', prune_rate=0.3, recount=1, redistribution='none', remode='pixel', reprob=0.25, resplit=False, resume='/gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_VitTeacher_STRN50_NKD/checkpoint-best.pth', save_ckpt=True, save_ckpt_freq=10, save_ckpt_num=3, seed=0, smoothing=0.1, sparse=False, sparse_init='snip', sparsity=0.4, start_epoch=0, train_interpolation='bicubic', update_freq=1, update_frequency=100, use_amp=False, verbose=False, wandb_ckpt=False, warmup_epochs=20, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, width_factor=1.0, world_size=1)
Transform = 
Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /home/sliu/project_space/datasets/sketch
Number of the class = 1000
Mixup is activated!
pretrained url https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/vit_small_p16_224-15ec54c9.pth
Model = VisionTransformer(
  (patch_embed): PatchEmbed(
    (proj): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))
    (norm): Identity()
  )
  (pos_drop): Dropout(p=0.0, inplace=False)
  (blocks): Sequential(
    (0): Block(
      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=2304, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=2304, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (1): Block(
      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=2304, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=2304, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (2): Block(
      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=2304, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=2304, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (3): Block(
      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=2304, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=2304, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (4): Block(
      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=2304, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=2304, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (5): Block(
      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=2304, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=2304, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (6): Block(
      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=2304, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=2304, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
    (7): Block(
      (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (attn): Attention(
        (qkv): Linear(in_features=768, out_features=2304, bias=False)
        (attn_drop): Dropout(p=0.0, inplace=False)
        (proj): Linear(in_features=768, out_features=768, bias=True)
        (proj_drop): Dropout(p=0.0, inplace=False)
      )
      (drop_path): Identity()
      (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
      (mlp): Mlp(
        (fc1): Linear(in_features=768, out_features=2304, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=2304, out_features=768, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)
  (pre_logits): Identity()
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
number of params: 48754408
LR = 0.00400000
Batch size = 64
Update frequent = 1
Param groups = {
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "cls_token",
      "pos_embed",
      "patch_embed.proj.bias",
      "blocks.0.norm1.weight",
      "blocks.0.norm1.bias",
      "blocks.0.attn.proj.bias",
      "blocks.0.norm2.weight",
      "blocks.0.norm2.bias",
      "blocks.0.mlp.fc1.bias",
      "blocks.0.mlp.fc2.bias",
      "blocks.1.norm1.weight",
      "blocks.1.norm1.bias",
      "blocks.1.attn.proj.bias",
      "blocks.1.norm2.weight",
      "blocks.1.norm2.bias",
      "blocks.1.mlp.fc1.bias",
      "blocks.1.mlp.fc2.bias",
      "blocks.2.norm1.weight",
      "blocks.2.norm1.bias",
      "blocks.2.attn.proj.bias",
      "blocks.2.norm2.weight",
      "blocks.2.norm2.bias",
      "blocks.2.mlp.fc1.bias",
      "blocks.2.mlp.fc2.bias",
      "blocks.3.norm1.weight",
      "blocks.3.norm1.bias",
      "blocks.3.attn.proj.bias",
      "blocks.3.norm2.weight",
      "blocks.3.norm2.bias",
      "blocks.3.mlp.fc1.bias",
      "blocks.3.mlp.fc2.bias",
      "blocks.4.norm1.weight",
      "blocks.4.norm1.bias",
      "blocks.4.attn.proj.bias",
      "blocks.4.norm2.weight",
      "blocks.4.norm2.bias",
      "blocks.4.mlp.fc1.bias",
      "blocks.4.mlp.fc2.bias",
      "blocks.5.norm1.weight",
      "blocks.5.norm1.bias",
      "blocks.5.attn.proj.bias",
      "blocks.5.norm2.weight",
      "blocks.5.norm2.bias",
      "blocks.5.mlp.fc1.bias",
      "blocks.5.mlp.fc2.bias",
      "blocks.6.norm1.weight",
      "blocks.6.norm1.bias",
      "blocks.6.attn.proj.bias",
      "blocks.6.norm2.weight",
      "blocks.6.norm2.bias",
      "blocks.6.mlp.fc1.bias",
      "blocks.6.mlp.fc2.bias",
      "blocks.7.norm1.weight",
      "blocks.7.norm1.bias",
      "blocks.7.attn.proj.bias",
      "blocks.7.norm2.weight",
      "blocks.7.norm2.bias",
      "blocks.7.mlp.fc1.bias",
      "blocks.7.mlp.fc2.bias",
      "norm.weight",
      "norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  },
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "patch_embed.proj.weight",
      "blocks.0.attn.qkv.weight",
      "blocks.0.attn.proj.weight",
      "blocks.0.mlp.fc1.weight",
      "blocks.0.mlp.fc2.weight",
      "blocks.1.attn.qkv.weight",
      "blocks.1.attn.proj.weight",
      "blocks.1.mlp.fc1.weight",
      "blocks.1.mlp.fc2.weight",
      "blocks.2.attn.qkv.weight",
      "blocks.2.attn.proj.weight",
      "blocks.2.mlp.fc1.weight",
      "blocks.2.mlp.fc2.weight",
      "blocks.3.attn.qkv.weight",
      "blocks.3.attn.proj.weight",
      "blocks.3.mlp.fc1.weight",
      "blocks.3.mlp.fc2.weight",
      "blocks.4.attn.qkv.weight",
      "blocks.4.attn.proj.weight",
      "blocks.4.mlp.fc1.weight",
      "blocks.4.mlp.fc2.weight",
      "blocks.5.attn.qkv.weight",
      "blocks.5.attn.proj.weight",
      "blocks.5.mlp.fc1.weight",
      "blocks.5.mlp.fc2.weight",
      "blocks.6.attn.qkv.weight",
      "blocks.6.attn.proj.weight",
      "blocks.6.mlp.fc1.weight",
      "blocks.6.mlp.fc2.weight",
      "blocks.7.attn.qkv.weight",
      "blocks.7.attn.proj.weight",
      "blocks.7.mlp.fc1.weight",
      "blocks.7.mlp.fc2.weight",
      "head.weight"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
criterion = SoftTargetCrossEntropy()
cls_token density is 1.0
pos_embed density is 1.0
patch_embed.proj.weight density is 1.0
patch_embed.proj.bias density is 1.0
blocks.0.norm1.weight density is 1.0
blocks.0.norm1.bias density is 1.0
blocks.0.attn.qkv.weight density is 1.0
blocks.0.attn.proj.weight density is 1.0
blocks.0.attn.proj.bias density is 1.0
blocks.0.norm2.weight density is 1.0
blocks.0.norm2.bias density is 1.0
blocks.0.mlp.fc1.weight density is 1.0
blocks.0.mlp.fc1.bias density is 1.0
blocks.0.mlp.fc2.weight density is 1.0
blocks.0.mlp.fc2.bias density is 1.0
blocks.1.norm1.weight density is 1.0
blocks.1.norm1.bias density is 1.0
blocks.1.attn.qkv.weight density is 1.0
blocks.1.attn.proj.weight density is 1.0
blocks.1.attn.proj.bias density is 1.0
blocks.1.norm2.weight density is 1.0
blocks.1.norm2.bias density is 1.0
blocks.1.mlp.fc1.weight density is 1.0
blocks.1.mlp.fc1.bias density is 1.0
blocks.1.mlp.fc2.weight density is 1.0
blocks.1.mlp.fc2.bias density is 1.0
blocks.2.norm1.weight density is 1.0
blocks.2.norm1.bias density is 1.0
blocks.2.attn.qkv.weight density is 1.0
blocks.2.attn.proj.weight density is 1.0
blocks.2.attn.proj.bias density is 1.0
blocks.2.norm2.weight density is 1.0
blocks.2.norm2.bias density is 1.0
blocks.2.mlp.fc1.weight density is 1.0
blocks.2.mlp.fc1.bias density is 1.0
blocks.2.mlp.fc2.weight density is 1.0
blocks.2.mlp.fc2.bias density is 1.0
blocks.3.norm1.weight density is 1.0
blocks.3.norm1.bias density is 1.0
blocks.3.attn.qkv.weight density is 1.0
blocks.3.attn.proj.weight density is 1.0
blocks.3.attn.proj.bias density is 1.0
blocks.3.norm2.weight density is 1.0
blocks.3.norm2.bias density is 1.0
blocks.3.mlp.fc1.weight density is 1.0
blocks.3.mlp.fc1.bias density is 1.0
blocks.3.mlp.fc2.weight density is 1.0
blocks.3.mlp.fc2.bias density is 1.0
blocks.4.norm1.weight density is 1.0
blocks.4.norm1.bias density is 1.0
blocks.4.attn.qkv.weight density is 1.0
blocks.4.attn.proj.weight density is 1.0
blocks.4.attn.proj.bias density is 1.0
blocks.4.norm2.weight density is 1.0
blocks.4.norm2.bias density is 1.0
blocks.4.mlp.fc1.weight density is 1.0
blocks.4.mlp.fc1.bias density is 1.0
blocks.4.mlp.fc2.weight density is 1.0
blocks.4.mlp.fc2.bias density is 1.0
blocks.5.norm1.weight density is 1.0
blocks.5.norm1.bias density is 1.0
blocks.5.attn.qkv.weight density is 1.0
blocks.5.attn.proj.weight density is 1.0
blocks.5.attn.proj.bias density is 1.0
blocks.5.norm2.weight density is 1.0
blocks.5.norm2.bias density is 1.0
blocks.5.mlp.fc1.weight density is 1.0
blocks.5.mlp.fc1.bias density is 1.0
blocks.5.mlp.fc2.weight density is 1.0
blocks.5.mlp.fc2.bias density is 1.0
blocks.6.norm1.weight density is 1.0
blocks.6.norm1.bias density is 1.0
blocks.6.attn.qkv.weight density is 1.0
blocks.6.attn.proj.weight density is 1.0
blocks.6.attn.proj.bias density is 1.0
blocks.6.norm2.weight density is 1.0
blocks.6.norm2.bias density is 1.0
blocks.6.mlp.fc1.weight density is 1.0
blocks.6.mlp.fc1.bias density is 1.0
blocks.6.mlp.fc2.weight density is 1.0
blocks.6.mlp.fc2.bias density is 1.0
blocks.7.norm1.weight density is 1.0
blocks.7.norm1.bias density is 1.0
blocks.7.attn.qkv.weight density is 1.0
blocks.7.attn.proj.weight density is 1.0
blocks.7.attn.proj.bias density is 1.0
blocks.7.norm2.weight density is 1.0
blocks.7.norm2.bias density is 1.0
blocks.7.mlp.fc1.weight density is 1.0
blocks.7.mlp.fc1.bias density is 1.0
blocks.7.mlp.fc2.weight density is 1.0
blocks.7.mlp.fc2.bias density is 1.0
norm.weight density is 1.0
norm.bias density is 1.0
head.weight density is 1.0
head.bias density is 1.0
Test:  [  0/531]  eta: 0:41:41  loss: 5.1564 (5.1564)  acc1: 15.6250 (15.6250)  acc5: 40.6250 (40.6250)  time: 4.7113  data: 3.6698  max mem: 803
Test:  [ 10/531]  eta: 0:04:53  loss: 4.8550 (4.5374)  acc1: 20.8333 (27.3674)  acc5: 40.6250 (43.8447)  time: 0.5624  data: 0.4138  max mem: 914
Test:  [ 20/531]  eta: 0:03:06  loss: 4.2539 (4.3778)  acc1: 20.8333 (25.9425)  acc5: 43.7500 (46.2302)  time: 0.1473  data: 0.0772  max mem: 914
Test:  [ 30/531]  eta: 0:02:44  loss: 4.2119 (4.7874)  acc1: 19.7917 (23.0175)  acc5: 43.7500 (41.6331)  time: 0.2007  data: 0.1121  max mem: 914
Test:  [ 40/531]  eta: 0:02:31  loss: 5.7468 (4.8406)  acc1: 15.6250 (22.3323)  acc5: 33.3333 (41.0315)  time: 0.2489  data: 0.1501  max mem: 914
Test:  [ 50/531]  eta: 0:02:21  loss: 4.2704 (4.7209)  acc1: 22.9167 (23.5703)  acc5: 43.7500 (42.6879)  time: 0.2416  data: 0.1456  max mem: 914
Test:  [ 60/531]  eta: 0:02:11  loss: 4.4681 (4.7251)  acc1: 29.1667 (23.8388)  acc5: 43.7500 (41.8887)  time: 0.2201  data: 0.1207  max mem: 914
Test:  [ 70/531]  eta: 0:02:19  loss: 4.4681 (4.6964)  acc1: 23.9583 (24.0317)  acc5: 43.7500 (42.3709)  time: 0.3261  data: 0.2216  max mem: 914
Test:  [ 80/531]  eta: 0:02:13  loss: 4.3637 (4.6659)  acc1: 21.8750 (24.3956)  acc5: 46.8750 (42.4897)  time: 0.3480  data: 0.2477  max mem: 914
Test:  [ 90/531]  eta: 0:02:04  loss: 3.8318 (4.5128)  acc1: 28.1250 (25.8471)  acc5: 51.0417 (44.6772)  time: 0.2049  data: 0.1049  max mem: 914
Test:  [100/531]  eta: 0:01:56  loss: 2.9423 (4.3857)  acc1: 41.6667 (27.0215)  acc5: 63.5417 (46.4625)  time: 0.1594  data: 0.0581  max mem: 914
Test:  [110/531]  eta: 0:01:49  loss: 3.1141 (4.2703)  acc1: 35.4167 (27.8435)  acc5: 66.6667 (48.2827)  time: 0.1658  data: 0.0642  max mem: 914
Test:  [120/531]  eta: 0:01:41  loss: 2.8923 (4.1762)  acc1: 33.3333 (28.6760)  acc5: 67.7083 (49.8450)  time: 0.1417  data: 0.0408  max mem: 914
Test:  [130/531]  eta: 0:01:37  loss: 3.1712 (4.1281)  acc1: 36.4583 (29.0156)  acc5: 64.5833 (50.4691)  time: 0.1415  data: 0.0412  max mem: 914
Test:  [140/531]  eta: 0:01:32  loss: 2.8083 (4.0451)  acc1: 36.4583 (30.0163)  acc5: 67.7083 (51.8322)  time: 0.1727  data: 0.0720  max mem: 914
Test:  [150/531]  eta: 0:01:29  loss: 3.0875 (4.0357)  acc1: 32.2917 (29.9738)  acc5: 60.4167 (52.1109)  time: 0.1842  data: 0.0827  max mem: 914
Test:  [160/531]  eta: 0:01:26  loss: 3.5259 (4.0228)  acc1: 32.2917 (30.2536)  acc5: 59.3750 (52.3163)  time: 0.2134  data: 0.1116  max mem: 914
Test:  [170/531]  eta: 0:01:22  loss: 3.9608 (4.0508)  acc1: 30.2083 (30.0621)  acc5: 50.0000 (51.9371)  time: 0.1769  data: 0.0752  max mem: 914
Test:  [180/531]  eta: 0:01:18  loss: 4.4857 (4.0439)  acc1: 27.0833 (30.3177)  acc5: 42.7083 (51.9970)  time: 0.1498  data: 0.0494  max mem: 914
Test:  [190/531]  eta: 0:01:16  loss: 3.8866 (4.0467)  acc1: 29.1667 (30.4101)  acc5: 52.0833 (52.0779)  time: 0.1980  data: 0.0984  max mem: 914
Test:  [200/531]  eta: 0:01:13  loss: 3.8866 (4.0508)  acc1: 27.0833 (30.3483)  acc5: 52.0833 (51.8553)  time: 0.1866  data: 0.0953  max mem: 914
Test:  [210/531]  eta: 0:01:10  loss: 4.4668 (4.0982)  acc1: 16.6667 (29.8628)  acc5: 43.7500 (51.1799)  time: 0.1837  data: 0.1181  max mem: 914
Test:  [220/531]  eta: 0:01:08  loss: 4.4668 (4.1119)  acc1: 26.0417 (29.8925)  acc5: 45.8333 (50.9427)  time: 0.2172  data: 0.1688  max mem: 914
Test:  [230/531]  eta: 0:01:06  loss: 4.1505 (4.1332)  acc1: 28.1250 (29.7619)  acc5: 47.9167 (50.5051)  time: 0.2081  data: 0.1590  max mem: 914
Test:  [240/531]  eta: 0:01:04  loss: 3.9588 (4.1359)  acc1: 28.1250 (29.6853)  acc5: 48.9583 (50.2982)  time: 0.2304  data: 0.1788  max mem: 914
Test:  [250/531]  eta: 0:01:02  loss: 4.3263 (4.1815)  acc1: 18.7500 (29.1916)  acc5: 43.7500 (49.5725)  time: 0.2708  data: 0.2165  max mem: 914
Test:  [260/531]  eta: 0:01:00  loss: 4.8882 (4.1924)  acc1: 17.7083 (28.8793)  acc5: 36.4583 (49.3016)  time: 0.2366  data: 0.1830  max mem: 914
Test:  [270/531]  eta: 0:00:57  loss: 4.1138 (4.1848)  acc1: 26.0417 (29.0898)  acc5: 44.7917 (49.3389)  time: 0.1728  data: 0.0975  max mem: 914
Test:  [280/531]  eta: 0:00:54  loss: 3.6843 (4.1669)  acc1: 32.2917 (29.3372)  acc5: 53.1250 (49.5255)  time: 0.1665  data: 0.0679  max mem: 914
Test:  [290/531]  eta: 0:00:52  loss: 3.6293 (4.1675)  acc1: 30.2083 (29.3707)  acc5: 50.0000 (49.4523)  time: 0.1622  data: 0.0637  max mem: 914
Test:  [300/531]  eta: 0:00:49  loss: 3.8918 (4.1639)  acc1: 33.3333 (29.5058)  acc5: 52.0833 (49.5224)  time: 0.1591  data: 0.0591  max mem: 914
Test:  [310/531]  eta: 0:00:47  loss: 3.8918 (4.1594)  acc1: 32.2917 (29.5552)  acc5: 51.0417 (49.5713)  time: 0.1979  data: 0.1070  max mem: 914
Test:  [320/531]  eta: 0:00:45  loss: 3.9787 (4.1556)  acc1: 27.0833 (29.6599)  acc5: 46.8750 (49.5197)  time: 0.2255  data: 0.1377  max mem: 914
Test:  [330/531]  eta: 0:00:42  loss: 4.1068 (4.1485)  acc1: 26.0417 (29.5758)  acc5: 46.8750 (49.5531)  time: 0.1927  data: 0.0986  max mem: 914
Test:  [340/531]  eta: 0:00:40  loss: 4.2236 (4.1646)  acc1: 21.8750 (29.2461)  acc5: 44.7917 (49.2546)  time: 0.1700  data: 0.0734  max mem: 914
Test:  [350/531]  eta: 0:00:38  loss: 4.3046 (4.1738)  acc1: 17.7083 (29.0836)  acc5: 38.5417 (49.0118)  time: 0.1824  data: 0.0805  max mem: 914
Test:  [360/531]  eta: 0:00:36  loss: 4.3818 (4.1871)  acc1: 18.7500 (28.7800)  acc5: 39.5833 (48.7044)  time: 0.1926  data: 0.0981  max mem: 914
Test:  [370/531]  eta: 0:00:34  loss: 4.5899 (4.1962)  acc1: 18.7500 (28.7343)  acc5: 39.5833 (48.5568)  time: 0.2001  data: 0.1328  max mem: 914
Test:  [380/531]  eta: 0:00:32  loss: 4.0132 (4.1904)  acc1: 23.9583 (28.7675)  acc5: 43.7500 (48.5701)  time: 0.2247  data: 0.1492  max mem: 914
Test:  [390/531]  eta: 0:00:29  loss: 4.3214 (4.2183)  acc1: 22.9167 (28.5326)  acc5: 39.5833 (48.1724)  time: 0.2007  data: 0.0992  max mem: 914
Test:  [400/531]  eta: 0:00:27  loss: 4.8353 (4.2283)  acc1: 17.7083 (28.4731)  acc5: 34.3750 (47.9686)  time: 0.1756  data: 0.0756  max mem: 914
Test:  [410/531]  eta: 0:00:25  loss: 4.1148 (4.2246)  acc1: 31.2500 (28.6192)  acc5: 44.7917 (48.0180)  time: 0.1780  data: 0.0786  max mem: 914
Test:  [420/531]  eta: 0:00:23  loss: 4.4472 (4.2317)  acc1: 25.0000 (28.5258)  acc5: 43.7500 (47.8771)  time: 0.1648  data: 0.0661  max mem: 914
Test:  [430/531]  eta: 0:00:20  loss: 4.5812 (4.2477)  acc1: 19.7917 (28.3522)  acc5: 41.6667 (47.6049)  time: 0.1734  data: 0.0742  max mem: 914
Test:  [440/531]  eta: 0:00:18  loss: 4.4904 (4.2406)  acc1: 28.1250 (28.4297)  acc5: 41.6667 (47.7253)  time: 0.1913  data: 0.1135  max mem: 914
Test:  [450/531]  eta: 0:00:16  loss: 3.9144 (4.2382)  acc1: 27.0833 (28.3560)  acc5: 50.0000 (47.7411)  time: 0.2213  data: 0.1671  max mem: 914
Test:  [460/531]  eta: 0:00:14  loss: 4.1330 (4.2418)  acc1: 23.9583 (28.3464)  acc5: 44.7917 (47.5800)  time: 0.2084  data: 0.1570  max mem: 914
Test:  [470/531]  eta: 0:00:12  loss: 4.1330 (4.2371)  acc1: 31.2500 (28.5275)  acc5: 44.7917 (47.6491)  time: 0.1726  data: 0.1208  max mem: 914
Test:  [480/531]  eta: 0:00:10  loss: 4.0547 (4.2342)  acc1: 30.2083 (28.4520)  acc5: 48.9583 (47.7218)  time: 0.1605  data: 0.1072  max mem: 914
Test:  [490/531]  eta: 0:00:08  loss: 4.4985 (4.2532)  acc1: 19.7917 (28.3159)  acc5: 43.7500 (47.4839)  time: 0.2116  data: 0.1596  max mem: 914
Test:  [500/531]  eta: 0:00:06  loss: 6.2465 (4.3060)  acc1: 9.3750 (27.9462)  acc5: 22.9167 (46.8334)  time: 0.2369  data: 0.1877  max mem: 914
Test:  [510/531]  eta: 0:00:04  loss: 6.6886 (4.3459)  acc1: 6.2500 (27.6113)  acc5: 12.5000 (46.2757)  time: 0.2092  data: 0.1604  max mem: 914
Test:  [520/531]  eta: 0:00:02  loss: 5.8670 (4.3780)  acc1: 9.3750 (27.2913)  acc5: 18.7500 (45.8213)  time: 0.2244  data: 0.1737  max mem: 914
Test:  [530/531]  eta: 0:00:00  loss: 6.3107 (4.4185)  acc1: 7.2917 (26.9194)  acc5: 14.5833 (45.2750)  time: 0.2231  data: 0.1725  max mem: 914
Test: Total time: 0:01:50 (0.2077 s / it)
* Acc@1 26.919 Acc@5 45.275 loss 4.418
Accuracy of the network on 50889 test images: 26.91937%
Not using distributed mode
Namespace(Decom=False, aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=64, bn=True, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/sliu/project_space/datasets/sketch', data_set='IMNET', data_type='imagenet-s', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.1, enable_wandb=False, epochs=300, eval=True, eval_data_path=None, finetune='', fix=False, growth='random', head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, kernel_size=[7, 7, 7, 7, 100], layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=-1, log_dir=None, lr=0.004, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='cswin', model_ema=False, model_ema_decay=0.9999, model_ema_eval=False, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_workers=10, only_L=False, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', parallel=True, pin_mem=True, project='SLaK', prune='magnitude', prune_rate=0.3, recount=1, redistribution='none', remode='pixel', reprob=0.25, resplit=False, resume='/gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_VitTeacher_STRN50_NKD/checkpoint-best.pth', save_ckpt=True, save_ckpt_freq=10, save_ckpt_num=3, seed=0, smoothing=0.1, sparse=False, sparse_init='snip', sparsity=0.4, start_epoch=0, train_interpolation='bicubic', update_freq=1, update_frequency=100, use_amp=False, verbose=False, wandb_ckpt=False, warmup_epochs=20, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, width_factor=1.0, world_size=1)
Transform = 
Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /home/sliu/project_space/datasets/sketch
Number of the class = 1000
Mixup is activated!
Model = CSWinTransformer(
  (stage1_conv_embed): Sequential(
    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))
    (1): Rearrange('b c h w -> b (h w) c', h=56, w=56)
    (2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
  )
  (stage1): ModuleList(
    (0): CSWinBlock(
      (qkv): Linear(in_features=64, out_features=192, bias=True)
      (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
      (proj): Linear(in_features=64, out_features=64, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
      (attns): ModuleList(
        (0): LePEAttention(
          (get_v): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
        (1): LePEAttention(
          (get_v): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (drop_path): Identity()
      (mlp): Mlp(
        (fc1): Linear(in_features=64, out_features=256, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=256, out_features=64, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
    )
  )
  (merge1): Merge_Block(
    (conv): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
  )
  (stage2): ModuleList(
    (0): CSWinBlock(
      (qkv): Linear(in_features=128, out_features=384, bias=True)
      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (proj): Linear(in_features=128, out_features=128, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
      (attns): ModuleList(
        (0): LePEAttention(
          (get_v): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
        (1): LePEAttention(
          (get_v): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (drop_path): Identity()
      (mlp): Mlp(
        (fc1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=512, out_features=128, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
    (1): CSWinBlock(
      (qkv): Linear(in_features=128, out_features=384, bias=True)
      (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
      (proj): Linear(in_features=128, out_features=128, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
      (attns): ModuleList(
        (0): LePEAttention(
          (get_v): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
        (1): LePEAttention(
          (get_v): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=64)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (drop_path): Identity()
      (mlp): Mlp(
        (fc1): Linear(in_features=128, out_features=512, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=512, out_features=128, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
    )
  )
  (merge2): Merge_Block(
    (conv): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
  )
  (stage3): ModuleList(
    (0): CSWinBlock(
      (qkv): Linear(in_features=256, out_features=768, bias=True)
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (proj): Linear(in_features=256, out_features=256, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
      (attns): ModuleList(
        (0): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
        (1): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (drop_path): Identity()
      (mlp): Mlp(
        (fc1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1024, out_features=256, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (1): CSWinBlock(
      (qkv): Linear(in_features=256, out_features=768, bias=True)
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (proj): Linear(in_features=256, out_features=256, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
      (attns): ModuleList(
        (0): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
        (1): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (drop_path): Identity()
      (mlp): Mlp(
        (fc1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1024, out_features=256, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (2): CSWinBlock(
      (qkv): Linear(in_features=256, out_features=768, bias=True)
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (proj): Linear(in_features=256, out_features=256, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
      (attns): ModuleList(
        (0): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
        (1): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (drop_path): Identity()
      (mlp): Mlp(
        (fc1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1024, out_features=256, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (3): CSWinBlock(
      (qkv): Linear(in_features=256, out_features=768, bias=True)
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (proj): Linear(in_features=256, out_features=256, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
      (attns): ModuleList(
        (0): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
        (1): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (drop_path): Identity()
      (mlp): Mlp(
        (fc1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1024, out_features=256, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (4): CSWinBlock(
      (qkv): Linear(in_features=256, out_features=768, bias=True)
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (proj): Linear(in_features=256, out_features=256, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
      (attns): ModuleList(
        (0): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
        (1): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (drop_path): Identity()
      (mlp): Mlp(
        (fc1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1024, out_features=256, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (5): CSWinBlock(
      (qkv): Linear(in_features=256, out_features=768, bias=True)
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (proj): Linear(in_features=256, out_features=256, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
      (attns): ModuleList(
        (0): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
        (1): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (drop_path): Identity()
      (mlp): Mlp(
        (fc1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1024, out_features=256, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (6): CSWinBlock(
      (qkv): Linear(in_features=256, out_features=768, bias=True)
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (proj): Linear(in_features=256, out_features=256, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
      (attns): ModuleList(
        (0): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
        (1): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (drop_path): Identity()
      (mlp): Mlp(
        (fc1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1024, out_features=256, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (7): CSWinBlock(
      (qkv): Linear(in_features=256, out_features=768, bias=True)
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (proj): Linear(in_features=256, out_features=256, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
      (attns): ModuleList(
        (0): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
        (1): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (drop_path): Identity()
      (mlp): Mlp(
        (fc1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1024, out_features=256, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (8): CSWinBlock(
      (qkv): Linear(in_features=256, out_features=768, bias=True)
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (proj): Linear(in_features=256, out_features=256, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
      (attns): ModuleList(
        (0): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
        (1): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (drop_path): Identity()
      (mlp): Mlp(
        (fc1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1024, out_features=256, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (9): CSWinBlock(
      (qkv): Linear(in_features=256, out_features=768, bias=True)
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (proj): Linear(in_features=256, out_features=256, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
      (attns): ModuleList(
        (0): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
        (1): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (drop_path): Identity()
      (mlp): Mlp(
        (fc1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1024, out_features=256, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (10): CSWinBlock(
      (qkv): Linear(in_features=256, out_features=768, bias=True)
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (proj): Linear(in_features=256, out_features=256, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
      (attns): ModuleList(
        (0): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
        (1): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (drop_path): Identity()
      (mlp): Mlp(
        (fc1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1024, out_features=256, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (11): CSWinBlock(
      (qkv): Linear(in_features=256, out_features=768, bias=True)
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (proj): Linear(in_features=256, out_features=256, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
      (attns): ModuleList(
        (0): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
        (1): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (drop_path): Identity()
      (mlp): Mlp(
        (fc1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1024, out_features=256, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (12): CSWinBlock(
      (qkv): Linear(in_features=256, out_features=768, bias=True)
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (proj): Linear(in_features=256, out_features=256, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
      (attns): ModuleList(
        (0): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
        (1): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (drop_path): Identity()
      (mlp): Mlp(
        (fc1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1024, out_features=256, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (13): CSWinBlock(
      (qkv): Linear(in_features=256, out_features=768, bias=True)
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (proj): Linear(in_features=256, out_features=256, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
      (attns): ModuleList(
        (0): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
        (1): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (drop_path): Identity()
      (mlp): Mlp(
        (fc1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1024, out_features=256, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (14): CSWinBlock(
      (qkv): Linear(in_features=256, out_features=768, bias=True)
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (proj): Linear(in_features=256, out_features=256, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
      (attns): ModuleList(
        (0): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
        (1): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (drop_path): Identity()
      (mlp): Mlp(
        (fc1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1024, out_features=256, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (15): CSWinBlock(
      (qkv): Linear(in_features=256, out_features=768, bias=True)
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (proj): Linear(in_features=256, out_features=256, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
      (attns): ModuleList(
        (0): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
        (1): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (drop_path): Identity()
      (mlp): Mlp(
        (fc1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1024, out_features=256, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (16): CSWinBlock(
      (qkv): Linear(in_features=256, out_features=768, bias=True)
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (proj): Linear(in_features=256, out_features=256, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
      (attns): ModuleList(
        (0): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
        (1): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (drop_path): Identity()
      (mlp): Mlp(
        (fc1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1024, out_features=256, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (17): CSWinBlock(
      (qkv): Linear(in_features=256, out_features=768, bias=True)
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (proj): Linear(in_features=256, out_features=256, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
      (attns): ModuleList(
        (0): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
        (1): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (drop_path): Identity()
      (mlp): Mlp(
        (fc1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1024, out_features=256, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (18): CSWinBlock(
      (qkv): Linear(in_features=256, out_features=768, bias=True)
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (proj): Linear(in_features=256, out_features=256, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
      (attns): ModuleList(
        (0): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
        (1): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (drop_path): Identity()
      (mlp): Mlp(
        (fc1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1024, out_features=256, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (19): CSWinBlock(
      (qkv): Linear(in_features=256, out_features=768, bias=True)
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (proj): Linear(in_features=256, out_features=256, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
      (attns): ModuleList(
        (0): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
        (1): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (drop_path): Identity()
      (mlp): Mlp(
        (fc1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1024, out_features=256, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
    (20): CSWinBlock(
      (qkv): Linear(in_features=256, out_features=768, bias=True)
      (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
      (proj): Linear(in_features=256, out_features=256, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
      (attns): ModuleList(
        (0): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
        (1): LePEAttention(
          (get_v): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=128)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (drop_path): Identity()
      (mlp): Mlp(
        (fc1): Linear(in_features=256, out_features=1024, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=1024, out_features=256, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
    )
  )
  (merge3): Merge_Block(
    (conv): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))
    (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  )
  (stage4): ModuleList(
    (0): CSWinBlock(
      (qkv): Linear(in_features=512, out_features=1536, bias=True)
      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (proj): Linear(in_features=512, out_features=512, bias=True)
      (proj_drop): Dropout(p=0.0, inplace=False)
      (attns): ModuleList(
        (0): LePEAttention(
          (get_v): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512)
          (attn_drop): Dropout(p=0.0, inplace=False)
        )
      )
      (drop_path): Identity()
      (mlp): Mlp(
        (fc1): Linear(in_features=512, out_features=2048, bias=True)
        (act): GELU()
        (fc2): Linear(in_features=2048, out_features=512, bias=True)
        (drop): Dropout(p=0.0, inplace=False)
      )
      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
    )
  )
  (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (head): Linear(in_features=512, out_features=1000, bias=True)
)
number of params: 22320552
LR = 0.00400000
Batch size = 64
Update frequent = 1
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "stage1_conv_embed.0.weight",
      "stage1.0.qkv.weight",
      "stage1.0.proj.weight",
      "stage1.0.attns.0.get_v.weight",
      "stage1.0.attns.1.get_v.weight",
      "stage1.0.mlp.fc1.weight",
      "stage1.0.mlp.fc2.weight",
      "merge1.conv.weight",
      "stage2.0.qkv.weight",
      "stage2.0.proj.weight",
      "stage2.0.attns.0.get_v.weight",
      "stage2.0.attns.1.get_v.weight",
      "stage2.0.mlp.fc1.weight",
      "stage2.0.mlp.fc2.weight",
      "stage2.1.qkv.weight",
      "stage2.1.proj.weight",
      "stage2.1.attns.0.get_v.weight",
      "stage2.1.attns.1.get_v.weight",
      "stage2.1.mlp.fc1.weight",
      "stage2.1.mlp.fc2.weight",
      "merge2.conv.weight",
      "stage3.0.qkv.weight",
      "stage3.0.proj.weight",
      "stage3.0.attns.0.get_v.weight",
      "stage3.0.attns.1.get_v.weight",
      "stage3.0.mlp.fc1.weight",
      "stage3.0.mlp.fc2.weight",
      "stage3.1.qkv.weight",
      "stage3.1.proj.weight",
      "stage3.1.attns.0.get_v.weight",
      "stage3.1.attns.1.get_v.weight",
      "stage3.1.mlp.fc1.weight",
      "stage3.1.mlp.fc2.weight",
      "stage3.2.qkv.weight",
      "stage3.2.proj.weight",
      "stage3.2.attns.0.get_v.weight",
      "stage3.2.attns.1.get_v.weight",
      "stage3.2.mlp.fc1.weight",
      "stage3.2.mlp.fc2.weight",
      "stage3.3.qkv.weight",
      "stage3.3.proj.weight",
      "stage3.3.attns.0.get_v.weight",
      "stage3.3.attns.1.get_v.weight",
      "stage3.3.mlp.fc1.weight",
      "stage3.3.mlp.fc2.weight",
      "stage3.4.qkv.weight",
      "stage3.4.proj.weight",
      "stage3.4.attns.0.get_v.weight",
      "stage3.4.attns.1.get_v.weight",
      "stage3.4.mlp.fc1.weight",
      "stage3.4.mlp.fc2.weight",
      "stage3.5.qkv.weight",
      "stage3.5.proj.weight",
      "stage3.5.attns.0.get_v.weight",
      "stage3.5.attns.1.get_v.weight",
      "stage3.5.mlp.fc1.weight",
      "stage3.5.mlp.fc2.weight",
      "stage3.6.qkv.weight",
      "stage3.6.proj.weight",
      "stage3.6.attns.0.get_v.weight",
      "stage3.6.attns.1.get_v.weight",
      "stage3.6.mlp.fc1.weight",
      "stage3.6.mlp.fc2.weight",
      "stage3.7.qkv.weight",
      "stage3.7.proj.weight",
      "stage3.7.attns.0.get_v.weight",
      "stage3.7.attns.1.get_v.weight",
      "stage3.7.mlp.fc1.weight",
      "stage3.7.mlp.fc2.weight",
      "stage3.8.qkv.weight",
      "stage3.8.proj.weight",
      "stage3.8.attns.0.get_v.weight",
      "stage3.8.attns.1.get_v.weight",
      "stage3.8.mlp.fc1.weight",
      "stage3.8.mlp.fc2.weight",
      "stage3.9.qkv.weight",
      "stage3.9.proj.weight",
      "stage3.9.attns.0.get_v.weight",
      "stage3.9.attns.1.get_v.weight",
      "stage3.9.mlp.fc1.weight",
      "stage3.9.mlp.fc2.weight",
      "stage3.10.qkv.weight",
      "stage3.10.proj.weight",
      "stage3.10.attns.0.get_v.weight",
      "stage3.10.attns.1.get_v.weight",
      "stage3.10.mlp.fc1.weight",
      "stage3.10.mlp.fc2.weight",
      "stage3.11.qkv.weight",
      "stage3.11.proj.weight",
      "stage3.11.attns.0.get_v.weight",
      "stage3.11.attns.1.get_v.weight",
      "stage3.11.mlp.fc1.weight",
      "stage3.11.mlp.fc2.weight",
      "stage3.12.qkv.weight",
      "stage3.12.proj.weight",
      "stage3.12.attns.0.get_v.weight",
      "stage3.12.attns.1.get_v.weight",
      "stage3.12.mlp.fc1.weight",
      "stage3.12.mlp.fc2.weight",
      "stage3.13.qkv.weight",
      "stage3.13.proj.weight",
      "stage3.13.attns.0.get_v.weight",
      "stage3.13.attns.1.get_v.weight",
      "stage3.13.mlp.fc1.weight",
      "stage3.13.mlp.fc2.weight",
      "stage3.14.qkv.weight",
      "stage3.14.proj.weight",
      "stage3.14.attns.0.get_v.weight",
      "stage3.14.attns.1.get_v.weight",
      "stage3.14.mlp.fc1.weight",
      "stage3.14.mlp.fc2.weight",
      "stage3.15.qkv.weight",
      "stage3.15.proj.weight",
      "stage3.15.attns.0.get_v.weight",
      "stage3.15.attns.1.get_v.weight",
      "stage3.15.mlp.fc1.weight",
      "stage3.15.mlp.fc2.weight",
      "stage3.16.qkv.weight",
      "stage3.16.proj.weight",
      "stage3.16.attns.0.get_v.weight",
      "stage3.16.attns.1.get_v.weight",
      "stage3.16.mlp.fc1.weight",
      "stage3.16.mlp.fc2.weight",
      "stage3.17.qkv.weight",
      "stage3.17.proj.weight",
      "stage3.17.attns.0.get_v.weight",
      "stage3.17.attns.1.get_v.weight",
      "stage3.17.mlp.fc1.weight",
      "stage3.17.mlp.fc2.weight",
      "stage3.18.qkv.weight",
      "stage3.18.proj.weight",
      "stage3.18.attns.0.get_v.weight",
      "stage3.18.attns.1.get_v.weight",
      "stage3.18.mlp.fc1.weight",
      "stage3.18.mlp.fc2.weight",
      "stage3.19.qkv.weight",
      "stage3.19.proj.weight",
      "stage3.19.attns.0.get_v.weight",
      "stage3.19.attns.1.get_v.weight",
      "stage3.19.mlp.fc1.weight",
      "stage3.19.mlp.fc2.weight",
      "stage3.20.qkv.weight",
      "stage3.20.proj.weight",
      "stage3.20.attns.0.get_v.weight",
      "stage3.20.attns.1.get_v.weight",
      "stage3.20.mlp.fc1.weight",
      "stage3.20.mlp.fc2.weight",
      "merge3.conv.weight",
      "stage4.0.qkv.weight",
      "stage4.0.proj.weight",
      "stage4.0.attns.0.get_v.weight",
      "stage4.0.mlp.fc1.weight",
      "stage4.0.mlp.fc2.weight",
      "head.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "stage1_conv_embed.0.bias",
      "stage1_conv_embed.2.weight",
      "stage1_conv_embed.2.bias",
      "stage1.0.qkv.bias",
      "stage1.0.norm1.weight",
      "stage1.0.norm1.bias",
      "stage1.0.proj.bias",
      "stage1.0.attns.0.get_v.bias",
      "stage1.0.attns.1.get_v.bias",
      "stage1.0.mlp.fc1.bias",
      "stage1.0.mlp.fc2.bias",
      "stage1.0.norm2.weight",
      "stage1.0.norm2.bias",
      "merge1.conv.bias",
      "merge1.norm.weight",
      "merge1.norm.bias",
      "stage2.0.qkv.bias",
      "stage2.0.norm1.weight",
      "stage2.0.norm1.bias",
      "stage2.0.proj.bias",
      "stage2.0.attns.0.get_v.bias",
      "stage2.0.attns.1.get_v.bias",
      "stage2.0.mlp.fc1.bias",
      "stage2.0.mlp.fc2.bias",
      "stage2.0.norm2.weight",
      "stage2.0.norm2.bias",
      "stage2.1.qkv.bias",
      "stage2.1.norm1.weight",
      "stage2.1.norm1.bias",
      "stage2.1.proj.bias",
      "stage2.1.attns.0.get_v.bias",
      "stage2.1.attns.1.get_v.bias",
      "stage2.1.mlp.fc1.bias",
      "stage2.1.mlp.fc2.bias",
      "stage2.1.norm2.weight",
      "stage2.1.norm2.bias",
      "merge2.conv.bias",
      "merge2.norm.weight",
      "merge2.norm.bias",
      "stage3.0.qkv.bias",
      "stage3.0.norm1.weight",
      "stage3.0.norm1.bias",
      "stage3.0.proj.bias",
      "stage3.0.attns.0.get_v.bias",
      "stage3.0.attns.1.get_v.bias",
      "stage3.0.mlp.fc1.bias",
      "stage3.0.mlp.fc2.bias",
      "stage3.0.norm2.weight",
      "stage3.0.norm2.bias",
      "stage3.1.qkv.bias",
      "stage3.1.norm1.weight",
      "stage3.1.norm1.bias",
      "stage3.1.proj.bias",
      "stage3.1.attns.0.get_v.bias",
      "stage3.1.attns.1.get_v.bias",
      "stage3.1.mlp.fc1.bias",
      "stage3.1.mlp.fc2.bias",
      "stage3.1.norm2.weight",
      "stage3.1.norm2.bias",
      "stage3.2.qkv.bias",
      "stage3.2.norm1.weight",
      "stage3.2.norm1.bias",
      "stage3.2.proj.bias",
      "stage3.2.attns.0.get_v.bias",
      "stage3.2.attns.1.get_v.bias",
      "stage3.2.mlp.fc1.bias",
      "stage3.2.mlp.fc2.bias",
      "stage3.2.norm2.weight",
      "stage3.2.norm2.bias",
      "stage3.3.qkv.bias",
      "stage3.3.norm1.weight",
      "stage3.3.norm1.bias",
      "stage3.3.proj.bias",
      "stage3.3.attns.0.get_v.bias",
      "stage3.3.attns.1.get_v.bias",
      "stage3.3.mlp.fc1.bias",
      "stage3.3.mlp.fc2.bias",
      "stage3.3.norm2.weight",
      "stage3.3.norm2.bias",
      "stage3.4.qkv.bias",
      "stage3.4.norm1.weight",
      "stage3.4.norm1.bias",
      "stage3.4.proj.bias",
      "stage3.4.attns.0.get_v.bias",
      "stage3.4.attns.1.get_v.bias",
      "stage3.4.mlp.fc1.bias",
      "stage3.4.mlp.fc2.bias",
      "stage3.4.norm2.weight",
      "stage3.4.norm2.bias",
      "stage3.5.qkv.bias",
      "stage3.5.norm1.weight",
      "stage3.5.norm1.bias",
      "stage3.5.proj.bias",
      "stage3.5.attns.0.get_v.bias",
      "stage3.5.attns.1.get_v.bias",
      "stage3.5.mlp.fc1.bias",
      "stage3.5.mlp.fc2.bias",
      "stage3.5.norm2.weight",
      "stage3.5.norm2.bias",
      "stage3.6.qkv.bias",
      "stage3.6.norm1.weight",
      "stage3.6.norm1.bias",
      "stage3.6.proj.bias",
      "stage3.6.attns.0.get_v.bias",
      "stage3.6.attns.1.get_v.bias",
      "stage3.6.mlp.fc1.bias",
      "stage3.6.mlp.fc2.bias",
      "stage3.6.norm2.weight",
      "stage3.6.norm2.bias",
      "stage3.7.qkv.bias",
      "stage3.7.norm1.weight",
      "stage3.7.norm1.bias",
      "stage3.7.proj.bias",
      "stage3.7.attns.0.get_v.bias",
      "stage3.7.attns.1.get_v.bias",
      "stage3.7.mlp.fc1.bias",
      "stage3.7.mlp.fc2.bias",
      "stage3.7.norm2.weight",
      "stage3.7.norm2.bias",
      "stage3.8.qkv.bias",
      "stage3.8.norm1.weight",
      "stage3.8.norm1.bias",
      "stage3.8.proj.bias",
      "stage3.8.attns.0.get_v.bias",
      "stage3.8.attns.1.get_v.bias",
      "stage3.8.mlp.fc1.bias",
      "stage3.8.mlp.fc2.bias",
      "stage3.8.norm2.weight",
      "stage3.8.norm2.bias",
      "stage3.9.qkv.bias",
      "stage3.9.norm1.weight",
      "stage3.9.norm1.bias",
      "stage3.9.proj.bias",
      "stage3.9.attns.0.get_v.bias",
      "stage3.9.attns.1.get_v.bias",
      "stage3.9.mlp.fc1.bias",
      "stage3.9.mlp.fc2.bias",
      "stage3.9.norm2.weight",
      "stage3.9.norm2.bias",
      "stage3.10.qkv.bias",
      "stage3.10.norm1.weight",
      "stage3.10.norm1.bias",
      "stage3.10.proj.bias",
      "stage3.10.attns.0.get_v.bias",
      "stage3.10.attns.1.get_v.bias",
      "stage3.10.mlp.fc1.bias",
      "stage3.10.mlp.fc2.bias",
      "stage3.10.norm2.weight",
      "stage3.10.norm2.bias",
      "stage3.11.qkv.bias",
      "stage3.11.norm1.weight",
      "stage3.11.norm1.bias",
      "stage3.11.proj.bias",
      "stage3.11.attns.0.get_v.bias",
      "stage3.11.attns.1.get_v.bias",
      "stage3.11.mlp.fc1.bias",
      "stage3.11.mlp.fc2.bias",
      "stage3.11.norm2.weight",
      "stage3.11.norm2.bias",
      "stage3.12.qkv.bias",
      "stage3.12.norm1.weight",
      "stage3.12.norm1.bias",
      "stage3.12.proj.bias",
      "stage3.12.attns.0.get_v.bias",
      "stage3.12.attns.1.get_v.bias",
      "stage3.12.mlp.fc1.bias",
      "stage3.12.mlp.fc2.bias",
      "stage3.12.norm2.weight",
      "stage3.12.norm2.bias",
      "stage3.13.qkv.bias",
      "stage3.13.norm1.weight",
      "stage3.13.norm1.bias",
      "stage3.13.proj.bias",
      "stage3.13.attns.0.get_v.bias",
      "stage3.13.attns.1.get_v.bias",
      "stage3.13.mlp.fc1.bias",
      "stage3.13.mlp.fc2.bias",
      "stage3.13.norm2.weight",
      "stage3.13.norm2.bias",
      "stage3.14.qkv.bias",
      "stage3.14.norm1.weight",
      "stage3.14.norm1.bias",
      "stage3.14.proj.bias",
      "stage3.14.attns.0.get_v.bias",
      "stage3.14.attns.1.get_v.bias",
      "stage3.14.mlp.fc1.bias",
      "stage3.14.mlp.fc2.bias",
      "stage3.14.norm2.weight",
      "stage3.14.norm2.bias",
      "stage3.15.qkv.bias",
      "stage3.15.norm1.weight",
      "stage3.15.norm1.bias",
      "stage3.15.proj.bias",
      "stage3.15.attns.0.get_v.bias",
      "stage3.15.attns.1.get_v.bias",
      "stage3.15.mlp.fc1.bias",
      "stage3.15.mlp.fc2.bias",
      "stage3.15.norm2.weight",
      "stage3.15.norm2.bias",
      "stage3.16.qkv.bias",
      "stage3.16.norm1.weight",
      "stage3.16.norm1.bias",
      "stage3.16.proj.bias",
      "stage3.16.attns.0.get_v.bias",
      "stage3.16.attns.1.get_v.bias",
      "stage3.16.mlp.fc1.bias",
      "stage3.16.mlp.fc2.bias",
      "stage3.16.norm2.weight",
      "stage3.16.norm2.bias",
      "stage3.17.qkv.bias",
      "stage3.17.norm1.weight",
      "stage3.17.norm1.bias",
      "stage3.17.proj.bias",
      "stage3.17.attns.0.get_v.bias",
      "stage3.17.attns.1.get_v.bias",
      "stage3.17.mlp.fc1.bias",
      "stage3.17.mlp.fc2.bias",
      "stage3.17.norm2.weight",
      "stage3.17.norm2.bias",
      "stage3.18.qkv.bias",
      "stage3.18.norm1.weight",
      "stage3.18.norm1.bias",
      "stage3.18.proj.bias",
      "stage3.18.attns.0.get_v.bias",
      "stage3.18.attns.1.get_v.bias",
      "stage3.18.mlp.fc1.bias",
      "stage3.18.mlp.fc2.bias",
      "stage3.18.norm2.weight",
      "stage3.18.norm2.bias",
      "stage3.19.qkv.bias",
      "stage3.19.norm1.weight",
      "stage3.19.norm1.bias",
      "stage3.19.proj.bias",
      "stage3.19.attns.0.get_v.bias",
      "stage3.19.attns.1.get_v.bias",
      "stage3.19.mlp.fc1.bias",
      "stage3.19.mlp.fc2.bias",
      "stage3.19.norm2.weight",
      "stage3.19.norm2.bias",
      "stage3.20.qkv.bias",
      "stage3.20.norm1.weight",
      "stage3.20.norm1.bias",
      "stage3.20.proj.bias",
      "stage3.20.attns.0.get_v.bias",
      "stage3.20.attns.1.get_v.bias",
      "stage3.20.mlp.fc1.bias",
      "stage3.20.mlp.fc2.bias",
      "stage3.20.norm2.weight",
      "stage3.20.norm2.bias",
      "merge3.conv.bias",
      "merge3.norm.weight",
      "merge3.norm.bias",
      "stage4.0.qkv.bias",
      "stage4.0.norm1.weight",
      "stage4.0.norm1.bias",
      "stage4.0.proj.bias",
      "stage4.0.attns.0.get_v.bias",
      "stage4.0.mlp.fc1.bias",
      "stage4.0.mlp.fc2.bias",
      "stage4.0.norm2.weight",
      "stage4.0.norm2.bias",
      "norm.weight",
      "norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
criterion = SoftTargetCrossEntropy()
stage1_conv_embed.0.weight density is 1.0
stage1_conv_embed.0.bias density is 1.0
stage1_conv_embed.2.weight density is 1.0
stage1_conv_embed.2.bias density is 1.0
stage1.0.qkv.weight density is 1.0
stage1.0.qkv.bias density is 1.0
stage1.0.norm1.weight density is 1.0
stage1.0.norm1.bias density is 1.0
stage1.0.proj.weight density is 1.0
stage1.0.proj.bias density is 1.0
stage1.0.attns.0.get_v.weight density is 1.0
stage1.0.attns.0.get_v.bias density is 1.0
stage1.0.attns.1.get_v.weight density is 1.0
stage1.0.attns.1.get_v.bias density is 1.0
stage1.0.mlp.fc1.weight density is 1.0
stage1.0.mlp.fc1.bias density is 1.0
stage1.0.mlp.fc2.weight density is 1.0
stage1.0.mlp.fc2.bias density is 1.0
stage1.0.norm2.weight density is 1.0
stage1.0.norm2.bias density is 1.0
merge1.conv.weight density is 1.0
merge1.conv.bias density is 1.0
merge1.norm.weight density is 1.0
merge1.norm.bias density is 1.0
stage2.0.qkv.weight density is 1.0
stage2.0.qkv.bias density is 1.0
stage2.0.norm1.weight density is 1.0
stage2.0.norm1.bias density is 1.0
stage2.0.proj.weight density is 1.0
stage2.0.proj.bias density is 1.0
stage2.0.attns.0.get_v.weight density is 1.0
stage2.0.attns.0.get_v.bias density is 1.0
stage2.0.attns.1.get_v.weight density is 1.0
stage2.0.attns.1.get_v.bias density is 1.0
stage2.0.mlp.fc1.weight density is 1.0
stage2.0.mlp.fc1.bias density is 1.0
stage2.0.mlp.fc2.weight density is 1.0
stage2.0.mlp.fc2.bias density is 1.0
stage2.0.norm2.weight density is 1.0
stage2.0.norm2.bias density is 1.0
stage2.1.qkv.weight density is 1.0
stage2.1.qkv.bias density is 1.0
stage2.1.norm1.weight density is 1.0
stage2.1.norm1.bias density is 1.0
stage2.1.proj.weight density is 1.0
stage2.1.proj.bias density is 1.0
stage2.1.attns.0.get_v.weight density is 1.0
stage2.1.attns.0.get_v.bias density is 1.0
stage2.1.attns.1.get_v.weight density is 1.0
stage2.1.attns.1.get_v.bias density is 1.0
stage2.1.mlp.fc1.weight density is 1.0
stage2.1.mlp.fc1.bias density is 1.0
stage2.1.mlp.fc2.weight density is 1.0
stage2.1.mlp.fc2.bias density is 1.0
stage2.1.norm2.weight density is 1.0
stage2.1.norm2.bias density is 1.0
merge2.conv.weight density is 1.0
merge2.conv.bias density is 1.0
merge2.norm.weight density is 1.0
merge2.norm.bias density is 1.0
stage3.0.qkv.weight density is 1.0
stage3.0.qkv.bias density is 1.0
stage3.0.norm1.weight density is 1.0
stage3.0.norm1.bias density is 1.0
stage3.0.proj.weight density is 1.0
stage3.0.proj.bias density is 1.0
stage3.0.attns.0.get_v.weight density is 1.0
stage3.0.attns.0.get_v.bias density is 1.0
stage3.0.attns.1.get_v.weight density is 1.0
stage3.0.attns.1.get_v.bias density is 1.0
stage3.0.mlp.fc1.weight density is 1.0
stage3.0.mlp.fc1.bias density is 1.0
stage3.0.mlp.fc2.weight density is 1.0
stage3.0.mlp.fc2.bias density is 1.0
stage3.0.norm2.weight density is 1.0
stage3.0.norm2.bias density is 1.0
stage3.1.qkv.weight density is 1.0
stage3.1.qkv.bias density is 1.0
stage3.1.norm1.weight density is 1.0
stage3.1.norm1.bias density is 1.0
stage3.1.proj.weight density is 1.0
stage3.1.proj.bias density is 1.0
stage3.1.attns.0.get_v.weight density is 1.0
stage3.1.attns.0.get_v.bias density is 1.0
stage3.1.attns.1.get_v.weight density is 1.0
stage3.1.attns.1.get_v.bias density is 1.0
stage3.1.mlp.fc1.weight density is 1.0
stage3.1.mlp.fc1.bias density is 1.0
stage3.1.mlp.fc2.weight density is 1.0
stage3.1.mlp.fc2.bias density is 1.0
stage3.1.norm2.weight density is 1.0
stage3.1.norm2.bias density is 1.0
stage3.2.qkv.weight density is 1.0
stage3.2.qkv.bias density is 1.0
stage3.2.norm1.weight density is 1.0
stage3.2.norm1.bias density is 1.0
stage3.2.proj.weight density is 1.0
stage3.2.proj.bias density is 1.0
stage3.2.attns.0.get_v.weight density is 1.0
stage3.2.attns.0.get_v.bias density is 1.0
stage3.2.attns.1.get_v.weight density is 1.0
stage3.2.attns.1.get_v.bias density is 1.0
stage3.2.mlp.fc1.weight density is 1.0
stage3.2.mlp.fc1.bias density is 1.0
stage3.2.mlp.fc2.weight density is 1.0
stage3.2.mlp.fc2.bias density is 1.0
stage3.2.norm2.weight density is 1.0
stage3.2.norm2.bias density is 1.0
stage3.3.qkv.weight density is 1.0
stage3.3.qkv.bias density is 1.0
stage3.3.norm1.weight density is 1.0
stage3.3.norm1.bias density is 1.0
stage3.3.proj.weight density is 1.0
stage3.3.proj.bias density is 1.0
stage3.3.attns.0.get_v.weight density is 1.0
stage3.3.attns.0.get_v.bias density is 1.0
stage3.3.attns.1.get_v.weight density is 1.0
stage3.3.attns.1.get_v.bias density is 1.0
stage3.3.mlp.fc1.weight density is 1.0
stage3.3.mlp.fc1.bias density is 1.0
stage3.3.mlp.fc2.weight density is 1.0
stage3.3.mlp.fc2.bias density is 1.0
stage3.3.norm2.weight density is 1.0
stage3.3.norm2.bias density is 1.0
stage3.4.qkv.weight density is 1.0
stage3.4.qkv.bias density is 1.0
stage3.4.norm1.weight density is 1.0
stage3.4.norm1.bias density is 1.0
stage3.4.proj.weight density is 1.0
stage3.4.proj.bias density is 1.0
stage3.4.attns.0.get_v.weight density is 1.0
stage3.4.attns.0.get_v.bias density is 1.0
stage3.4.attns.1.get_v.weight density is 1.0
stage3.4.attns.1.get_v.bias density is 1.0
stage3.4.mlp.fc1.weight density is 1.0
stage3.4.mlp.fc1.bias density is 1.0
stage3.4.mlp.fc2.weight density is 1.0
stage3.4.mlp.fc2.bias density is 1.0
stage3.4.norm2.weight density is 1.0
stage3.4.norm2.bias density is 1.0
stage3.5.qkv.weight density is 1.0
stage3.5.qkv.bias density is 1.0
stage3.5.norm1.weight density is 1.0
stage3.5.norm1.bias density is 1.0
stage3.5.proj.weight density is 1.0
stage3.5.proj.bias density is 1.0
stage3.5.attns.0.get_v.weight density is 1.0
stage3.5.attns.0.get_v.bias density is 1.0
stage3.5.attns.1.get_v.weight density is 1.0
stage3.5.attns.1.get_v.bias density is 1.0
stage3.5.mlp.fc1.weight density is 1.0
stage3.5.mlp.fc1.bias density is 1.0
stage3.5.mlp.fc2.weight density is 1.0
stage3.5.mlp.fc2.bias density is 1.0
stage3.5.norm2.weight density is 1.0
stage3.5.norm2.bias density is 1.0
stage3.6.qkv.weight density is 1.0
stage3.6.qkv.bias density is 1.0
stage3.6.norm1.weight density is 1.0
stage3.6.norm1.bias density is 1.0
stage3.6.proj.weight density is 1.0
stage3.6.proj.bias density is 1.0
stage3.6.attns.0.get_v.weight density is 1.0
stage3.6.attns.0.get_v.bias density is 1.0
stage3.6.attns.1.get_v.weight density is 1.0
stage3.6.attns.1.get_v.bias density is 1.0
stage3.6.mlp.fc1.weight density is 1.0
stage3.6.mlp.fc1.bias density is 1.0
stage3.6.mlp.fc2.weight density is 1.0
stage3.6.mlp.fc2.bias density is 1.0
stage3.6.norm2.weight density is 1.0
stage3.6.norm2.bias density is 1.0
stage3.7.qkv.weight density is 1.0
stage3.7.qkv.bias density is 1.0
stage3.7.norm1.weight density is 1.0
stage3.7.norm1.bias density is 1.0
stage3.7.proj.weight density is 1.0
stage3.7.proj.bias density is 1.0
stage3.7.attns.0.get_v.weight density is 1.0
stage3.7.attns.0.get_v.bias density is 1.0
stage3.7.attns.1.get_v.weight density is 1.0
stage3.7.attns.1.get_v.bias density is 1.0
stage3.7.mlp.fc1.weight density is 1.0
stage3.7.mlp.fc1.bias density is 1.0
stage3.7.mlp.fc2.weight density is 1.0
stage3.7.mlp.fc2.bias density is 1.0
stage3.7.norm2.weight density is 1.0
stage3.7.norm2.bias density is 1.0
stage3.8.qkv.weight density is 1.0
stage3.8.qkv.bias density is 1.0
stage3.8.norm1.weight density is 1.0
stage3.8.norm1.bias density is 1.0
stage3.8.proj.weight density is 1.0
stage3.8.proj.bias density is 1.0
stage3.8.attns.0.get_v.weight density is 1.0
stage3.8.attns.0.get_v.bias density is 1.0
stage3.8.attns.1.get_v.weight density is 1.0
stage3.8.attns.1.get_v.bias density is 1.0
stage3.8.mlp.fc1.weight density is 1.0
stage3.8.mlp.fc1.bias density is 1.0
stage3.8.mlp.fc2.weight density is 1.0
stage3.8.mlp.fc2.bias density is 1.0
stage3.8.norm2.weight density is 1.0
stage3.8.norm2.bias density is 1.0
stage3.9.qkv.weight density is 1.0
stage3.9.qkv.bias density is 1.0
stage3.9.norm1.weight density is 1.0
stage3.9.norm1.bias density is 1.0
stage3.9.proj.weight density is 1.0
stage3.9.proj.bias density is 1.0
stage3.9.attns.0.get_v.weight density is 1.0
stage3.9.attns.0.get_v.bias density is 1.0
stage3.9.attns.1.get_v.weight density is 1.0
stage3.9.attns.1.get_v.bias density is 1.0
stage3.9.mlp.fc1.weight density is 1.0
stage3.9.mlp.fc1.bias density is 1.0
stage3.9.mlp.fc2.weight density is 1.0
stage3.9.mlp.fc2.bias density is 1.0
stage3.9.norm2.weight density is 1.0
stage3.9.norm2.bias density is 1.0
stage3.10.qkv.weight density is 1.0
stage3.10.qkv.bias density is 1.0
stage3.10.norm1.weight density is 1.0
stage3.10.norm1.bias density is 1.0
stage3.10.proj.weight density is 1.0
stage3.10.proj.bias density is 1.0
stage3.10.attns.0.get_v.weight density is 1.0
stage3.10.attns.0.get_v.bias density is 1.0
stage3.10.attns.1.get_v.weight density is 1.0
stage3.10.attns.1.get_v.bias density is 1.0
stage3.10.mlp.fc1.weight density is 1.0
stage3.10.mlp.fc1.bias density is 1.0
stage3.10.mlp.fc2.weight density is 1.0
stage3.10.mlp.fc2.bias density is 1.0
stage3.10.norm2.weight density is 1.0
stage3.10.norm2.bias density is 1.0
stage3.11.qkv.weight density is 1.0
stage3.11.qkv.bias density is 1.0
stage3.11.norm1.weight density is 1.0
stage3.11.norm1.bias density is 1.0
stage3.11.proj.weight density is 1.0
stage3.11.proj.bias density is 1.0
stage3.11.attns.0.get_v.weight density is 1.0
stage3.11.attns.0.get_v.bias density is 1.0
stage3.11.attns.1.get_v.weight density is 1.0
stage3.11.attns.1.get_v.bias density is 1.0
stage3.11.mlp.fc1.weight density is 1.0
stage3.11.mlp.fc1.bias density is 1.0
stage3.11.mlp.fc2.weight density is 1.0
stage3.11.mlp.fc2.bias density is 1.0
stage3.11.norm2.weight density is 1.0
stage3.11.norm2.bias density is 1.0
stage3.12.qkv.weight density is 1.0
stage3.12.qkv.bias density is 1.0
stage3.12.norm1.weight density is 1.0
stage3.12.norm1.bias density is 1.0
stage3.12.proj.weight density is 1.0
stage3.12.proj.bias density is 1.0
stage3.12.attns.0.get_v.weight density is 1.0
stage3.12.attns.0.get_v.bias density is 1.0
stage3.12.attns.1.get_v.weight density is 1.0
stage3.12.attns.1.get_v.bias density is 1.0
stage3.12.mlp.fc1.weight density is 1.0
stage3.12.mlp.fc1.bias density is 1.0
stage3.12.mlp.fc2.weight density is 1.0
stage3.12.mlp.fc2.bias density is 1.0
stage3.12.norm2.weight density is 1.0
stage3.12.norm2.bias density is 1.0
stage3.13.qkv.weight density is 1.0
stage3.13.qkv.bias density is 1.0
stage3.13.norm1.weight density is 1.0
stage3.13.norm1.bias density is 1.0
stage3.13.proj.weight density is 1.0
stage3.13.proj.bias density is 1.0
stage3.13.attns.0.get_v.weight density is 1.0
stage3.13.attns.0.get_v.bias density is 1.0
stage3.13.attns.1.get_v.weight density is 1.0
stage3.13.attns.1.get_v.bias density is 1.0
stage3.13.mlp.fc1.weight density is 1.0
stage3.13.mlp.fc1.bias density is 1.0
stage3.13.mlp.fc2.weight density is 1.0
stage3.13.mlp.fc2.bias density is 1.0
stage3.13.norm2.weight density is 1.0
stage3.13.norm2.bias density is 1.0
stage3.14.qkv.weight density is 1.0
stage3.14.qkv.bias density is 1.0
stage3.14.norm1.weight density is 1.0
stage3.14.norm1.bias density is 1.0
stage3.14.proj.weight density is 1.0
stage3.14.proj.bias density is 1.0
stage3.14.attns.0.get_v.weight density is 1.0
stage3.14.attns.0.get_v.bias density is 1.0
stage3.14.attns.1.get_v.weight density is 1.0
stage3.14.attns.1.get_v.bias density is 1.0
stage3.14.mlp.fc1.weight density is 1.0
stage3.14.mlp.fc1.bias density is 1.0
stage3.14.mlp.fc2.weight density is 1.0
stage3.14.mlp.fc2.bias density is 1.0
stage3.14.norm2.weight density is 1.0
stage3.14.norm2.bias density is 1.0
stage3.15.qkv.weight density is 1.0
stage3.15.qkv.bias density is 1.0
stage3.15.norm1.weight density is 1.0
stage3.15.norm1.bias density is 1.0
stage3.15.proj.weight density is 1.0
stage3.15.proj.bias density is 1.0
stage3.15.attns.0.get_v.weight density is 1.0
stage3.15.attns.0.get_v.bias density is 1.0
stage3.15.attns.1.get_v.weight density is 1.0
stage3.15.attns.1.get_v.bias density is 1.0
stage3.15.mlp.fc1.weight density is 1.0
stage3.15.mlp.fc1.bias density is 1.0
stage3.15.mlp.fc2.weight density is 1.0
stage3.15.mlp.fc2.bias density is 1.0
stage3.15.norm2.weight density is 1.0
stage3.15.norm2.bias density is 1.0
stage3.16.qkv.weight density is 1.0
stage3.16.qkv.bias density is 1.0
stage3.16.norm1.weight density is 1.0
stage3.16.norm1.bias density is 1.0
stage3.16.proj.weight density is 1.0
stage3.16.proj.bias density is 1.0
stage3.16.attns.0.get_v.weight density is 1.0
stage3.16.attns.0.get_v.bias density is 1.0
stage3.16.attns.1.get_v.weight density is 1.0
stage3.16.attns.1.get_v.bias density is 1.0
stage3.16.mlp.fc1.weight density is 1.0
stage3.16.mlp.fc1.bias density is 1.0
stage3.16.mlp.fc2.weight density is 1.0
stage3.16.mlp.fc2.bias density is 1.0
stage3.16.norm2.weight density is 1.0
stage3.16.norm2.bias density is 1.0
stage3.17.qkv.weight density is 1.0
stage3.17.qkv.bias density is 1.0
stage3.17.norm1.weight density is 1.0
stage3.17.norm1.bias density is 1.0
stage3.17.proj.weight density is 1.0
stage3.17.proj.bias density is 1.0
stage3.17.attns.0.get_v.weight density is 1.0
stage3.17.attns.0.get_v.bias density is 1.0
stage3.17.attns.1.get_v.weight density is 1.0
stage3.17.attns.1.get_v.bias density is 1.0
stage3.17.mlp.fc1.weight density is 1.0
stage3.17.mlp.fc1.bias density is 1.0
stage3.17.mlp.fc2.weight density is 1.0
stage3.17.mlp.fc2.bias density is 1.0
stage3.17.norm2.weight density is 1.0
stage3.17.norm2.bias density is 1.0
stage3.18.qkv.weight density is 1.0
stage3.18.qkv.bias density is 1.0
stage3.18.norm1.weight density is 1.0
stage3.18.norm1.bias density is 1.0
stage3.18.proj.weight density is 1.0
stage3.18.proj.bias density is 1.0
stage3.18.attns.0.get_v.weight density is 1.0
stage3.18.attns.0.get_v.bias density is 1.0
stage3.18.attns.1.get_v.weight density is 1.0
stage3.18.attns.1.get_v.bias density is 1.0
stage3.18.mlp.fc1.weight density is 1.0
stage3.18.mlp.fc1.bias density is 1.0
stage3.18.mlp.fc2.weight density is 1.0
stage3.18.mlp.fc2.bias density is 1.0
stage3.18.norm2.weight density is 1.0
stage3.18.norm2.bias density is 1.0
stage3.19.qkv.weight density is 1.0
stage3.19.qkv.bias density is 1.0
stage3.19.norm1.weight density is 1.0
stage3.19.norm1.bias density is 1.0
stage3.19.proj.weight density is 1.0
stage3.19.proj.bias density is 1.0
stage3.19.attns.0.get_v.weight density is 1.0
stage3.19.attns.0.get_v.bias density is 1.0
stage3.19.attns.1.get_v.weight density is 1.0
stage3.19.attns.1.get_v.bias density is 1.0
stage3.19.mlp.fc1.weight density is 1.0
stage3.19.mlp.fc1.bias density is 1.0
stage3.19.mlp.fc2.weight density is 1.0
stage3.19.mlp.fc2.bias density is 1.0
stage3.19.norm2.weight density is 1.0
stage3.19.norm2.bias density is 1.0
stage3.20.qkv.weight density is 1.0
stage3.20.qkv.bias density is 1.0
stage3.20.norm1.weight density is 1.0
stage3.20.norm1.bias density is 1.0
stage3.20.proj.weight density is 1.0
stage3.20.proj.bias density is 1.0
stage3.20.attns.0.get_v.weight density is 1.0
stage3.20.attns.0.get_v.bias density is 1.0
stage3.20.attns.1.get_v.weight density is 1.0
stage3.20.attns.1.get_v.bias density is 1.0
stage3.20.mlp.fc1.weight density is 1.0
stage3.20.mlp.fc1.bias density is 1.0
stage3.20.mlp.fc2.weight density is 1.0
stage3.20.mlp.fc2.bias density is 1.0
stage3.20.norm2.weight density is 1.0
stage3.20.norm2.bias density is 1.0
merge3.conv.weight density is 1.0
merge3.conv.bias density is 1.0
merge3.norm.weight density is 1.0
merge3.norm.bias density is 1.0
stage4.0.qkv.weight density is 1.0
stage4.0.qkv.bias density is 1.0
stage4.0.norm1.weight density is 1.0
stage4.0.norm1.bias density is 1.0
stage4.0.proj.weight density is 1.0
stage4.0.proj.bias density is 1.0
stage4.0.attns.0.get_v.weight density is 1.0
stage4.0.attns.0.get_v.bias density is 1.0
stage4.0.mlp.fc1.weight density is 1.0
stage4.0.mlp.fc1.bias density is 1.0
stage4.0.mlp.fc2.weight density is 1.0
stage4.0.mlp.fc2.bias density is 1.0
stage4.0.norm2.weight density is 1.0
stage4.0.norm2.bias density is 1.0
norm.weight density is 1.0
norm.bias density is 1.0
head.weight density is 1.0
head.bias density is 1.0
Test:  [  0/531]  eta: 0:36:10  loss: 4.9601 (4.9601)  acc1: 15.6250 (15.6250)  acc5: 43.7500 (43.7500)  time: 4.0868  data: 3.0704  max mem: 1393
Test:  [ 10/531]  eta: 0:04:26  loss: 4.2109 (3.9088)  acc1: 30.2083 (34.8485)  acc5: 50.0000 (54.0720)  time: 0.5115  data: 0.3428  max mem: 1393
Test:  [ 20/531]  eta: 0:02:46  loss: 3.7552 (3.8097)  acc1: 35.4167 (33.9286)  acc5: 55.2083 (56.2500)  time: 0.1369  data: 0.0535  max mem: 1393
Test:  [ 30/531]  eta: 0:02:21  loss: 4.0819 (4.3289)  acc1: 26.0417 (29.5027)  acc5: 45.8333 (49.5968)  time: 0.1557  data: 0.0727  max mem: 1393
Test:  [ 40/531]  eta: 0:02:01  loss: 4.6229 (4.3213)  acc1: 17.7083 (29.4461)  acc5: 36.4583 (50.2287)  time: 0.1652  data: 0.0823  max mem: 1393
Test:  [ 50/531]  eta: 0:01:54  loss: 3.8418 (4.2154)  acc1: 29.1667 (30.7394)  acc5: 51.0417 (51.4910)  time: 0.1675  data: 0.0846  max mem: 1393
Test:  [ 60/531]  eta: 0:01:48  loss: 3.8953 (4.1980)  acc1: 35.4167 (31.3695)  acc5: 53.1250 (51.5198)  time: 0.1944  data: 0.1115  max mem: 1393
Test:  [ 70/531]  eta: 0:01:58  loss: 3.7598 (4.1294)  acc1: 34.3750 (31.3527)  acc5: 55.2083 (52.2447)  time: 0.3098  data: 0.2251  max mem: 1393
Test:  [ 80/531]  eta: 0:01:55  loss: 3.7456 (4.1042)  acc1: 30.2083 (31.6487)  acc5: 54.1667 (52.3791)  time: 0.3328  data: 0.2480  max mem: 1393
Test:  [ 90/531]  eta: 0:01:46  loss: 3.2516 (3.9906)  acc1: 34.3750 (32.8640)  acc5: 58.3333 (53.9835)  time: 0.1892  data: 0.1059  max mem: 1393
Test:  [100/531]  eta: 0:01:39  loss: 2.8545 (3.8871)  acc1: 43.7500 (33.9109)  acc5: 70.8333 (55.4559)  time: 0.1363  data: 0.0529  max mem: 1393
Test:  [110/531]  eta: 0:01:35  loss: 2.8638 (3.7984)  acc1: 36.4583 (34.4501)  acc5: 71.8750 (57.1040)  time: 0.1523  data: 0.0693  max mem: 1393
Test:  [120/531]  eta: 0:01:28  loss: 3.0530 (3.7427)  acc1: 38.5417 (34.9346)  acc5: 72.9167 (57.9029)  time: 0.1352  data: 0.0522  max mem: 1393
Test:  [130/531]  eta: 0:01:25  loss: 3.0619 (3.7006)  acc1: 39.5833 (35.4962)  acc5: 67.7083 (58.4685)  time: 0.1443  data: 0.0613  max mem: 1393
Test:  [140/531]  eta: 0:01:22  loss: 2.7693 (3.6371)  acc1: 41.6667 (36.2293)  acc5: 67.7083 (59.3011)  time: 0.1738  data: 0.0909  max mem: 1393
Test:  [150/531]  eta: 0:01:20  loss: 2.9171 (3.6435)  acc1: 30.2083 (36.0789)  acc5: 65.6250 (59.2301)  time: 0.1905  data: 0.1076  max mem: 1393
Test:  [160/531]  eta: 0:01:18  loss: 3.3342 (3.6361)  acc1: 36.4583 (36.4325)  acc5: 58.3333 (59.3944)  time: 0.2259  data: 0.1429  max mem: 1393
Test:  [170/531]  eta: 0:01:14  loss: 3.4672 (3.6463)  acc1: 37.5000 (36.5253)  acc5: 60.4167 (59.3506)  time: 0.1821  data: 0.0991  max mem: 1393
Test:  [180/531]  eta: 0:01:11  loss: 3.7514 (3.6576)  acc1: 37.5000 (36.6137)  acc5: 57.2917 (59.1218)  time: 0.1492  data: 0.0663  max mem: 1393
Test:  [190/531]  eta: 0:01:09  loss: 3.7767 (3.6554)  acc1: 37.5000 (36.8128)  acc5: 56.2500 (59.1787)  time: 0.1863  data: 0.1034  max mem: 1393
Test:  [200/531]  eta: 0:01:07  loss: 3.4578 (3.6617)  acc1: 37.5000 (36.6501)  acc5: 56.2500 (58.9138)  time: 0.1910  data: 0.1081  max mem: 1393
Test:  [210/531]  eta: 0:01:05  loss: 3.8776 (3.7012)  acc1: 30.2083 (36.3448)  acc5: 52.0833 (58.3185)  time: 0.1820  data: 0.0991  max mem: 1393
Test:  [220/531]  eta: 0:01:03  loss: 3.8776 (3.7086)  acc1: 34.3750 (36.4583)  acc5: 52.0833 (58.0788)  time: 0.2183  data: 0.1354  max mem: 1393
Test:  [230/531]  eta: 0:01:00  loss: 3.6528 (3.7236)  acc1: 36.4583 (36.2509)  acc5: 56.2500 (57.8057)  time: 0.1988  data: 0.1159  max mem: 1393
Test:  [240/531]  eta: 0:00:58  loss: 3.5965 (3.7407)  acc1: 35.4167 (36.0564)  acc5: 52.0833 (57.4646)  time: 0.1687  data: 0.0858  max mem: 1393
Test:  [250/531]  eta: 0:00:57  loss: 3.7997 (3.7769)  acc1: 28.1250 (35.7030)  acc5: 51.0417 (56.8974)  time: 0.2141  data: 0.1312  max mem: 1393
Test:  [260/531]  eta: 0:00:54  loss: 3.8153 (3.7716)  acc1: 28.1250 (35.6362)  acc5: 48.9583 (56.8646)  time: 0.2120  data: 0.1291  max mem: 1393
Test:  [270/531]  eta: 0:00:52  loss: 3.3847 (3.7648)  acc1: 39.5833 (35.8203)  acc5: 57.2917 (56.8765)  time: 0.1885  data: 0.1056  max mem: 1393
Test:  [280/531]  eta: 0:00:50  loss: 3.3293 (3.7436)  acc1: 43.7500 (36.1803)  acc5: 58.3333 (57.0581)  time: 0.2069  data: 0.1239  max mem: 1393
Test:  [290/531]  eta: 0:00:48  loss: 3.7244 (3.7535)  acc1: 35.4167 (36.0180)  acc5: 52.0833 (56.9051)  time: 0.1780  data: 0.0950  max mem: 1393
Test:  [300/531]  eta: 0:00:46  loss: 3.7244 (3.7508)  acc1: 31.2500 (36.1053)  acc5: 53.1250 (56.8902)  time: 0.1626  data: 0.0797  max mem: 1393
Test:  [310/531]  eta: 0:00:44  loss: 3.5702 (3.7516)  acc1: 41.6667 (36.2674)  acc5: 54.1667 (56.8060)  time: 0.2105  data: 0.1276  max mem: 1393
Test:  [320/531]  eta: 0:00:42  loss: 3.6072 (3.7482)  acc1: 41.6667 (36.4097)  acc5: 53.1250 (56.7692)  time: 0.2205  data: 0.1373  max mem: 1393
Test:  [330/531]  eta: 0:00:40  loss: 3.0975 (3.7327)  acc1: 38.5417 (36.4898)  acc5: 55.2083 (56.9046)  time: 0.1656  data: 0.0823  max mem: 1393
Test:  [340/531]  eta: 0:00:37  loss: 3.5622 (3.7459)  acc1: 31.2500 (36.1651)  acc5: 56.2500 (56.7296)  time: 0.1517  data: 0.0684  max mem: 1393
Test:  [350/531]  eta: 0:00:36  loss: 4.1112 (3.7558)  acc1: 25.0000 (36.0221)  acc5: 53.1250 (56.5557)  time: 0.1972  data: 0.1139  max mem: 1393
Test:  [360/531]  eta: 0:00:33  loss: 4.1112 (3.7657)  acc1: 25.0000 (35.7370)  acc5: 52.0833 (56.3654)  time: 0.1908  data: 0.1079  max mem: 1393
Test:  [370/531]  eta: 0:00:32  loss: 4.2033 (3.7849)  acc1: 21.8750 (35.5065)  acc5: 50.0000 (56.0422)  time: 0.2128  data: 0.1285  max mem: 1393
Test:  [380/531]  eta: 0:00:30  loss: 3.6684 (3.7829)  acc1: 27.0833 (35.5780)  acc5: 56.2500 (56.0422)  time: 0.2246  data: 0.1403  max mem: 1393
Test:  [390/531]  eta: 0:00:28  loss: 4.0435 (3.8107)  acc1: 27.0833 (35.3394)  acc5: 46.8750 (55.6106)  time: 0.1819  data: 0.0990  max mem: 1393
Test:  [400/531]  eta: 0:00:26  loss: 4.5939 (3.8203)  acc1: 26.0417 (35.2530)  acc5: 41.6667 (55.4110)  time: 0.1897  data: 0.1068  max mem: 1393
Test:  [410/531]  eta: 0:00:23  loss: 3.7015 (3.8153)  acc1: 35.4167 (35.3964)  acc5: 51.0417 (55.4440)  time: 0.1778  data: 0.0949  max mem: 1393
Test:  [420/531]  eta: 0:00:21  loss: 3.9041 (3.8233)  acc1: 36.4583 (35.3474)  acc5: 48.9583 (55.3123)  time: 0.1595  data: 0.0765  max mem: 1393
Test:  [430/531]  eta: 0:00:19  loss: 4.3171 (3.8351)  acc1: 28.1250 (35.2306)  acc5: 45.8333 (55.1552)  time: 0.1619  data: 0.0789  max mem: 1393
Test:  [440/531]  eta: 0:00:17  loss: 3.5773 (3.8279)  acc1: 33.3333 (35.3033)  acc5: 55.2083 (55.2131)  time: 0.1973  data: 0.1144  max mem: 1393
Test:  [450/531]  eta: 0:00:16  loss: 3.1465 (3.8166)  acc1: 41.6667 (35.4028)  acc5: 61.4583 (55.3654)  time: 0.2572  data: 0.1728  max mem: 1393
Test:  [460/531]  eta: 0:00:14  loss: 3.9507 (3.8260)  acc1: 32.2917 (35.3059)  acc5: 53.1250 (55.1699)  time: 0.2193  data: 0.1348  max mem: 1393
Test:  [470/531]  eta: 0:00:11  loss: 4.0131 (3.8191)  acc1: 32.2917 (35.4919)  acc5: 50.0000 (55.2194)  time: 0.1465  data: 0.0636  max mem: 1393
Test:  [480/531]  eta: 0:00:09  loss: 3.3668 (3.8169)  acc1: 33.3333 (35.3495)  acc5: 55.2083 (55.2906)  time: 0.1482  data: 0.0653  max mem: 1393
Test:  [490/531]  eta: 0:00:08  loss: 3.9657 (3.8367)  acc1: 23.9583 (35.1684)  acc5: 52.0833 (55.0174)  time: 0.2305  data: 0.1476  max mem: 1393
Test:  [500/531]  eta: 0:00:06  loss: 5.8907 (3.8932)  acc1: 18.7500 (34.7305)  acc5: 25.0000 (54.2977)  time: 0.2583  data: 0.1750  max mem: 1393
Test:  [510/531]  eta: 0:00:04  loss: 6.2233 (3.9390)  acc1: 10.4167 (34.3179)  acc5: 16.6667 (53.6815)  time: 0.2099  data: 0.1267  max mem: 1393
Test:  [520/531]  eta: 0:00:02  loss: 5.7193 (3.9760)  acc1: 11.4583 (33.9531)  acc5: 22.9167 (53.1630)  time: 0.2043  data: 0.1214  max mem: 1393
Test:  [530/531]  eta: 0:00:00  loss: 5.6274 (4.0190)  acc1: 10.4167 (33.5259)  acc5: 23.9583 (52.6027)  time: 0.2065  data: 0.1260  max mem: 1393
Test: Total time: 0:01:45 (0.1990 s / it)
* Acc@1 33.526 Acc@5 52.603 loss 4.019
Accuracy of the network on 50889 test images: 33.52591%
Not using distributed mode
Namespace(Decom=False, aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=64, bn=True, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/sliu/project_space/datasets/sketch', data_set='IMNET', data_type='imagenet-s', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.1, enable_wandb=False, epochs=300, eval=True, eval_data_path=None, finetune='', fix=False, growth='random', head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, kernel_size=[7, 7, 7, 7, 100], layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=-1, log_dir=None, lr=0.004, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='convnext', model_ema=False, model_ema_decay=0.9999, model_ema_eval=False, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_workers=10, only_L=False, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', parallel=True, pin_mem=True, project='SLaK', prune='magnitude', prune_rate=0.3, recount=1, redistribution='none', remode='pixel', reprob=0.25, resplit=False, resume='/gpfs/work3/0/prjste21060/projects/datasets/checkpoints/convnext300epoch/checkpoint-best.pth', save_ckpt=True, save_ckpt_freq=10, save_ckpt_num=3, seed=0, smoothing=0.1, sparse=False, sparse_init='snip', sparsity=0.4, start_epoch=0, train_interpolation='bicubic', update_freq=1, update_frequency=100, use_amp=False, verbose=False, wandb_ckpt=False, warmup_epochs=20, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, width_factor=1.0, world_size=1)
Transform = 
Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /home/sliu/project_space/datasets/sketch
Number of the class = 1000
Mixup is activated!
Model = ConvNeXt(
  (downsample_layers): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (1): LayerNorm()
    )
    (1): Sequential(
      (0): LayerNorm()
      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))
    )
    (2): Sequential(
      (0): LayerNorm()
      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))
    )
    (3): Sequential(
      (0): LayerNorm()
      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (stages): ModuleList(
    (0): Sequential(
      (0): Block(
        (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): Identity()
      )
      (1): Block(
        (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (dwconv): Conv2d(96, 96, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=96)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): DropPath()
      )
    )
    (1): Sequential(
      (0): Block(
        (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (dwconv): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
    )
    (2): Sequential(
      (0): Block(
        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (3): Block(
        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (4): Block(
        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (5): Block(
        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (6): Block(
        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (7): Block(
        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (8): Block(
        (dwconv): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
    )
    (3): Sequential(
      (0): Block(
        (dwconv): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (dwconv): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (dwconv): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768)
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
number of params: 28589128
LR = 0.00400000
Batch size = 64
Update frequent = 1
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "downsample_layers.0.0.weight",
      "downsample_layers.1.1.weight",
      "downsample_layers.2.1.weight",
      "downsample_layers.3.1.weight",
      "stages.0.0.dwconv.weight",
      "stages.0.0.pwconv1.weight",
      "stages.0.0.pwconv2.weight",
      "stages.0.1.dwconv.weight",
      "stages.0.1.pwconv1.weight",
      "stages.0.1.pwconv2.weight",
      "stages.0.2.dwconv.weight",
      "stages.0.2.pwconv1.weight",
      "stages.0.2.pwconv2.weight",
      "stages.1.0.dwconv.weight",
      "stages.1.0.pwconv1.weight",
      "stages.1.0.pwconv2.weight",
      "stages.1.1.dwconv.weight",
      "stages.1.1.pwconv1.weight",
      "stages.1.1.pwconv2.weight",
      "stages.1.2.dwconv.weight",
      "stages.1.2.pwconv1.weight",
      "stages.1.2.pwconv2.weight",
      "stages.2.0.dwconv.weight",
      "stages.2.0.pwconv1.weight",
      "stages.2.0.pwconv2.weight",
      "stages.2.1.dwconv.weight",
      "stages.2.1.pwconv1.weight",
      "stages.2.1.pwconv2.weight",
      "stages.2.2.dwconv.weight",
      "stages.2.2.pwconv1.weight",
      "stages.2.2.pwconv2.weight",
      "stages.2.3.dwconv.weight",
      "stages.2.3.pwconv1.weight",
      "stages.2.3.pwconv2.weight",
      "stages.2.4.dwconv.weight",
      "stages.2.4.pwconv1.weight",
      "stages.2.4.pwconv2.weight",
      "stages.2.5.dwconv.weight",
      "stages.2.5.pwconv1.weight",
      "stages.2.5.pwconv2.weight",
      "stages.2.6.dwconv.weight",
      "stages.2.6.pwconv1.weight",
      "stages.2.6.pwconv2.weight",
      "stages.2.7.dwconv.weight",
      "stages.2.7.pwconv1.weight",
      "stages.2.7.pwconv2.weight",
      "stages.2.8.dwconv.weight",
      "stages.2.8.pwconv1.weight",
      "stages.2.8.pwconv2.weight",
      "stages.3.0.dwconv.weight",
      "stages.3.0.pwconv1.weight",
      "stages.3.0.pwconv2.weight",
      "stages.3.1.dwconv.weight",
      "stages.3.1.pwconv1.weight",
      "stages.3.1.pwconv2.weight",
      "stages.3.2.dwconv.weight",
      "stages.3.2.pwconv1.weight",
      "stages.3.2.pwconv2.weight",
      "head.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "downsample_layers.0.0.bias",
      "downsample_layers.0.1.weight",
      "downsample_layers.0.1.bias",
      "downsample_layers.1.0.weight",
      "downsample_layers.1.0.bias",
      "downsample_layers.1.1.bias",
      "downsample_layers.2.0.weight",
      "downsample_layers.2.0.bias",
      "downsample_layers.2.1.bias",
      "downsample_layers.3.0.weight",
      "downsample_layers.3.0.bias",
      "downsample_layers.3.1.bias",
      "stages.0.0.gamma",
      "stages.0.0.dwconv.bias",
      "stages.0.0.norm.weight",
      "stages.0.0.norm.bias",
      "stages.0.0.pwconv1.bias",
      "stages.0.0.pwconv2.bias",
      "stages.0.1.gamma",
      "stages.0.1.dwconv.bias",
      "stages.0.1.norm.weight",
      "stages.0.1.norm.bias",
      "stages.0.1.pwconv1.bias",
      "stages.0.1.pwconv2.bias",
      "stages.0.2.gamma",
      "stages.0.2.dwconv.bias",
      "stages.0.2.norm.weight",
      "stages.0.2.norm.bias",
      "stages.0.2.pwconv1.bias",
      "stages.0.2.pwconv2.bias",
      "stages.1.0.gamma",
      "stages.1.0.dwconv.bias",
      "stages.1.0.norm.weight",
      "stages.1.0.norm.bias",
      "stages.1.0.pwconv1.bias",
      "stages.1.0.pwconv2.bias",
      "stages.1.1.gamma",
      "stages.1.1.dwconv.bias",
      "stages.1.1.norm.weight",
      "stages.1.1.norm.bias",
      "stages.1.1.pwconv1.bias",
      "stages.1.1.pwconv2.bias",
      "stages.1.2.gamma",
      "stages.1.2.dwconv.bias",
      "stages.1.2.norm.weight",
      "stages.1.2.norm.bias",
      "stages.1.2.pwconv1.bias",
      "stages.1.2.pwconv2.bias",
      "stages.2.0.gamma",
      "stages.2.0.dwconv.bias",
      "stages.2.0.norm.weight",
      "stages.2.0.norm.bias",
      "stages.2.0.pwconv1.bias",
      "stages.2.0.pwconv2.bias",
      "stages.2.1.gamma",
      "stages.2.1.dwconv.bias",
      "stages.2.1.norm.weight",
      "stages.2.1.norm.bias",
      "stages.2.1.pwconv1.bias",
      "stages.2.1.pwconv2.bias",
      "stages.2.2.gamma",
      "stages.2.2.dwconv.bias",
      "stages.2.2.norm.weight",
      "stages.2.2.norm.bias",
      "stages.2.2.pwconv1.bias",
      "stages.2.2.pwconv2.bias",
      "stages.2.3.gamma",
      "stages.2.3.dwconv.bias",
      "stages.2.3.norm.weight",
      "stages.2.3.norm.bias",
      "stages.2.3.pwconv1.bias",
      "stages.2.3.pwconv2.bias",
      "stages.2.4.gamma",
      "stages.2.4.dwconv.bias",
      "stages.2.4.norm.weight",
      "stages.2.4.norm.bias",
      "stages.2.4.pwconv1.bias",
      "stages.2.4.pwconv2.bias",
      "stages.2.5.gamma",
      "stages.2.5.dwconv.bias",
      "stages.2.5.norm.weight",
      "stages.2.5.norm.bias",
      "stages.2.5.pwconv1.bias",
      "stages.2.5.pwconv2.bias",
      "stages.2.6.gamma",
      "stages.2.6.dwconv.bias",
      "stages.2.6.norm.weight",
      "stages.2.6.norm.bias",
      "stages.2.6.pwconv1.bias",
      "stages.2.6.pwconv2.bias",
      "stages.2.7.gamma",
      "stages.2.7.dwconv.bias",
      "stages.2.7.norm.weight",
      "stages.2.7.norm.bias",
      "stages.2.7.pwconv1.bias",
      "stages.2.7.pwconv2.bias",
      "stages.2.8.gamma",
      "stages.2.8.dwconv.bias",
      "stages.2.8.norm.weight",
      "stages.2.8.norm.bias",
      "stages.2.8.pwconv1.bias",
      "stages.2.8.pwconv2.bias",
      "stages.3.0.gamma",
      "stages.3.0.dwconv.bias",
      "stages.3.0.norm.weight",
      "stages.3.0.norm.bias",
      "stages.3.0.pwconv1.bias",
      "stages.3.0.pwconv2.bias",
      "stages.3.1.gamma",
      "stages.3.1.dwconv.bias",
      "stages.3.1.norm.weight",
      "stages.3.1.norm.bias",
      "stages.3.1.pwconv1.bias",
      "stages.3.1.pwconv2.bias",
      "stages.3.2.gamma",
      "stages.3.2.dwconv.bias",
      "stages.3.2.norm.weight",
      "stages.3.2.norm.bias",
      "stages.3.2.pwconv1.bias",
      "stages.3.2.pwconv2.bias",
      "norm.weight",
      "norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
criterion = SoftTargetCrossEntropy()
dict_keys(['model'])
Resume checkpoint /gpfs/work3/0/prjste21060/projects/datasets/checkpoints/convnext300epoch/checkpoint-best.pth
downsample_layers.0.0.weight density is 1.0
downsample_layers.0.0.bias density is 1.0
downsample_layers.0.1.weight density is 1.0
downsample_layers.0.1.bias density is 1.0
downsample_layers.1.0.weight density is 1.0
downsample_layers.1.0.bias density is 1.0
downsample_layers.1.1.weight density is 1.0
downsample_layers.1.1.bias density is 1.0
downsample_layers.2.0.weight density is 1.0
downsample_layers.2.0.bias density is 1.0
downsample_layers.2.1.weight density is 1.0
downsample_layers.2.1.bias density is 1.0
downsample_layers.3.0.weight density is 1.0
downsample_layers.3.0.bias density is 1.0
downsample_layers.3.1.weight density is 1.0
downsample_layers.3.1.bias density is 1.0
stages.0.0.gamma density is 1.0
stages.0.0.dwconv.weight density is 1.0
stages.0.0.dwconv.bias density is 1.0
stages.0.0.norm.weight density is 1.0
stages.0.0.norm.bias density is 1.0
stages.0.0.pwconv1.weight density is 1.0
stages.0.0.pwconv1.bias density is 1.0
stages.0.0.pwconv2.weight density is 1.0
stages.0.0.pwconv2.bias density is 1.0
stages.0.1.gamma density is 1.0
stages.0.1.dwconv.weight density is 1.0
stages.0.1.dwconv.bias density is 1.0
stages.0.1.norm.weight density is 1.0
stages.0.1.norm.bias density is 1.0
stages.0.1.pwconv1.weight density is 1.0
stages.0.1.pwconv1.bias density is 1.0
stages.0.1.pwconv2.weight density is 1.0
stages.0.1.pwconv2.bias density is 1.0
stages.0.2.gamma density is 1.0
stages.0.2.dwconv.weight density is 1.0
stages.0.2.dwconv.bias density is 1.0
stages.0.2.norm.weight density is 1.0
stages.0.2.norm.bias density is 1.0
stages.0.2.pwconv1.weight density is 1.0
stages.0.2.pwconv1.bias density is 1.0
stages.0.2.pwconv2.weight density is 1.0
stages.0.2.pwconv2.bias density is 1.0
stages.1.0.gamma density is 1.0
stages.1.0.dwconv.weight density is 1.0
stages.1.0.dwconv.bias density is 1.0
stages.1.0.norm.weight density is 1.0
stages.1.0.norm.bias density is 1.0
stages.1.0.pwconv1.weight density is 1.0
stages.1.0.pwconv1.bias density is 1.0
stages.1.0.pwconv2.weight density is 1.0
stages.1.0.pwconv2.bias density is 1.0
stages.1.1.gamma density is 1.0
stages.1.1.dwconv.weight density is 1.0
stages.1.1.dwconv.bias density is 1.0
stages.1.1.norm.weight density is 1.0
stages.1.1.norm.bias density is 1.0
stages.1.1.pwconv1.weight density is 1.0
stages.1.1.pwconv1.bias density is 1.0
stages.1.1.pwconv2.weight density is 1.0
stages.1.1.pwconv2.bias density is 1.0
stages.1.2.gamma density is 1.0
stages.1.2.dwconv.weight density is 1.0
stages.1.2.dwconv.bias density is 1.0
stages.1.2.norm.weight density is 1.0
stages.1.2.norm.bias density is 1.0
stages.1.2.pwconv1.weight density is 1.0
stages.1.2.pwconv1.bias density is 1.0
stages.1.2.pwconv2.weight density is 1.0
stages.1.2.pwconv2.bias density is 1.0
stages.2.0.gamma density is 1.0
stages.2.0.dwconv.weight density is 1.0
stages.2.0.dwconv.bias density is 1.0
stages.2.0.norm.weight density is 1.0
stages.2.0.norm.bias density is 1.0
stages.2.0.pwconv1.weight density is 1.0
stages.2.0.pwconv1.bias density is 1.0
stages.2.0.pwconv2.weight density is 1.0
stages.2.0.pwconv2.bias density is 1.0
stages.2.1.gamma density is 1.0
stages.2.1.dwconv.weight density is 1.0
stages.2.1.dwconv.bias density is 1.0
stages.2.1.norm.weight density is 1.0
stages.2.1.norm.bias density is 1.0
stages.2.1.pwconv1.weight density is 1.0
stages.2.1.pwconv1.bias density is 1.0
stages.2.1.pwconv2.weight density is 1.0
stages.2.1.pwconv2.bias density is 1.0
stages.2.2.gamma density is 1.0
stages.2.2.dwconv.weight density is 1.0
stages.2.2.dwconv.bias density is 1.0
stages.2.2.norm.weight density is 1.0
stages.2.2.norm.bias density is 1.0
stages.2.2.pwconv1.weight density is 1.0
stages.2.2.pwconv1.bias density is 1.0
stages.2.2.pwconv2.weight density is 1.0
stages.2.2.pwconv2.bias density is 1.0
stages.2.3.gamma density is 1.0
stages.2.3.dwconv.weight density is 1.0
stages.2.3.dwconv.bias density is 1.0
stages.2.3.norm.weight density is 1.0
stages.2.3.norm.bias density is 1.0
stages.2.3.pwconv1.weight density is 1.0
stages.2.3.pwconv1.bias density is 1.0
stages.2.3.pwconv2.weight density is 1.0
stages.2.3.pwconv2.bias density is 1.0
stages.2.4.gamma density is 1.0
stages.2.4.dwconv.weight density is 1.0
stages.2.4.dwconv.bias density is 1.0
stages.2.4.norm.weight density is 1.0
stages.2.4.norm.bias density is 1.0
stages.2.4.pwconv1.weight density is 1.0
stages.2.4.pwconv1.bias density is 1.0
stages.2.4.pwconv2.weight density is 1.0
stages.2.4.pwconv2.bias density is 1.0
stages.2.5.gamma density is 1.0
stages.2.5.dwconv.weight density is 1.0
stages.2.5.dwconv.bias density is 1.0
stages.2.5.norm.weight density is 1.0
stages.2.5.norm.bias density is 1.0
stages.2.5.pwconv1.weight density is 1.0
stages.2.5.pwconv1.bias density is 1.0
stages.2.5.pwconv2.weight density is 1.0
stages.2.5.pwconv2.bias density is 1.0
stages.2.6.gamma density is 1.0
stages.2.6.dwconv.weight density is 1.0
stages.2.6.dwconv.bias density is 1.0
stages.2.6.norm.weight density is 1.0
stages.2.6.norm.bias density is 1.0
stages.2.6.pwconv1.weight density is 1.0
stages.2.6.pwconv1.bias density is 1.0
stages.2.6.pwconv2.weight density is 1.0
stages.2.6.pwconv2.bias density is 1.0
stages.2.7.gamma density is 1.0
stages.2.7.dwconv.weight density is 1.0
stages.2.7.dwconv.bias density is 1.0
stages.2.7.norm.weight density is 1.0
stages.2.7.norm.bias density is 1.0
stages.2.7.pwconv1.weight density is 1.0
stages.2.7.pwconv1.bias density is 1.0
stages.2.7.pwconv2.weight density is 1.0
stages.2.7.pwconv2.bias density is 1.0
stages.2.8.gamma density is 1.0
stages.2.8.dwconv.weight density is 1.0
stages.2.8.dwconv.bias density is 1.0
stages.2.8.norm.weight density is 1.0
stages.2.8.norm.bias density is 1.0
stages.2.8.pwconv1.weight density is 1.0
stages.2.8.pwconv1.bias density is 1.0
stages.2.8.pwconv2.weight density is 1.0
stages.2.8.pwconv2.bias density is 1.0
stages.3.0.gamma density is 1.0
stages.3.0.dwconv.weight density is 1.0
stages.3.0.dwconv.bias density is 1.0
stages.3.0.norm.weight density is 1.0
stages.3.0.norm.bias density is 1.0
stages.3.0.pwconv1.weight density is 1.0
stages.3.0.pwconv1.bias density is 1.0
stages.3.0.pwconv2.weight density is 1.0
stages.3.0.pwconv2.bias density is 1.0
stages.3.1.gamma density is 1.0
stages.3.1.dwconv.weight density is 1.0
stages.3.1.dwconv.bias density is 1.0
stages.3.1.norm.weight density is 1.0
stages.3.1.norm.bias density is 1.0
stages.3.1.pwconv1.weight density is 1.0
stages.3.1.pwconv1.bias density is 1.0
stages.3.1.pwconv2.weight density is 1.0
stages.3.1.pwconv2.bias density is 1.0
stages.3.2.gamma density is 1.0
stages.3.2.dwconv.weight density is 1.0
stages.3.2.dwconv.bias density is 1.0
stages.3.2.norm.weight density is 1.0
stages.3.2.norm.bias density is 1.0
stages.3.2.pwconv1.weight density is 1.0
stages.3.2.pwconv1.bias density is 1.0
stages.3.2.pwconv2.weight density is 1.0
stages.3.2.pwconv2.bias density is 1.0
norm.weight density is 1.0
norm.bias density is 1.0
head.weight density is 1.0
head.bias density is 1.0
Test:  [  0/531]  eta: 0:34:35  loss: 5.2095 (5.2095)  acc1: 12.5000 (12.5000)  acc5: 36.4583 (36.4583)  time: 3.9080  data: 2.9160  max mem: 2112
Test:  [ 10/531]  eta: 0:04:19  loss: 4.4931 (3.9347)  acc1: 30.2083 (33.5227)  acc5: 46.8750 (52.7462)  time: 0.4975  data: 0.3461  max mem: 2112
Test:  [ 20/531]  eta: 0:02:42  loss: 3.4867 (3.8304)  acc1: 28.1250 (31.7460)  acc5: 55.2083 (55.0099)  time: 0.1393  data: 0.0735  max mem: 2112
Test:  [ 30/531]  eta: 0:02:15  loss: 3.9425 (4.2589)  acc1: 23.9583 (27.9234)  acc5: 51.0417 (49.6976)  time: 0.1445  data: 0.0806  max mem: 2112
Test:  [ 40/531]  eta: 0:01:58  loss: 4.2609 (4.2058)  acc1: 22.9167 (28.4299)  acc5: 44.7917 (50.6606)  time: 0.1592  data: 0.0957  max mem: 2112
Test:  [ 50/531]  eta: 0:01:54  loss: 3.5444 (4.1249)  acc1: 32.2917 (30.1471)  acc5: 47.9167 (51.5727)  time: 0.1879  data: 0.1232  max mem: 2112
Test:  [ 60/531]  eta: 0:01:50  loss: 4.1249 (4.1293)  acc1: 34.3750 (30.6011)  acc5: 50.0000 (51.2978)  time: 0.2186  data: 0.1539  max mem: 2112
Test:  [ 70/531]  eta: 0:02:00  loss: 3.6523 (4.0783)  acc1: 32.2917 (30.8832)  acc5: 55.2083 (51.9660)  time: 0.3196  data: 0.2549  max mem: 2112
Test:  [ 80/531]  eta: 0:01:56  loss: 3.6523 (4.0668)  acc1: 29.1667 (31.0442)  acc5: 50.0000 (51.8390)  time: 0.3362  data: 0.2702  max mem: 2112
Test:  [ 90/531]  eta: 0:01:48  loss: 3.4164 (3.9656)  acc1: 29.1667 (32.0627)  acc5: 57.2917 (53.3310)  time: 0.1909  data: 0.1254  max mem: 2112
Test:  [100/531]  eta: 0:01:41  loss: 2.6150 (3.8583)  acc1: 45.8333 (33.2921)  acc5: 73.9583 (54.9402)  time: 0.1360  data: 0.0713  max mem: 2112
Test:  [110/531]  eta: 0:01:36  loss: 2.6825 (3.7650)  acc1: 45.8333 (33.9433)  acc5: 72.9167 (56.5221)  time: 0.1524  data: 0.0873  max mem: 2112
Test:  [120/531]  eta: 0:01:29  loss: 2.6599 (3.6834)  acc1: 40.6250 (34.6849)  acc5: 73.9583 (57.8426)  time: 0.1340  data: 0.0684  max mem: 2112
Test:  [130/531]  eta: 0:01:26  loss: 2.7991 (3.6413)  acc1: 40.6250 (35.0827)  acc5: 69.7917 (58.4447)  time: 0.1394  data: 0.0747  max mem: 2112
Test:  [140/531]  eta: 0:01:22  loss: 2.7422 (3.5705)  acc1: 40.6250 (35.8673)  acc5: 69.7917 (59.5449)  time: 0.1708  data: 0.1064  max mem: 2112
Test:  [150/531]  eta: 0:01:20  loss: 3.1901 (3.5802)  acc1: 30.2083 (35.5546)  acc5: 63.5417 (59.4578)  time: 0.1935  data: 0.1290  max mem: 2112
Test:  [160/531]  eta: 0:01:19  loss: 3.2452 (3.5781)  acc1: 32.2917 (35.9925)  acc5: 62.5000 (59.5497)  time: 0.2288  data: 0.1648  max mem: 2112
Test:  [170/531]  eta: 0:01:15  loss: 3.3492 (3.5887)  acc1: 38.5417 (36.1294)  acc5: 62.5000 (59.3872)  time: 0.1811  data: 0.1163  max mem: 2112
Test:  [180/531]  eta: 0:01:12  loss: 3.5746 (3.5926)  acc1: 34.3750 (36.3835)  acc5: 52.0833 (59.2714)  time: 0.1449  data: 0.0798  max mem: 2112
Test:  [190/531]  eta: 0:01:10  loss: 3.3411 (3.5847)  acc1: 35.4167 (36.5892)  acc5: 60.4167 (59.4568)  time: 0.1916  data: 0.1273  max mem: 2112
Test:  [200/531]  eta: 0:01:07  loss: 3.2321 (3.5963)  acc1: 35.4167 (36.4791)  acc5: 60.4167 (59.1211)  time: 0.1952  data: 0.1308  max mem: 2112
Test:  [210/531]  eta: 0:01:05  loss: 3.9675 (3.6420)  acc1: 30.2083 (36.1128)  acc5: 51.0417 (58.4469)  time: 0.1929  data: 0.1281  max mem: 2112
Test:  [220/531]  eta: 0:01:04  loss: 3.9675 (3.6500)  acc1: 30.2083 (36.1520)  acc5: 48.9583 (58.1024)  time: 0.2232  data: 0.1589  max mem: 2112
Test:  [230/531]  eta: 0:01:01  loss: 3.7936 (3.6644)  acc1: 33.3333 (36.0254)  acc5: 48.9583 (57.8644)  time: 0.1958  data: 0.1319  max mem: 2112
Test:  [240/531]  eta: 0:00:58  loss: 3.4854 (3.6757)  acc1: 33.3333 (35.8748)  acc5: 57.2917 (57.6591)  time: 0.1613  data: 0.0966  max mem: 2112
Test:  [250/531]  eta: 0:00:57  loss: 3.8042 (3.7052)  acc1: 30.2083 (35.6076)  acc5: 53.1250 (57.2253)  time: 0.2110  data: 0.1458  max mem: 2112
Test:  [260/531]  eta: 0:00:55  loss: 4.1275 (3.7172)  acc1: 27.0833 (35.2810)  acc5: 48.9583 (56.9844)  time: 0.2090  data: 0.1442  max mem: 2112
Test:  [270/531]  eta: 0:00:52  loss: 3.4106 (3.7121)  acc1: 31.2500 (35.4513)  acc5: 55.2083 (57.0572)  time: 0.1770  data: 0.1131  max mem: 2112
Test:  [280/531]  eta: 0:00:51  loss: 3.2755 (3.6853)  acc1: 42.7083 (35.9356)  acc5: 62.5000 (57.3473)  time: 0.2073  data: 0.1434  max mem: 2112
Test:  [290/531]  eta: 0:00:48  loss: 3.5910 (3.6891)  acc1: 39.5833 (35.8820)  acc5: 55.2083 (57.3418)  time: 0.1764  data: 0.1121  max mem: 2112
Test:  [300/531]  eta: 0:00:46  loss: 3.5904 (3.6826)  acc1: 34.3750 (36.0361)  acc5: 55.2083 (57.4024)  time: 0.1544  data: 0.0901  max mem: 2112
Test:  [310/531]  eta: 0:00:44  loss: 3.4157 (3.6786)  acc1: 39.5833 (36.2004)  acc5: 57.2917 (57.4290)  time: 0.1938  data: 0.1299  max mem: 2112
Test:  [320/531]  eta: 0:00:42  loss: 3.4157 (3.6683)  acc1: 41.6667 (36.4097)  acc5: 56.2500 (57.4734)  time: 0.1964  data: 0.1329  max mem: 2112
Test:  [330/531]  eta: 0:00:39  loss: 3.2339 (3.6543)  acc1: 40.6250 (36.4206)  acc5: 61.4583 (57.6536)  time: 0.1552  data: 0.0917  max mem: 2112
Test:  [340/531]  eta: 0:00:37  loss: 3.4632 (3.6680)  acc1: 25.0000 (36.1284)  acc5: 59.3750 (57.4475)  time: 0.1538  data: 0.0899  max mem: 2112
Test:  [350/531]  eta: 0:00:35  loss: 3.8545 (3.6740)  acc1: 22.9167 (36.0458)  acc5: 54.1667 (57.3006)  time: 0.1642  data: 0.0998  max mem: 2112
Test:  [360/531]  eta: 0:00:33  loss: 4.0041 (3.6837)  acc1: 27.0833 (35.8264)  acc5: 53.1250 (57.1359)  time: 0.1577  data: 0.0929  max mem: 2112
Test:  [370/531]  eta: 0:00:31  loss: 4.0041 (3.6995)  acc1: 27.0833 (35.7171)  acc5: 52.0833 (56.8958)  time: 0.2014  data: 0.1367  max mem: 2112
Test:  [380/531]  eta: 0:00:29  loss: 3.9142 (3.6987)  acc1: 31.2500 (35.7912)  acc5: 50.0000 (56.8843)  time: 0.2228  data: 0.1585  max mem: 2112
Test:  [390/531]  eta: 0:00:27  loss: 4.1051 (3.7233)  acc1: 28.1250 (35.5419)  acc5: 46.8750 (56.4924)  time: 0.1667  data: 0.1016  max mem: 2112
Test:  [400/531]  eta: 0:00:25  loss: 4.1186 (3.7324)  acc1: 25.0000 (35.4478)  acc5: 46.8750 (56.3175)  time: 0.1670  data: 0.1019  max mem: 2112
Test:  [410/531]  eta: 0:00:23  loss: 3.9022 (3.7310)  acc1: 36.4583 (35.5763)  acc5: 52.0833 (56.3235)  time: 0.1854  data: 0.1211  max mem: 2112
Test:  [420/531]  eta: 0:00:21  loss: 3.9022 (3.7338)  acc1: 35.4167 (35.5503)  acc5: 52.0833 (56.2327)  time: 0.1771  data: 0.1128  max mem: 2112
Test:  [430/531]  eta: 0:00:19  loss: 3.9467 (3.7439)  acc1: 29.1667 (35.4360)  acc5: 50.0000 (56.0808)  time: 0.1932  data: 0.1289  max mem: 2112
Test:  [440/531]  eta: 0:00:17  loss: 3.6637 (3.7345)  acc1: 31.2500 (35.5395)  acc5: 55.2083 (56.1791)  time: 0.2176  data: 0.1537  max mem: 2112
Test:  [450/531]  eta: 0:00:15  loss: 3.1082 (3.7242)  acc1: 41.6667 (35.5991)  acc5: 59.3750 (56.3424)  time: 0.2465  data: 0.1810  max mem: 2112
Test:  [460/531]  eta: 0:00:13  loss: 3.2982 (3.7255)  acc1: 42.7083 (35.6494)  acc5: 55.2083 (56.2862)  time: 0.2073  data: 0.1414  max mem: 2112
Test:  [470/531]  eta: 0:00:11  loss: 3.6034 (3.7164)  acc1: 42.7083 (35.8700)  acc5: 55.2083 (56.3562)  time: 0.1448  data: 0.0805  max mem: 2112
Test:  [480/531]  eta: 0:00:09  loss: 3.2661 (3.7180)  acc1: 34.3750 (35.7112)  acc5: 57.2917 (56.3301)  time: 0.1506  data: 0.0867  max mem: 2112
Test:  [490/531]  eta: 0:00:08  loss: 4.2493 (3.7415)  acc1: 21.8750 (35.5015)  acc5: 46.8750 (55.9975)  time: 0.2195  data: 0.1560  max mem: 2112
Test:  [500/531]  eta: 0:00:06  loss: 5.2534 (3.7943)  acc1: 17.7083 (35.0466)  acc5: 29.1667 (55.3019)  time: 0.2425  data: 0.1782  max mem: 2112
Test:  [510/531]  eta: 0:00:04  loss: 6.3210 (3.8403)  acc1: 8.3333 (34.6359)  acc5: 15.6250 (54.6885)  time: 0.2133  data: 0.1485  max mem: 2112
Test:  [520/531]  eta: 0:00:02  loss: 5.6810 (3.8774)  acc1: 11.4583 (34.2710)  acc5: 28.1250 (54.1867)  time: 0.2136  data: 0.1488  max mem: 2112
Test:  [530/531]  eta: 0:00:00  loss: 5.5887 (3.9124)  acc1: 10.4167 (33.8501)  acc5: 28.1250 (53.6894)  time: 0.2113  data: 0.1492  max mem: 2112
Test: Total time: 0:01:44 (0.1969 s / it)
* Acc@1 33.850 Acc@5 53.689 loss 3.912
Accuracy of the network on 50889 test images: 33.85015%
Not using distributed mode
Namespace(Decom=False, aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=64, bn=True, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/sliu/project_space/datasets/sketch', data_set='IMNET', data_type='imagenet-s', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.1, enable_wandb=False, epochs=300, eval=True, eval_data_path=None, finetune='', fix=False, growth='random', head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, kernel_size=[51, 49, 47, 13, 5], layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=-1, log_dir=None, lr=0.004, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='SLaK_tiny', model_ema=False, model_ema_decay=0.9999, model_ema_eval=False, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_workers=10, only_L=False, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', parallel=True, pin_mem=True, project='SLaK', prune='magnitude', prune_rate=0.3, recount=1, redistribution='none', remode='pixel', reprob=0.25, resplit=False, resume='./checkpoints/SLaK_tiny_checkpoint.pth', save_ckpt=True, save_ckpt_freq=10, save_ckpt_num=3, seed=0, smoothing=0.1, sparse=False, sparse_init='snip', sparsity=0.4, start_epoch=0, train_interpolation='bicubic', update_freq=1, update_frequency=100, use_amp=False, verbose=False, wandb_ckpt=False, warmup_epochs=20, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, width_factor=1.0, world_size=1)
Transform = 
Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /home/sliu/project_space/datasets/sketch
Number of the class = 1000
Mixup is activated!
Model = SLaK(
  (downsample_layers): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (1): LayerNorm()
    )
    (1): Sequential(
      (0): LayerNorm()
      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))
    )
    (2): Sequential(
      (0): LayerNorm()
      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))
    )
    (3): Sequential(
      (0): LayerNorm()
      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (stages): ModuleList(
    (0): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(51, 51), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (small_conv): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(5, 5), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): Identity()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(51, 51), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (small_conv): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(5, 5), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(51, 51), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (small_conv): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(5, 5), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): DropPath()
      )
    )
    (1): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(49, 49), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (small_conv): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(5, 5), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(49, 49), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (small_conv): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(5, 5), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(49, 49), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (small_conv): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(5, 5), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
    )
    (2): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(47, 47), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (small_conv): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(5, 5), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(47, 47), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (small_conv): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(5, 5), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(47, 47), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (small_conv): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(5, 5), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (3): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(47, 47), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (small_conv): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(5, 5), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (4): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(47, 47), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (small_conv): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(5, 5), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (5): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(47, 47), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (small_conv): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(5, 5), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (6): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(47, 47), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (small_conv): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(5, 5), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (7): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(47, 47), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (small_conv): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(5, 5), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (8): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(47, 47), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (small_conv): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(5, 5), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
    )
    (3): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(13, 13), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (small_conv): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(5, 5), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(13, 13), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (small_conv): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(5, 5), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(13, 13), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
          (small_conv): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(5, 5), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
number of params: 38605768
LR = 0.00400000
Batch size = 64
Update frequent = 1
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "downsample_layers.0.0.weight",
      "downsample_layers.1.1.weight",
      "downsample_layers.2.1.weight",
      "downsample_layers.3.1.weight",
      "stages.0.0.large_kernel.lkb_origin.conv.weight",
      "stages.0.0.large_kernel.small_conv.conv.weight",
      "stages.0.0.pwconv1.weight",
      "stages.0.0.pwconv2.weight",
      "stages.0.1.large_kernel.lkb_origin.conv.weight",
      "stages.0.1.large_kernel.small_conv.conv.weight",
      "stages.0.1.pwconv1.weight",
      "stages.0.1.pwconv2.weight",
      "stages.0.2.large_kernel.lkb_origin.conv.weight",
      "stages.0.2.large_kernel.small_conv.conv.weight",
      "stages.0.2.pwconv1.weight",
      "stages.0.2.pwconv2.weight",
      "stages.1.0.large_kernel.lkb_origin.conv.weight",
      "stages.1.0.large_kernel.small_conv.conv.weight",
      "stages.1.0.pwconv1.weight",
      "stages.1.0.pwconv2.weight",
      "stages.1.1.large_kernel.lkb_origin.conv.weight",
      "stages.1.1.large_kernel.small_conv.conv.weight",
      "stages.1.1.pwconv1.weight",
      "stages.1.1.pwconv2.weight",
      "stages.1.2.large_kernel.lkb_origin.conv.weight",
      "stages.1.2.large_kernel.small_conv.conv.weight",
      "stages.1.2.pwconv1.weight",
      "stages.1.2.pwconv2.weight",
      "stages.2.0.large_kernel.lkb_origin.conv.weight",
      "stages.2.0.large_kernel.small_conv.conv.weight",
      "stages.2.0.pwconv1.weight",
      "stages.2.0.pwconv2.weight",
      "stages.2.1.large_kernel.lkb_origin.conv.weight",
      "stages.2.1.large_kernel.small_conv.conv.weight",
      "stages.2.1.pwconv1.weight",
      "stages.2.1.pwconv2.weight",
      "stages.2.2.large_kernel.lkb_origin.conv.weight",
      "stages.2.2.large_kernel.small_conv.conv.weight",
      "stages.2.2.pwconv1.weight",
      "stages.2.2.pwconv2.weight",
      "stages.2.3.large_kernel.lkb_origin.conv.weight",
      "stages.2.3.large_kernel.small_conv.conv.weight",
      "stages.2.3.pwconv1.weight",
      "stages.2.3.pwconv2.weight",
      "stages.2.4.large_kernel.lkb_origin.conv.weight",
      "stages.2.4.large_kernel.small_conv.conv.weight",
      "stages.2.4.pwconv1.weight",
      "stages.2.4.pwconv2.weight",
      "stages.2.5.large_kernel.lkb_origin.conv.weight",
      "stages.2.5.large_kernel.small_conv.conv.weight",
      "stages.2.5.pwconv1.weight",
      "stages.2.5.pwconv2.weight",
      "stages.2.6.large_kernel.lkb_origin.conv.weight",
      "stages.2.6.large_kernel.small_conv.conv.weight",
      "stages.2.6.pwconv1.weight",
      "stages.2.6.pwconv2.weight",
      "stages.2.7.large_kernel.lkb_origin.conv.weight",
      "stages.2.7.large_kernel.small_conv.conv.weight",
      "stages.2.7.pwconv1.weight",
      "stages.2.7.pwconv2.weight",
      "stages.2.8.large_kernel.lkb_origin.conv.weight",
      "stages.2.8.large_kernel.small_conv.conv.weight",
      "stages.2.8.pwconv1.weight",
      "stages.2.8.pwconv2.weight",
      "stages.3.0.large_kernel.lkb_origin.conv.weight",
      "stages.3.0.large_kernel.small_conv.conv.weight",
      "stages.3.0.pwconv1.weight",
      "stages.3.0.pwconv2.weight",
      "stages.3.1.large_kernel.lkb_origin.conv.weight",
      "stages.3.1.large_kernel.small_conv.conv.weight",
      "stages.3.1.pwconv1.weight",
      "stages.3.1.pwconv2.weight",
      "stages.3.2.large_kernel.lkb_origin.conv.weight",
      "stages.3.2.large_kernel.small_conv.conv.weight",
      "stages.3.2.pwconv1.weight",
      "stages.3.2.pwconv2.weight",
      "head.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "downsample_layers.0.0.bias",
      "downsample_layers.0.1.weight",
      "downsample_layers.0.1.bias",
      "downsample_layers.1.0.weight",
      "downsample_layers.1.0.bias",
      "downsample_layers.1.1.bias",
      "downsample_layers.2.0.weight",
      "downsample_layers.2.0.bias",
      "downsample_layers.2.1.bias",
      "downsample_layers.3.0.weight",
      "downsample_layers.3.0.bias",
      "downsample_layers.3.1.bias",
      "stages.0.0.gamma",
      "stages.0.0.large_kernel.lkb_origin.bn.weight",
      "stages.0.0.large_kernel.lkb_origin.bn.bias",
      "stages.0.0.large_kernel.small_conv.bn.weight",
      "stages.0.0.large_kernel.small_conv.bn.bias",
      "stages.0.0.norm.weight",
      "stages.0.0.norm.bias",
      "stages.0.0.pwconv1.bias",
      "stages.0.0.pwconv2.bias",
      "stages.0.1.gamma",
      "stages.0.1.large_kernel.lkb_origin.bn.weight",
      "stages.0.1.large_kernel.lkb_origin.bn.bias",
      "stages.0.1.large_kernel.small_conv.bn.weight",
      "stages.0.1.large_kernel.small_conv.bn.bias",
      "stages.0.1.norm.weight",
      "stages.0.1.norm.bias",
      "stages.0.1.pwconv1.bias",
      "stages.0.1.pwconv2.bias",
      "stages.0.2.gamma",
      "stages.0.2.large_kernel.lkb_origin.bn.weight",
      "stages.0.2.large_kernel.lkb_origin.bn.bias",
      "stages.0.2.large_kernel.small_conv.bn.weight",
      "stages.0.2.large_kernel.small_conv.bn.bias",
      "stages.0.2.norm.weight",
      "stages.0.2.norm.bias",
      "stages.0.2.pwconv1.bias",
      "stages.0.2.pwconv2.bias",
      "stages.1.0.gamma",
      "stages.1.0.large_kernel.lkb_origin.bn.weight",
      "stages.1.0.large_kernel.lkb_origin.bn.bias",
      "stages.1.0.large_kernel.small_conv.bn.weight",
      "stages.1.0.large_kernel.small_conv.bn.bias",
      "stages.1.0.norm.weight",
      "stages.1.0.norm.bias",
      "stages.1.0.pwconv1.bias",
      "stages.1.0.pwconv2.bias",
      "stages.1.1.gamma",
      "stages.1.1.large_kernel.lkb_origin.bn.weight",
      "stages.1.1.large_kernel.lkb_origin.bn.bias",
      "stages.1.1.large_kernel.small_conv.bn.weight",
      "stages.1.1.large_kernel.small_conv.bn.bias",
      "stages.1.1.norm.weight",
      "stages.1.1.norm.bias",
      "stages.1.1.pwconv1.bias",
      "stages.1.1.pwconv2.bias",
      "stages.1.2.gamma",
      "stages.1.2.large_kernel.lkb_origin.bn.weight",
      "stages.1.2.large_kernel.lkb_origin.bn.bias",
      "stages.1.2.large_kernel.small_conv.bn.weight",
      "stages.1.2.large_kernel.small_conv.bn.bias",
      "stages.1.2.norm.weight",
      "stages.1.2.norm.bias",
      "stages.1.2.pwconv1.bias",
      "stages.1.2.pwconv2.bias",
      "stages.2.0.gamma",
      "stages.2.0.large_kernel.lkb_origin.bn.weight",
      "stages.2.0.large_kernel.lkb_origin.bn.bias",
      "stages.2.0.large_kernel.small_conv.bn.weight",
      "stages.2.0.large_kernel.small_conv.bn.bias",
      "stages.2.0.norm.weight",
      "stages.2.0.norm.bias",
      "stages.2.0.pwconv1.bias",
      "stages.2.0.pwconv2.bias",
      "stages.2.1.gamma",
      "stages.2.1.large_kernel.lkb_origin.bn.weight",
      "stages.2.1.large_kernel.lkb_origin.bn.bias",
      "stages.2.1.large_kernel.small_conv.bn.weight",
      "stages.2.1.large_kernel.small_conv.bn.bias",
      "stages.2.1.norm.weight",
      "stages.2.1.norm.bias",
      "stages.2.1.pwconv1.bias",
      "stages.2.1.pwconv2.bias",
      "stages.2.2.gamma",
      "stages.2.2.large_kernel.lkb_origin.bn.weight",
      "stages.2.2.large_kernel.lkb_origin.bn.bias",
      "stages.2.2.large_kernel.small_conv.bn.weight",
      "stages.2.2.large_kernel.small_conv.bn.bias",
      "stages.2.2.norm.weight",
      "stages.2.2.norm.bias",
      "stages.2.2.pwconv1.bias",
      "stages.2.2.pwconv2.bias",
      "stages.2.3.gamma",
      "stages.2.3.large_kernel.lkb_origin.bn.weight",
      "stages.2.3.large_kernel.lkb_origin.bn.bias",
      "stages.2.3.large_kernel.small_conv.bn.weight",
      "stages.2.3.large_kernel.small_conv.bn.bias",
      "stages.2.3.norm.weight",
      "stages.2.3.norm.bias",
      "stages.2.3.pwconv1.bias",
      "stages.2.3.pwconv2.bias",
      "stages.2.4.gamma",
      "stages.2.4.large_kernel.lkb_origin.bn.weight",
      "stages.2.4.large_kernel.lkb_origin.bn.bias",
      "stages.2.4.large_kernel.small_conv.bn.weight",
      "stages.2.4.large_kernel.small_conv.bn.bias",
      "stages.2.4.norm.weight",
      "stages.2.4.norm.bias",
      "stages.2.4.pwconv1.bias",
      "stages.2.4.pwconv2.bias",
      "stages.2.5.gamma",
      "stages.2.5.large_kernel.lkb_origin.bn.weight",
      "stages.2.5.large_kernel.lkb_origin.bn.bias",
      "stages.2.5.large_kernel.small_conv.bn.weight",
      "stages.2.5.large_kernel.small_conv.bn.bias",
      "stages.2.5.norm.weight",
      "stages.2.5.norm.bias",
      "stages.2.5.pwconv1.bias",
      "stages.2.5.pwconv2.bias",
      "stages.2.6.gamma",
      "stages.2.6.large_kernel.lkb_origin.bn.weight",
      "stages.2.6.large_kernel.lkb_origin.bn.bias",
      "stages.2.6.large_kernel.small_conv.bn.weight",
      "stages.2.6.large_kernel.small_conv.bn.bias",
      "stages.2.6.norm.weight",
      "stages.2.6.norm.bias",
      "stages.2.6.pwconv1.bias",
      "stages.2.6.pwconv2.bias",
      "stages.2.7.gamma",
      "stages.2.7.large_kernel.lkb_origin.bn.weight",
      "stages.2.7.large_kernel.lkb_origin.bn.bias",
      "stages.2.7.large_kernel.small_conv.bn.weight",
      "stages.2.7.large_kernel.small_conv.bn.bias",
      "stages.2.7.norm.weight",
      "stages.2.7.norm.bias",
      "stages.2.7.pwconv1.bias",
      "stages.2.7.pwconv2.bias",
      "stages.2.8.gamma",
      "stages.2.8.large_kernel.lkb_origin.bn.weight",
      "stages.2.8.large_kernel.lkb_origin.bn.bias",
      "stages.2.8.large_kernel.small_conv.bn.weight",
      "stages.2.8.large_kernel.small_conv.bn.bias",
      "stages.2.8.norm.weight",
      "stages.2.8.norm.bias",
      "stages.2.8.pwconv1.bias",
      "stages.2.8.pwconv2.bias",
      "stages.3.0.gamma",
      "stages.3.0.large_kernel.lkb_origin.bn.weight",
      "stages.3.0.large_kernel.lkb_origin.bn.bias",
      "stages.3.0.large_kernel.small_conv.bn.weight",
      "stages.3.0.large_kernel.small_conv.bn.bias",
      "stages.3.0.norm.weight",
      "stages.3.0.norm.bias",
      "stages.3.0.pwconv1.bias",
      "stages.3.0.pwconv2.bias",
      "stages.3.1.gamma",
      "stages.3.1.large_kernel.lkb_origin.bn.weight",
      "stages.3.1.large_kernel.lkb_origin.bn.bias",
      "stages.3.1.large_kernel.small_conv.bn.weight",
      "stages.3.1.large_kernel.small_conv.bn.bias",
      "stages.3.1.norm.weight",
      "stages.3.1.norm.bias",
      "stages.3.1.pwconv1.bias",
      "stages.3.1.pwconv2.bias",
      "stages.3.2.gamma",
      "stages.3.2.large_kernel.lkb_origin.bn.weight",
      "stages.3.2.large_kernel.lkb_origin.bn.bias",
      "stages.3.2.large_kernel.small_conv.bn.weight",
      "stages.3.2.large_kernel.small_conv.bn.bias",
      "stages.3.2.norm.weight",
      "stages.3.2.norm.bias",
      "stages.3.2.pwconv1.bias",
      "stages.3.2.pwconv2.bias",
      "norm.weight",
      "norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
criterion = SoftTargetCrossEntropy()
dict_keys(['model', 'optimizer', 'epoch', 'scaler', 'args', 'model_ema'])
Traceback (most recent call last):
  File "main.py", line 561, in <module>
    main(args)
  File "main.py", line 446, in main
    utils.auto_load_model(
  File "/gpfs/work3/0/prjste21060/projects/datasets/TJ_RobustData/robustness/SLaK/utils.py", line 498, in auto_load_model
    model_without_ddp.load_state_dict(checkpoint['model'])
  File "/home/sliu/miniconda3/envs/slak/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1482, in load_state_dict
    raise RuntimeError('Error(s) in loading state_dict for {}:\n\t{}'.format(
RuntimeError: Error(s) in loading state_dict for SLaK:
	Missing key(s) in state_dict: "stages.0.0.large_kernel.lkb_origin.conv.weight", "stages.0.0.large_kernel.lkb_origin.bn.weight", "stages.0.0.large_kernel.lkb_origin.bn.bias", "stages.0.0.large_kernel.lkb_origin.bn.running_mean", "stages.0.0.large_kernel.lkb_origin.bn.running_var", "stages.0.1.large_kernel.lkb_origin.conv.weight", "stages.0.1.large_kernel.lkb_origin.bn.weight", "stages.0.1.large_kernel.lkb_origin.bn.bias", "stages.0.1.large_kernel.lkb_origin.bn.running_mean", "stages.0.1.large_kernel.lkb_origin.bn.running_var", "stages.0.2.large_kernel.lkb_origin.conv.weight", "stages.0.2.large_kernel.lkb_origin.bn.weight", "stages.0.2.large_kernel.lkb_origin.bn.bias", "stages.0.2.large_kernel.lkb_origin.bn.running_mean", "stages.0.2.large_kernel.lkb_origin.bn.running_var", "stages.1.0.large_kernel.lkb_origin.conv.weight", "stages.1.0.large_kernel.lkb_origin.bn.weight", "stages.1.0.large_kernel.lkb_origin.bn.bias", "stages.1.0.large_kernel.lkb_origin.bn.running_mean", "stages.1.0.large_kernel.lkb_origin.bn.running_var", "stages.1.1.large_kernel.lkb_origin.conv.weight", "stages.1.1.large_kernel.lkb_origin.bn.weight", "stages.1.1.large_kernel.lkb_origin.bn.bias", "stages.1.1.large_kernel.lkb_origin.bn.running_mean", "stages.1.1.large_kernel.lkb_origin.bn.running_var", "stages.1.2.large_kernel.lkb_origin.conv.weight", "stages.1.2.large_kernel.lkb_origin.bn.weight", "stages.1.2.large_kernel.lkb_origin.bn.bias", "stages.1.2.large_kernel.lkb_origin.bn.running_mean", "stages.1.2.large_kernel.lkb_origin.bn.running_var", "stages.2.0.large_kernel.lkb_origin.conv.weight", "stages.2.0.large_kernel.lkb_origin.bn.weight", "stages.2.0.large_kernel.lkb_origin.bn.bias", "stages.2.0.large_kernel.lkb_origin.bn.running_mean", "stages.2.0.large_kernel.lkb_origin.bn.running_var", "stages.2.1.large_kernel.lkb_origin.conv.weight", "stages.2.1.large_kernel.lkb_origin.bn.weight", "stages.2.1.large_kernel.lkb_origin.bn.bias", "stages.2.1.large_kernel.lkb_origin.bn.running_mean", "stages.2.1.large_kernel.lkb_origin.bn.running_var", "stages.2.2.large_kernel.lkb_origin.conv.weight", "stages.2.2.large_kernel.lkb_origin.bn.weight", "stages.2.2.large_kernel.lkb_origin.bn.bias", "stages.2.2.large_kernel.lkb_origin.bn.running_mean", "stages.2.2.large_kernel.lkb_origin.bn.running_var", "stages.2.3.large_kernel.lkb_origin.conv.weight", "stages.2.3.large_kernel.lkb_origin.bn.weight", "stages.2.3.large_kernel.lkb_origin.bn.bias", "stages.2.3.large_kernel.lkb_origin.bn.running_mean", "stages.2.3.large_kernel.lkb_origin.bn.running_var", "stages.2.4.large_kernel.lkb_origin.conv.weight", "stages.2.4.large_kernel.lkb_origin.bn.weight", "stages.2.4.large_kernel.lkb_origin.bn.bias", "stages.2.4.large_kernel.lkb_origin.bn.running_mean", "stages.2.4.large_kernel.lkb_origin.bn.running_var", "stages.2.5.large_kernel.lkb_origin.conv.weight", "stages.2.5.large_kernel.lkb_origin.bn.weight", "stages.2.5.large_kernel.lkb_origin.bn.bias", "stages.2.5.large_kernel.lkb_origin.bn.running_mean", "stages.2.5.large_kernel.lkb_origin.bn.running_var", "stages.2.6.large_kernel.lkb_origin.conv.weight", "stages.2.6.large_kernel.lkb_origin.bn.weight", "stages.2.6.large_kernel.lkb_origin.bn.bias", "stages.2.6.large_kernel.lkb_origin.bn.running_mean", "stages.2.6.large_kernel.lkb_origin.bn.running_var", "stages.2.7.large_kernel.lkb_origin.conv.weight", "stages.2.7.large_kernel.lkb_origin.bn.weight", "stages.2.7.large_kernel.lkb_origin.bn.bias", "stages.2.7.large_kernel.lkb_origin.bn.running_mean", "stages.2.7.large_kernel.lkb_origin.bn.running_var", "stages.2.8.large_kernel.lkb_origin.conv.weight", "stages.2.8.large_kernel.lkb_origin.bn.weight", "stages.2.8.large_kernel.lkb_origin.bn.bias", "stages.2.8.large_kernel.lkb_origin.bn.running_mean", "stages.2.8.large_kernel.lkb_origin.bn.running_var", "stages.3.0.large_kernel.lkb_origin.conv.weight", "stages.3.0.large_kernel.lkb_origin.bn.weight", "stages.3.0.large_kernel.lkb_origin.bn.bias", "stages.3.0.large_kernel.lkb_origin.bn.running_mean", "stages.3.0.large_kernel.lkb_origin.bn.running_var", "stages.3.1.large_kernel.lkb_origin.conv.weight", "stages.3.1.large_kernel.lkb_origin.bn.weight", "stages.3.1.large_kernel.lkb_origin.bn.bias", "stages.3.1.large_kernel.lkb_origin.bn.running_mean", "stages.3.1.large_kernel.lkb_origin.bn.running_var", "stages.3.2.large_kernel.lkb_origin.conv.weight", "stages.3.2.large_kernel.lkb_origin.bn.weight", "stages.3.2.large_kernel.lkb_origin.bn.bias", "stages.3.2.large_kernel.lkb_origin.bn.running_mean", "stages.3.2.large_kernel.lkb_origin.bn.running_var". 
	Unexpected key(s) in state_dict: "stages.0.0.large_kernel.LoRA1.conv.weight", "stages.0.0.large_kernel.LoRA1.bn.weight", "stages.0.0.large_kernel.LoRA1.bn.bias", "stages.0.0.large_kernel.LoRA1.bn.running_mean", "stages.0.0.large_kernel.LoRA1.bn.running_var", "stages.0.0.large_kernel.LoRA1.bn.num_batches_tracked", "stages.0.0.large_kernel.LoRA2.conv.weight", "stages.0.0.large_kernel.LoRA2.bn.weight", "stages.0.0.large_kernel.LoRA2.bn.bias", "stages.0.0.large_kernel.LoRA2.bn.running_mean", "stages.0.0.large_kernel.LoRA2.bn.running_var", "stages.0.0.large_kernel.LoRA2.bn.num_batches_tracked", "stages.0.1.large_kernel.LoRA1.conv.weight", "stages.0.1.large_kernel.LoRA1.bn.weight", "stages.0.1.large_kernel.LoRA1.bn.bias", "stages.0.1.large_kernel.LoRA1.bn.running_mean", "stages.0.1.large_kernel.LoRA1.bn.running_var", "stages.0.1.large_kernel.LoRA1.bn.num_batches_tracked", "stages.0.1.large_kernel.LoRA2.conv.weight", "stages.0.1.large_kernel.LoRA2.bn.weight", "stages.0.1.large_kernel.LoRA2.bn.bias", "stages.0.1.large_kernel.LoRA2.bn.running_mean", "stages.0.1.large_kernel.LoRA2.bn.running_var", "stages.0.1.large_kernel.LoRA2.bn.num_batches_tracked", "stages.0.2.large_kernel.LoRA1.conv.weight", "stages.0.2.large_kernel.LoRA1.bn.weight", "stages.0.2.large_kernel.LoRA1.bn.bias", "stages.0.2.large_kernel.LoRA1.bn.running_mean", "stages.0.2.large_kernel.LoRA1.bn.running_var", "stages.0.2.large_kernel.LoRA1.bn.num_batches_tracked", "stages.0.2.large_kernel.LoRA2.conv.weight", "stages.0.2.large_kernel.LoRA2.bn.weight", "stages.0.2.large_kernel.LoRA2.bn.bias", "stages.0.2.large_kernel.LoRA2.bn.running_mean", "stages.0.2.large_kernel.LoRA2.bn.running_var", "stages.0.2.large_kernel.LoRA2.bn.num_batches_tracked", "stages.1.0.large_kernel.LoRA1.conv.weight", "stages.1.0.large_kernel.LoRA1.bn.weight", "stages.1.0.large_kernel.LoRA1.bn.bias", "stages.1.0.large_kernel.LoRA1.bn.running_mean", "stages.1.0.large_kernel.LoRA1.bn.running_var", "stages.1.0.large_kernel.LoRA1.bn.num_batches_tracked", "stages.1.0.large_kernel.LoRA2.conv.weight", "stages.1.0.large_kernel.LoRA2.bn.weight", "stages.1.0.large_kernel.LoRA2.bn.bias", "stages.1.0.large_kernel.LoRA2.bn.running_mean", "stages.1.0.large_kernel.LoRA2.bn.running_var", "stages.1.0.large_kernel.LoRA2.bn.num_batches_tracked", "stages.1.1.large_kernel.LoRA1.conv.weight", "stages.1.1.large_kernel.LoRA1.bn.weight", "stages.1.1.large_kernel.LoRA1.bn.bias", "stages.1.1.large_kernel.LoRA1.bn.running_mean", "stages.1.1.large_kernel.LoRA1.bn.running_var", "stages.1.1.large_kernel.LoRA1.bn.num_batches_tracked", "stages.1.1.large_kernel.LoRA2.conv.weight", "stages.1.1.large_kernel.LoRA2.bn.weight", "stages.1.1.large_kernel.LoRA2.bn.bias", "stages.1.1.large_kernel.LoRA2.bn.running_mean", "stages.1.1.large_kernel.LoRA2.bn.running_var", "stages.1.1.large_kernel.LoRA2.bn.num_batches_tracked", "stages.1.2.large_kernel.LoRA1.conv.weight", "stages.1.2.large_kernel.LoRA1.bn.weight", "stages.1.2.large_kernel.LoRA1.bn.bias", "stages.1.2.large_kernel.LoRA1.bn.running_mean", "stages.1.2.large_kernel.LoRA1.bn.running_var", "stages.1.2.large_kernel.LoRA1.bn.num_batches_tracked", "stages.1.2.large_kernel.LoRA2.conv.weight", "stages.1.2.large_kernel.LoRA2.bn.weight", "stages.1.2.large_kernel.LoRA2.bn.bias", "stages.1.2.large_kernel.LoRA2.bn.running_mean", "stages.1.2.large_kernel.LoRA2.bn.running_var", "stages.1.2.large_kernel.LoRA2.bn.num_batches_tracked", "stages.2.0.large_kernel.LoRA1.conv.weight", "stages.2.0.large_kernel.LoRA1.bn.weight", "stages.2.0.large_kernel.LoRA1.bn.bias", "stages.2.0.large_kernel.LoRA1.bn.running_mean", "stages.2.0.large_kernel.LoRA1.bn.running_var", "stages.2.0.large_kernel.LoRA1.bn.num_batches_tracked", "stages.2.0.large_kernel.LoRA2.conv.weight", "stages.2.0.large_kernel.LoRA2.bn.weight", "stages.2.0.large_kernel.LoRA2.bn.bias", "stages.2.0.large_kernel.LoRA2.bn.running_mean", "stages.2.0.large_kernel.LoRA2.bn.running_var", "stages.2.0.large_kernel.LoRA2.bn.num_batches_tracked", "stages.2.1.large_kernel.LoRA1.conv.weight", "stages.2.1.large_kernel.LoRA1.bn.weight", "stages.2.1.large_kernel.LoRA1.bn.bias", "stages.2.1.large_kernel.LoRA1.bn.running_mean", "stages.2.1.large_kernel.LoRA1.bn.running_var", "stages.2.1.large_kernel.LoRA1.bn.num_batches_tracked", "stages.2.1.large_kernel.LoRA2.conv.weight", "stages.2.1.large_kernel.LoRA2.bn.weight", "stages.2.1.large_kernel.LoRA2.bn.bias", "stages.2.1.large_kernel.LoRA2.bn.running_mean", "stages.2.1.large_kernel.LoRA2.bn.running_var", "stages.2.1.large_kernel.LoRA2.bn.num_batches_tracked", "stages.2.2.large_kernel.LoRA1.conv.weight", "stages.2.2.large_kernel.LoRA1.bn.weight", "stages.2.2.large_kernel.LoRA1.bn.bias", "stages.2.2.large_kernel.LoRA1.bn.running_mean", "stages.2.2.large_kernel.LoRA1.bn.running_var", "stages.2.2.large_kernel.LoRA1.bn.num_batches_tracked", "stages.2.2.large_kernel.LoRA2.conv.weight", "stages.2.2.large_kernel.LoRA2.bn.weight", "stages.2.2.large_kernel.LoRA2.bn.bias", "stages.2.2.large_kernel.LoRA2.bn.running_mean", "stages.2.2.large_kernel.LoRA2.bn.running_var", "stages.2.2.large_kernel.LoRA2.bn.num_batches_tracked", "stages.2.3.large_kernel.LoRA1.conv.weight", "stages.2.3.large_kernel.LoRA1.bn.weight", "stages.2.3.large_kernel.LoRA1.bn.bias", "stages.2.3.large_kernel.LoRA1.bn.running_mean", "stages.2.3.large_kernel.LoRA1.bn.running_var", "stages.2.3.large_kernel.LoRA1.bn.num_batches_tracked", "stages.2.3.large_kernel.LoRA2.conv.weight", "stages.2.3.large_kernel.LoRA2.bn.weight", "stages.2.3.large_kernel.LoRA2.bn.bias", "stages.2.3.large_kernel.LoRA2.bn.running_mean", "stages.2.3.large_kernel.LoRA2.bn.running_var", "stages.2.3.large_kernel.LoRA2.bn.num_batches_tracked", "stages.2.4.large_kernel.LoRA1.conv.weight", "stages.2.4.large_kernel.LoRA1.bn.weight", "stages.2.4.large_kernel.LoRA1.bn.bias", "stages.2.4.large_kernel.LoRA1.bn.running_mean", "stages.2.4.large_kernel.LoRA1.bn.running_var", "stages.2.4.large_kernel.LoRA1.bn.num_batches_tracked", "stages.2.4.large_kernel.LoRA2.conv.weight", "stages.2.4.large_kernel.LoRA2.bn.weight", "stages.2.4.large_kernel.LoRA2.bn.bias", "stages.2.4.large_kernel.LoRA2.bn.running_mean", "stages.2.4.large_kernel.LoRA2.bn.running_var", "stages.2.4.large_kernel.LoRA2.bn.num_batches_tracked", "stages.2.5.large_kernel.LoRA1.conv.weight", "stages.2.5.large_kernel.LoRA1.bn.weight", "stages.2.5.large_kernel.LoRA1.bn.bias", "stages.2.5.large_kernel.LoRA1.bn.running_mean", "stages.2.5.large_kernel.LoRA1.bn.running_var", "stages.2.5.large_kernel.LoRA1.bn.num_batches_tracked", "stages.2.5.large_kernel.LoRA2.conv.weight", "stages.2.5.large_kernel.LoRA2.bn.weight", "stages.2.5.large_kernel.LoRA2.bn.bias", "stages.2.5.large_kernel.LoRA2.bn.running_mean", "stages.2.5.large_kernel.LoRA2.bn.running_var", "stages.2.5.large_kernel.LoRA2.bn.num_batches_tracked", "stages.2.6.large_kernel.LoRA1.conv.weight", "stages.2.6.large_kernel.LoRA1.bn.weight", "stages.2.6.large_kernel.LoRA1.bn.bias", "stages.2.6.large_kernel.LoRA1.bn.running_mean", "stages.2.6.large_kernel.LoRA1.bn.running_var", "stages.2.6.large_kernel.LoRA1.bn.num_batches_tracked", "stages.2.6.large_kernel.LoRA2.conv.weight", "stages.2.6.large_kernel.LoRA2.bn.weight", "stages.2.6.large_kernel.LoRA2.bn.bias", "stages.2.6.large_kernel.LoRA2.bn.running_mean", "stages.2.6.large_kernel.LoRA2.bn.running_var", "stages.2.6.large_kernel.LoRA2.bn.num_batches_tracked", "stages.2.7.large_kernel.LoRA1.conv.weight", "stages.2.7.large_kernel.LoRA1.bn.weight", "stages.2.7.large_kernel.LoRA1.bn.bias", "stages.2.7.large_kernel.LoRA1.bn.running_mean", "stages.2.7.large_kernel.LoRA1.bn.running_var", "stages.2.7.large_kernel.LoRA1.bn.num_batches_tracked", "stages.2.7.large_kernel.LoRA2.conv.weight", "stages.2.7.large_kernel.LoRA2.bn.weight", "stages.2.7.large_kernel.LoRA2.bn.bias", "stages.2.7.large_kernel.LoRA2.bn.running_mean", "stages.2.7.large_kernel.LoRA2.bn.running_var", "stages.2.7.large_kernel.LoRA2.bn.num_batches_tracked", "stages.2.8.large_kernel.LoRA1.conv.weight", "stages.2.8.large_kernel.LoRA1.bn.weight", "stages.2.8.large_kernel.LoRA1.bn.bias", "stages.2.8.large_kernel.LoRA1.bn.running_mean", "stages.2.8.large_kernel.LoRA1.bn.running_var", "stages.2.8.large_kernel.LoRA1.bn.num_batches_tracked", "stages.2.8.large_kernel.LoRA2.conv.weight", "stages.2.8.large_kernel.LoRA2.bn.weight", "stages.2.8.large_kernel.LoRA2.bn.bias", "stages.2.8.large_kernel.LoRA2.bn.running_mean", "stages.2.8.large_kernel.LoRA2.bn.running_var", "stages.2.8.large_kernel.LoRA2.bn.num_batches_tracked", "stages.3.0.large_kernel.LoRA1.conv.weight", "stages.3.0.large_kernel.LoRA1.bn.weight", "stages.3.0.large_kernel.LoRA1.bn.bias", "stages.3.0.large_kernel.LoRA1.bn.running_mean", "stages.3.0.large_kernel.LoRA1.bn.running_var", "stages.3.0.large_kernel.LoRA1.bn.num_batches_tracked", "stages.3.0.large_kernel.LoRA2.conv.weight", "stages.3.0.large_kernel.LoRA2.bn.weight", "stages.3.0.large_kernel.LoRA2.bn.bias", "stages.3.0.large_kernel.LoRA2.bn.running_mean", "stages.3.0.large_kernel.LoRA2.bn.running_var", "stages.3.0.large_kernel.LoRA2.bn.num_batches_tracked", "stages.3.1.large_kernel.LoRA1.conv.weight", "stages.3.1.large_kernel.LoRA1.bn.weight", "stages.3.1.large_kernel.LoRA1.bn.bias", "stages.3.1.large_kernel.LoRA1.bn.running_mean", "stages.3.1.large_kernel.LoRA1.bn.running_var", "stages.3.1.large_kernel.LoRA1.bn.num_batches_tracked", "stages.3.1.large_kernel.LoRA2.conv.weight", "stages.3.1.large_kernel.LoRA2.bn.weight", "stages.3.1.large_kernel.LoRA2.bn.bias", "stages.3.1.large_kernel.LoRA2.bn.running_mean", "stages.3.1.large_kernel.LoRA2.bn.running_var", "stages.3.1.large_kernel.LoRA2.bn.num_batches_tracked", "stages.3.2.large_kernel.LoRA1.conv.weight", "stages.3.2.large_kernel.LoRA1.bn.weight", "stages.3.2.large_kernel.LoRA1.bn.bias", "stages.3.2.large_kernel.LoRA1.bn.running_mean", "stages.3.2.large_kernel.LoRA1.bn.running_var", "stages.3.2.large_kernel.LoRA1.bn.num_batches_tracked", "stages.3.2.large_kernel.LoRA2.conv.weight", "stages.3.2.large_kernel.LoRA2.bn.weight", "stages.3.2.large_kernel.LoRA2.bn.bias", "stages.3.2.large_kernel.LoRA2.bn.running_mean", "stages.3.2.large_kernel.LoRA2.bn.running_var", "stages.3.2.large_kernel.LoRA2.bn.num_batches_tracked". 
	size mismatch for downsample_layers.0.0.weight: copying a param with shape torch.Size([124, 3, 4, 4]) from checkpoint, the shape in current model is torch.Size([96, 3, 4, 4]).
	size mismatch for downsample_layers.0.0.bias: copying a param with shape torch.Size([124]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for downsample_layers.0.1.weight: copying a param with shape torch.Size([124]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for downsample_layers.0.1.bias: copying a param with shape torch.Size([124]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for downsample_layers.1.0.weight: copying a param with shape torch.Size([124]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for downsample_layers.1.0.bias: copying a param with shape torch.Size([124]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for downsample_layers.1.1.weight: copying a param with shape torch.Size([249, 124, 2, 2]) from checkpoint, the shape in current model is torch.Size([192, 96, 2, 2]).
	size mismatch for downsample_layers.1.1.bias: copying a param with shape torch.Size([249]) from checkpoint, the shape in current model is torch.Size([192]).
	size mismatch for downsample_layers.2.0.weight: copying a param with shape torch.Size([249]) from checkpoint, the shape in current model is torch.Size([192]).
	size mismatch for downsample_layers.2.0.bias: copying a param with shape torch.Size([249]) from checkpoint, the shape in current model is torch.Size([192]).
	size mismatch for downsample_layers.2.1.weight: copying a param with shape torch.Size([499, 249, 2, 2]) from checkpoint, the shape in current model is torch.Size([384, 192, 2, 2]).
	size mismatch for downsample_layers.2.1.bias: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for downsample_layers.3.0.weight: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for downsample_layers.3.0.bias: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for downsample_layers.3.1.weight: copying a param with shape torch.Size([998, 499, 2, 2]) from checkpoint, the shape in current model is torch.Size([768, 384, 2, 2]).
	size mismatch for downsample_layers.3.1.bias: copying a param with shape torch.Size([998]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for stages.0.0.gamma: copying a param with shape torch.Size([124]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for stages.0.0.large_kernel.small_conv.conv.weight: copying a param with shape torch.Size([124, 1, 5, 5]) from checkpoint, the shape in current model is torch.Size([96, 1, 5, 5]).
	size mismatch for stages.0.0.large_kernel.small_conv.bn.weight: copying a param with shape torch.Size([124]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for stages.0.0.large_kernel.small_conv.bn.bias: copying a param with shape torch.Size([124]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for stages.0.0.large_kernel.small_conv.bn.running_mean: copying a param with shape torch.Size([124]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for stages.0.0.large_kernel.small_conv.bn.running_var: copying a param with shape torch.Size([124]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for stages.0.0.norm.weight: copying a param with shape torch.Size([124]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for stages.0.0.norm.bias: copying a param with shape torch.Size([124]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for stages.0.0.pwconv1.weight: copying a param with shape torch.Size([496, 124]) from checkpoint, the shape in current model is torch.Size([384, 96]).
	size mismatch for stages.0.0.pwconv1.bias: copying a param with shape torch.Size([496]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.0.0.pwconv2.weight: copying a param with shape torch.Size([124, 496]) from checkpoint, the shape in current model is torch.Size([96, 384]).
	size mismatch for stages.0.0.pwconv2.bias: copying a param with shape torch.Size([124]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for stages.0.1.gamma: copying a param with shape torch.Size([124]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for stages.0.1.large_kernel.small_conv.conv.weight: copying a param with shape torch.Size([124, 1, 5, 5]) from checkpoint, the shape in current model is torch.Size([96, 1, 5, 5]).
	size mismatch for stages.0.1.large_kernel.small_conv.bn.weight: copying a param with shape torch.Size([124]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for stages.0.1.large_kernel.small_conv.bn.bias: copying a param with shape torch.Size([124]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for stages.0.1.large_kernel.small_conv.bn.running_mean: copying a param with shape torch.Size([124]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for stages.0.1.large_kernel.small_conv.bn.running_var: copying a param with shape torch.Size([124]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for stages.0.1.norm.weight: copying a param with shape torch.Size([124]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for stages.0.1.norm.bias: copying a param with shape torch.Size([124]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for stages.0.1.pwconv1.weight: copying a param with shape torch.Size([496, 124]) from checkpoint, the shape in current model is torch.Size([384, 96]).
	size mismatch for stages.0.1.pwconv1.bias: copying a param with shape torch.Size([496]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.0.1.pwconv2.weight: copying a param with shape torch.Size([124, 496]) from checkpoint, the shape in current model is torch.Size([96, 384]).
	size mismatch for stages.0.1.pwconv2.bias: copying a param with shape torch.Size([124]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for stages.0.2.gamma: copying a param with shape torch.Size([124]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for stages.0.2.large_kernel.small_conv.conv.weight: copying a param with shape torch.Size([124, 1, 5, 5]) from checkpoint, the shape in current model is torch.Size([96, 1, 5, 5]).
	size mismatch for stages.0.2.large_kernel.small_conv.bn.weight: copying a param with shape torch.Size([124]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for stages.0.2.large_kernel.small_conv.bn.bias: copying a param with shape torch.Size([124]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for stages.0.2.large_kernel.small_conv.bn.running_mean: copying a param with shape torch.Size([124]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for stages.0.2.large_kernel.small_conv.bn.running_var: copying a param with shape torch.Size([124]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for stages.0.2.norm.weight: copying a param with shape torch.Size([124]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for stages.0.2.norm.bias: copying a param with shape torch.Size([124]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for stages.0.2.pwconv1.weight: copying a param with shape torch.Size([496, 124]) from checkpoint, the shape in current model is torch.Size([384, 96]).
	size mismatch for stages.0.2.pwconv1.bias: copying a param with shape torch.Size([496]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.0.2.pwconv2.weight: copying a param with shape torch.Size([124, 496]) from checkpoint, the shape in current model is torch.Size([96, 384]).
	size mismatch for stages.0.2.pwconv2.bias: copying a param with shape torch.Size([124]) from checkpoint, the shape in current model is torch.Size([96]).
	size mismatch for stages.1.0.gamma: copying a param with shape torch.Size([249]) from checkpoint, the shape in current model is torch.Size([192]).
	size mismatch for stages.1.0.large_kernel.small_conv.conv.weight: copying a param with shape torch.Size([249, 1, 5, 5]) from checkpoint, the shape in current model is torch.Size([192, 1, 5, 5]).
	size mismatch for stages.1.0.large_kernel.small_conv.bn.weight: copying a param with shape torch.Size([249]) from checkpoint, the shape in current model is torch.Size([192]).
	size mismatch for stages.1.0.large_kernel.small_conv.bn.bias: copying a param with shape torch.Size([249]) from checkpoint, the shape in current model is torch.Size([192]).
	size mismatch for stages.1.0.large_kernel.small_conv.bn.running_mean: copying a param with shape torch.Size([249]) from checkpoint, the shape in current model is torch.Size([192]).
	size mismatch for stages.1.0.large_kernel.small_conv.bn.running_var: copying a param with shape torch.Size([249]) from checkpoint, the shape in current model is torch.Size([192]).
	size mismatch for stages.1.0.norm.weight: copying a param with shape torch.Size([249]) from checkpoint, the shape in current model is torch.Size([192]).
	size mismatch for stages.1.0.norm.bias: copying a param with shape torch.Size([249]) from checkpoint, the shape in current model is torch.Size([192]).
	size mismatch for stages.1.0.pwconv1.weight: copying a param with shape torch.Size([996, 249]) from checkpoint, the shape in current model is torch.Size([768, 192]).
	size mismatch for stages.1.0.pwconv1.bias: copying a param with shape torch.Size([996]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for stages.1.0.pwconv2.weight: copying a param with shape torch.Size([249, 996]) from checkpoint, the shape in current model is torch.Size([192, 768]).
	size mismatch for stages.1.0.pwconv2.bias: copying a param with shape torch.Size([249]) from checkpoint, the shape in current model is torch.Size([192]).
	size mismatch for stages.1.1.gamma: copying a param with shape torch.Size([249]) from checkpoint, the shape in current model is torch.Size([192]).
	size mismatch for stages.1.1.large_kernel.small_conv.conv.weight: copying a param with shape torch.Size([249, 1, 5, 5]) from checkpoint, the shape in current model is torch.Size([192, 1, 5, 5]).
	size mismatch for stages.1.1.large_kernel.small_conv.bn.weight: copying a param with shape torch.Size([249]) from checkpoint, the shape in current model is torch.Size([192]).
	size mismatch for stages.1.1.large_kernel.small_conv.bn.bias: copying a param with shape torch.Size([249]) from checkpoint, the shape in current model is torch.Size([192]).
	size mismatch for stages.1.1.large_kernel.small_conv.bn.running_mean: copying a param with shape torch.Size([249]) from checkpoint, the shape in current model is torch.Size([192]).
	size mismatch for stages.1.1.large_kernel.small_conv.bn.running_var: copying a param with shape torch.Size([249]) from checkpoint, the shape in current model is torch.Size([192]).
	size mismatch for stages.1.1.norm.weight: copying a param with shape torch.Size([249]) from checkpoint, the shape in current model is torch.Size([192]).
	size mismatch for stages.1.1.norm.bias: copying a param with shape torch.Size([249]) from checkpoint, the shape in current model is torch.Size([192]).
	size mismatch for stages.1.1.pwconv1.weight: copying a param with shape torch.Size([996, 249]) from checkpoint, the shape in current model is torch.Size([768, 192]).
	size mismatch for stages.1.1.pwconv1.bias: copying a param with shape torch.Size([996]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for stages.1.1.pwconv2.weight: copying a param with shape torch.Size([249, 996]) from checkpoint, the shape in current model is torch.Size([192, 768]).
	size mismatch for stages.1.1.pwconv2.bias: copying a param with shape torch.Size([249]) from checkpoint, the shape in current model is torch.Size([192]).
	size mismatch for stages.1.2.gamma: copying a param with shape torch.Size([249]) from checkpoint, the shape in current model is torch.Size([192]).
	size mismatch for stages.1.2.large_kernel.small_conv.conv.weight: copying a param with shape torch.Size([249, 1, 5, 5]) from checkpoint, the shape in current model is torch.Size([192, 1, 5, 5]).
	size mismatch for stages.1.2.large_kernel.small_conv.bn.weight: copying a param with shape torch.Size([249]) from checkpoint, the shape in current model is torch.Size([192]).
	size mismatch for stages.1.2.large_kernel.small_conv.bn.bias: copying a param with shape torch.Size([249]) from checkpoint, the shape in current model is torch.Size([192]).
	size mismatch for stages.1.2.large_kernel.small_conv.bn.running_mean: copying a param with shape torch.Size([249]) from checkpoint, the shape in current model is torch.Size([192]).
	size mismatch for stages.1.2.large_kernel.small_conv.bn.running_var: copying a param with shape torch.Size([249]) from checkpoint, the shape in current model is torch.Size([192]).
	size mismatch for stages.1.2.norm.weight: copying a param with shape torch.Size([249]) from checkpoint, the shape in current model is torch.Size([192]).
	size mismatch for stages.1.2.norm.bias: copying a param with shape torch.Size([249]) from checkpoint, the shape in current model is torch.Size([192]).
	size mismatch for stages.1.2.pwconv1.weight: copying a param with shape torch.Size([996, 249]) from checkpoint, the shape in current model is torch.Size([768, 192]).
	size mismatch for stages.1.2.pwconv1.bias: copying a param with shape torch.Size([996]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for stages.1.2.pwconv2.weight: copying a param with shape torch.Size([249, 996]) from checkpoint, the shape in current model is torch.Size([192, 768]).
	size mismatch for stages.1.2.pwconv2.bias: copying a param with shape torch.Size([249]) from checkpoint, the shape in current model is torch.Size([192]).
	size mismatch for stages.2.0.gamma: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.0.large_kernel.small_conv.conv.weight: copying a param with shape torch.Size([499, 1, 5, 5]) from checkpoint, the shape in current model is torch.Size([384, 1, 5, 5]).
	size mismatch for stages.2.0.large_kernel.small_conv.bn.weight: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.0.large_kernel.small_conv.bn.bias: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.0.large_kernel.small_conv.bn.running_mean: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.0.large_kernel.small_conv.bn.running_var: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.0.norm.weight: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.0.norm.bias: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.0.pwconv1.weight: copying a param with shape torch.Size([1996, 499]) from checkpoint, the shape in current model is torch.Size([1536, 384]).
	size mismatch for stages.2.0.pwconv1.bias: copying a param with shape torch.Size([1996]) from checkpoint, the shape in current model is torch.Size([1536]).
	size mismatch for stages.2.0.pwconv2.weight: copying a param with shape torch.Size([499, 1996]) from checkpoint, the shape in current model is torch.Size([384, 1536]).
	size mismatch for stages.2.0.pwconv2.bias: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.1.gamma: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.1.large_kernel.small_conv.conv.weight: copying a param with shape torch.Size([499, 1, 5, 5]) from checkpoint, the shape in current model is torch.Size([384, 1, 5, 5]).
	size mismatch for stages.2.1.large_kernel.small_conv.bn.weight: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.1.large_kernel.small_conv.bn.bias: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.1.large_kernel.small_conv.bn.running_mean: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.1.large_kernel.small_conv.bn.running_var: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.1.norm.weight: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.1.norm.bias: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.1.pwconv1.weight: copying a param with shape torch.Size([1996, 499]) from checkpoint, the shape in current model is torch.Size([1536, 384]).
	size mismatch for stages.2.1.pwconv1.bias: copying a param with shape torch.Size([1996]) from checkpoint, the shape in current model is torch.Size([1536]).
	size mismatch for stages.2.1.pwconv2.weight: copying a param with shape torch.Size([499, 1996]) from checkpoint, the shape in current model is torch.Size([384, 1536]).
	size mismatch for stages.2.1.pwconv2.bias: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.2.gamma: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.2.large_kernel.small_conv.conv.weight: copying a param with shape torch.Size([499, 1, 5, 5]) from checkpoint, the shape in current model is torch.Size([384, 1, 5, 5]).
	size mismatch for stages.2.2.large_kernel.small_conv.bn.weight: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.2.large_kernel.small_conv.bn.bias: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.2.large_kernel.small_conv.bn.running_mean: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.2.large_kernel.small_conv.bn.running_var: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.2.norm.weight: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.2.norm.bias: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.2.pwconv1.weight: copying a param with shape torch.Size([1996, 499]) from checkpoint, the shape in current model is torch.Size([1536, 384]).
	size mismatch for stages.2.2.pwconv1.bias: copying a param with shape torch.Size([1996]) from checkpoint, the shape in current model is torch.Size([1536]).
	size mismatch for stages.2.2.pwconv2.weight: copying a param with shape torch.Size([499, 1996]) from checkpoint, the shape in current model is torch.Size([384, 1536]).
	size mismatch for stages.2.2.pwconv2.bias: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.3.gamma: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.3.large_kernel.small_conv.conv.weight: copying a param with shape torch.Size([499, 1, 5, 5]) from checkpoint, the shape in current model is torch.Size([384, 1, 5, 5]).
	size mismatch for stages.2.3.large_kernel.small_conv.bn.weight: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.3.large_kernel.small_conv.bn.bias: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.3.large_kernel.small_conv.bn.running_mean: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.3.large_kernel.small_conv.bn.running_var: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.3.norm.weight: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.3.norm.bias: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.3.pwconv1.weight: copying a param with shape torch.Size([1996, 499]) from checkpoint, the shape in current model is torch.Size([1536, 384]).
	size mismatch for stages.2.3.pwconv1.bias: copying a param with shape torch.Size([1996]) from checkpoint, the shape in current model is torch.Size([1536]).
	size mismatch for stages.2.3.pwconv2.weight: copying a param with shape torch.Size([499, 1996]) from checkpoint, the shape in current model is torch.Size([384, 1536]).
	size mismatch for stages.2.3.pwconv2.bias: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.4.gamma: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.4.large_kernel.small_conv.conv.weight: copying a param with shape torch.Size([499, 1, 5, 5]) from checkpoint, the shape in current model is torch.Size([384, 1, 5, 5]).
	size mismatch for stages.2.4.large_kernel.small_conv.bn.weight: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.4.large_kernel.small_conv.bn.bias: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.4.large_kernel.small_conv.bn.running_mean: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.4.large_kernel.small_conv.bn.running_var: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.4.norm.weight: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.4.norm.bias: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.4.pwconv1.weight: copying a param with shape torch.Size([1996, 499]) from checkpoint, the shape in current model is torch.Size([1536, 384]).
	size mismatch for stages.2.4.pwconv1.bias: copying a param with shape torch.Size([1996]) from checkpoint, the shape in current model is torch.Size([1536]).
	size mismatch for stages.2.4.pwconv2.weight: copying a param with shape torch.Size([499, 1996]) from checkpoint, the shape in current model is torch.Size([384, 1536]).
	size mismatch for stages.2.4.pwconv2.bias: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.5.gamma: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.5.large_kernel.small_conv.conv.weight: copying a param with shape torch.Size([499, 1, 5, 5]) from checkpoint, the shape in current model is torch.Size([384, 1, 5, 5]).
	size mismatch for stages.2.5.large_kernel.small_conv.bn.weight: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.5.large_kernel.small_conv.bn.bias: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.5.large_kernel.small_conv.bn.running_mean: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.5.large_kernel.small_conv.bn.running_var: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.5.norm.weight: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.5.norm.bias: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.5.pwconv1.weight: copying a param with shape torch.Size([1996, 499]) from checkpoint, the shape in current model is torch.Size([1536, 384]).
	size mismatch for stages.2.5.pwconv1.bias: copying a param with shape torch.Size([1996]) from checkpoint, the shape in current model is torch.Size([1536]).
	size mismatch for stages.2.5.pwconv2.weight: copying a param with shape torch.Size([499, 1996]) from checkpoint, the shape in current model is torch.Size([384, 1536]).
	size mismatch for stages.2.5.pwconv2.bias: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.6.gamma: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.6.large_kernel.small_conv.conv.weight: copying a param with shape torch.Size([499, 1, 5, 5]) from checkpoint, the shape in current model is torch.Size([384, 1, 5, 5]).
	size mismatch for stages.2.6.large_kernel.small_conv.bn.weight: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.6.large_kernel.small_conv.bn.bias: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.6.large_kernel.small_conv.bn.running_mean: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.6.large_kernel.small_conv.bn.running_var: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.6.norm.weight: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.6.norm.bias: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.6.pwconv1.weight: copying a param with shape torch.Size([1996, 499]) from checkpoint, the shape in current model is torch.Size([1536, 384]).
	size mismatch for stages.2.6.pwconv1.bias: copying a param with shape torch.Size([1996]) from checkpoint, the shape in current model is torch.Size([1536]).
	size mismatch for stages.2.6.pwconv2.weight: copying a param with shape torch.Size([499, 1996]) from checkpoint, the shape in current model is torch.Size([384, 1536]).
	size mismatch for stages.2.6.pwconv2.bias: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.7.gamma: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.7.large_kernel.small_conv.conv.weight: copying a param with shape torch.Size([499, 1, 5, 5]) from checkpoint, the shape in current model is torch.Size([384, 1, 5, 5]).
	size mismatch for stages.2.7.large_kernel.small_conv.bn.weight: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.7.large_kernel.small_conv.bn.bias: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.7.large_kernel.small_conv.bn.running_mean: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.7.large_kernel.small_conv.bn.running_var: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.7.norm.weight: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.7.norm.bias: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.7.pwconv1.weight: copying a param with shape torch.Size([1996, 499]) from checkpoint, the shape in current model is torch.Size([1536, 384]).
	size mismatch for stages.2.7.pwconv1.bias: copying a param with shape torch.Size([1996]) from checkpoint, the shape in current model is torch.Size([1536]).
	size mismatch for stages.2.7.pwconv2.weight: copying a param with shape torch.Size([499, 1996]) from checkpoint, the shape in current model is torch.Size([384, 1536]).
	size mismatch for stages.2.7.pwconv2.bias: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.8.gamma: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.8.large_kernel.small_conv.conv.weight: copying a param with shape torch.Size([499, 1, 5, 5]) from checkpoint, the shape in current model is torch.Size([384, 1, 5, 5]).
	size mismatch for stages.2.8.large_kernel.small_conv.bn.weight: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.8.large_kernel.small_conv.bn.bias: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.8.large_kernel.small_conv.bn.running_mean: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.8.large_kernel.small_conv.bn.running_var: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.8.norm.weight: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.8.norm.bias: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.2.8.pwconv1.weight: copying a param with shape torch.Size([1996, 499]) from checkpoint, the shape in current model is torch.Size([1536, 384]).
	size mismatch for stages.2.8.pwconv1.bias: copying a param with shape torch.Size([1996]) from checkpoint, the shape in current model is torch.Size([1536]).
	size mismatch for stages.2.8.pwconv2.weight: copying a param with shape torch.Size([499, 1996]) from checkpoint, the shape in current model is torch.Size([384, 1536]).
	size mismatch for stages.2.8.pwconv2.bias: copying a param with shape torch.Size([499]) from checkpoint, the shape in current model is torch.Size([384]).
	size mismatch for stages.3.0.gamma: copying a param with shape torch.Size([998]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for stages.3.0.large_kernel.small_conv.conv.weight: copying a param with shape torch.Size([998, 1, 5, 5]) from checkpoint, the shape in current model is torch.Size([768, 1, 5, 5]).
	size mismatch for stages.3.0.large_kernel.small_conv.bn.weight: copying a param with shape torch.Size([998]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for stages.3.0.large_kernel.small_conv.bn.bias: copying a param with shape torch.Size([998]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for stages.3.0.large_kernel.small_conv.bn.running_mean: copying a param with shape torch.Size([998]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for stages.3.0.large_kernel.small_conv.bn.running_var: copying a param with shape torch.Size([998]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for stages.3.0.norm.weight: copying a param with shape torch.Size([998]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for stages.3.0.norm.bias: copying a param with shape torch.Size([998]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for stages.3.0.pwconv1.weight: copying a param with shape torch.Size([3992, 998]) from checkpoint, the shape in current model is torch.Size([3072, 768]).
	size mismatch for stages.3.0.pwconv1.bias: copying a param with shape torch.Size([3992]) from checkpoint, the shape in current model is torch.Size([3072]).
	size mismatch for stages.3.0.pwconv2.weight: copying a param with shape torch.Size([998, 3992]) from checkpoint, the shape in current model is torch.Size([768, 3072]).
	size mismatch for stages.3.0.pwconv2.bias: copying a param with shape torch.Size([998]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for stages.3.1.gamma: copying a param with shape torch.Size([998]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for stages.3.1.large_kernel.small_conv.conv.weight: copying a param with shape torch.Size([998, 1, 5, 5]) from checkpoint, the shape in current model is torch.Size([768, 1, 5, 5]).
	size mismatch for stages.3.1.large_kernel.small_conv.bn.weight: copying a param with shape torch.Size([998]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for stages.3.1.large_kernel.small_conv.bn.bias: copying a param with shape torch.Size([998]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for stages.3.1.large_kernel.small_conv.bn.running_mean: copying a param with shape torch.Size([998]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for stages.3.1.large_kernel.small_conv.bn.running_var: copying a param with shape torch.Size([998]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for stages.3.1.norm.weight: copying a param with shape torch.Size([998]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for stages.3.1.norm.bias: copying a param with shape torch.Size([998]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for stages.3.1.pwconv1.weight: copying a param with shape torch.Size([3992, 998]) from checkpoint, the shape in current model is torch.Size([3072, 768]).
	size mismatch for stages.3.1.pwconv1.bias: copying a param with shape torch.Size([3992]) from checkpoint, the shape in current model is torch.Size([3072]).
	size mismatch for stages.3.1.pwconv2.weight: copying a param with shape torch.Size([998, 3992]) from checkpoint, the shape in current model is torch.Size([768, 3072]).
	size mismatch for stages.3.1.pwconv2.bias: copying a param with shape torch.Size([998]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for stages.3.2.gamma: copying a param with shape torch.Size([998]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for stages.3.2.large_kernel.small_conv.conv.weight: copying a param with shape torch.Size([998, 1, 5, 5]) from checkpoint, the shape in current model is torch.Size([768, 1, 5, 5]).
	size mismatch for stages.3.2.large_kernel.small_conv.bn.weight: copying a param with shape torch.Size([998]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for stages.3.2.large_kernel.small_conv.bn.bias: copying a param with shape torch.Size([998]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for stages.3.2.large_kernel.small_conv.bn.running_mean: copying a param with shape torch.Size([998]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for stages.3.2.large_kernel.small_conv.bn.running_var: copying a param with shape torch.Size([998]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for stages.3.2.norm.weight: copying a param with shape torch.Size([998]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for stages.3.2.norm.bias: copying a param with shape torch.Size([998]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for stages.3.2.pwconv1.weight: copying a param with shape torch.Size([3992, 998]) from checkpoint, the shape in current model is torch.Size([3072, 768]).
	size mismatch for stages.3.2.pwconv1.bias: copying a param with shape torch.Size([3992]) from checkpoint, the shape in current model is torch.Size([3072]).
	size mismatch for stages.3.2.pwconv2.weight: copying a param with shape torch.Size([998, 3992]) from checkpoint, the shape in current model is torch.Size([768, 3072]).
	size mismatch for stages.3.2.pwconv2.bias: copying a param with shape torch.Size([998]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for norm.weight: copying a param with shape torch.Size([998]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for norm.bias: copying a param with shape torch.Size([998]) from checkpoint, the shape in current model is torch.Size([768]).
	size mismatch for head.weight: copying a param with shape torch.Size([1000, 998]) from checkpoint, the shape in current model is torch.Size([1000, 768]).
Not using distributed mode
Namespace(Decom=False, aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=64, bn=True, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/sliu/project_space/datasets/sketch', data_set='IMNET', data_type='imagenet-s', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.1, enable_wandb=False, epochs=300, eval=True, eval_data_path=None, finetune='', fix=False, growth='random', head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, kernel_size=[7, 7, 7, 7, 100], layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=-1, log_dir=None, lr=0.004, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='SLaK_tiny', model_ema=False, model_ema_decay=0.9999, model_ema_eval=False, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_workers=10, only_L=False, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', parallel=True, pin_mem=True, project='SLaK', prune='magnitude', prune_rate=0.3, recount=1, redistribution='none', remode='pixel', reprob=0.25, resplit=False, resume='/gpfs/work3/0/prjste21060/LoRA_LK/transfer/convnext_tiny/120epochs/checkpoint-best.pth', save_ckpt=True, save_ckpt_freq=10, save_ckpt_num=3, seed=0, smoothing=0.1, sparse=False, sparse_init='snip', sparsity=0.4, start_epoch=0, train_interpolation='bicubic', update_freq=1, update_frequency=100, use_amp=False, verbose=False, wandb_ckpt=False, warmup_epochs=20, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, width_factor=1.0, world_size=1)
Transform = 
Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /home/sliu/project_space/datasets/sketch
Number of the class = 1000
Mixup is activated!
Model = SLaK(
  (downsample_layers): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (1): LayerNorm()
    )
    (1): Sequential(
      (0): LayerNorm()
      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))
    )
    (2): Sequential(
      (0): LayerNorm()
      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))
    )
    (3): Sequential(
      (0): LayerNorm()
      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (stages): ModuleList(
    (0): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): Identity()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): DropPath()
      )
    )
    (1): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
    )
    (2): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (3): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (4): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (5): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (6): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (7): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (8): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
    )
    (3): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
number of params: 28595752
LR = 0.00400000
Batch size = 64
Update frequent = 1
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "downsample_layers.0.0.weight",
      "downsample_layers.1.1.weight",
      "downsample_layers.2.1.weight",
      "downsample_layers.3.1.weight",
      "stages.0.0.large_kernel.lkb_origin.conv.weight",
      "stages.0.0.pwconv1.weight",
      "stages.0.0.pwconv2.weight",
      "stages.0.1.large_kernel.lkb_origin.conv.weight",
      "stages.0.1.pwconv1.weight",
      "stages.0.1.pwconv2.weight",
      "stages.0.2.large_kernel.lkb_origin.conv.weight",
      "stages.0.2.pwconv1.weight",
      "stages.0.2.pwconv2.weight",
      "stages.1.0.large_kernel.lkb_origin.conv.weight",
      "stages.1.0.pwconv1.weight",
      "stages.1.0.pwconv2.weight",
      "stages.1.1.large_kernel.lkb_origin.conv.weight",
      "stages.1.1.pwconv1.weight",
      "stages.1.1.pwconv2.weight",
      "stages.1.2.large_kernel.lkb_origin.conv.weight",
      "stages.1.2.pwconv1.weight",
      "stages.1.2.pwconv2.weight",
      "stages.2.0.large_kernel.lkb_origin.conv.weight",
      "stages.2.0.pwconv1.weight",
      "stages.2.0.pwconv2.weight",
      "stages.2.1.large_kernel.lkb_origin.conv.weight",
      "stages.2.1.pwconv1.weight",
      "stages.2.1.pwconv2.weight",
      "stages.2.2.large_kernel.lkb_origin.conv.weight",
      "stages.2.2.pwconv1.weight",
      "stages.2.2.pwconv2.weight",
      "stages.2.3.large_kernel.lkb_origin.conv.weight",
      "stages.2.3.pwconv1.weight",
      "stages.2.3.pwconv2.weight",
      "stages.2.4.large_kernel.lkb_origin.conv.weight",
      "stages.2.4.pwconv1.weight",
      "stages.2.4.pwconv2.weight",
      "stages.2.5.large_kernel.lkb_origin.conv.weight",
      "stages.2.5.pwconv1.weight",
      "stages.2.5.pwconv2.weight",
      "stages.2.6.large_kernel.lkb_origin.conv.weight",
      "stages.2.6.pwconv1.weight",
      "stages.2.6.pwconv2.weight",
      "stages.2.7.large_kernel.lkb_origin.conv.weight",
      "stages.2.7.pwconv1.weight",
      "stages.2.7.pwconv2.weight",
      "stages.2.8.large_kernel.lkb_origin.conv.weight",
      "stages.2.8.pwconv1.weight",
      "stages.2.8.pwconv2.weight",
      "stages.3.0.large_kernel.lkb_origin.conv.weight",
      "stages.3.0.pwconv1.weight",
      "stages.3.0.pwconv2.weight",
      "stages.3.1.large_kernel.lkb_origin.conv.weight",
      "stages.3.1.pwconv1.weight",
      "stages.3.1.pwconv2.weight",
      "stages.3.2.large_kernel.lkb_origin.conv.weight",
      "stages.3.2.pwconv1.weight",
      "stages.3.2.pwconv2.weight",
      "head.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "downsample_layers.0.0.bias",
      "downsample_layers.0.1.weight",
      "downsample_layers.0.1.bias",
      "downsample_layers.1.0.weight",
      "downsample_layers.1.0.bias",
      "downsample_layers.1.1.bias",
      "downsample_layers.2.0.weight",
      "downsample_layers.2.0.bias",
      "downsample_layers.2.1.bias",
      "downsample_layers.3.0.weight",
      "downsample_layers.3.0.bias",
      "downsample_layers.3.1.bias",
      "stages.0.0.gamma",
      "stages.0.0.large_kernel.lkb_origin.bn.weight",
      "stages.0.0.large_kernel.lkb_origin.bn.bias",
      "stages.0.0.norm.weight",
      "stages.0.0.norm.bias",
      "stages.0.0.pwconv1.bias",
      "stages.0.0.pwconv2.bias",
      "stages.0.1.gamma",
      "stages.0.1.large_kernel.lkb_origin.bn.weight",
      "stages.0.1.large_kernel.lkb_origin.bn.bias",
      "stages.0.1.norm.weight",
      "stages.0.1.norm.bias",
      "stages.0.1.pwconv1.bias",
      "stages.0.1.pwconv2.bias",
      "stages.0.2.gamma",
      "stages.0.2.large_kernel.lkb_origin.bn.weight",
      "stages.0.2.large_kernel.lkb_origin.bn.bias",
      "stages.0.2.norm.weight",
      "stages.0.2.norm.bias",
      "stages.0.2.pwconv1.bias",
      "stages.0.2.pwconv2.bias",
      "stages.1.0.gamma",
      "stages.1.0.large_kernel.lkb_origin.bn.weight",
      "stages.1.0.large_kernel.lkb_origin.bn.bias",
      "stages.1.0.norm.weight",
      "stages.1.0.norm.bias",
      "stages.1.0.pwconv1.bias",
      "stages.1.0.pwconv2.bias",
      "stages.1.1.gamma",
      "stages.1.1.large_kernel.lkb_origin.bn.weight",
      "stages.1.1.large_kernel.lkb_origin.bn.bias",
      "stages.1.1.norm.weight",
      "stages.1.1.norm.bias",
      "stages.1.1.pwconv1.bias",
      "stages.1.1.pwconv2.bias",
      "stages.1.2.gamma",
      "stages.1.2.large_kernel.lkb_origin.bn.weight",
      "stages.1.2.large_kernel.lkb_origin.bn.bias",
      "stages.1.2.norm.weight",
      "stages.1.2.norm.bias",
      "stages.1.2.pwconv1.bias",
      "stages.1.2.pwconv2.bias",
      "stages.2.0.gamma",
      "stages.2.0.large_kernel.lkb_origin.bn.weight",
      "stages.2.0.large_kernel.lkb_origin.bn.bias",
      "stages.2.0.norm.weight",
      "stages.2.0.norm.bias",
      "stages.2.0.pwconv1.bias",
      "stages.2.0.pwconv2.bias",
      "stages.2.1.gamma",
      "stages.2.1.large_kernel.lkb_origin.bn.weight",
      "stages.2.1.large_kernel.lkb_origin.bn.bias",
      "stages.2.1.norm.weight",
      "stages.2.1.norm.bias",
      "stages.2.1.pwconv1.bias",
      "stages.2.1.pwconv2.bias",
      "stages.2.2.gamma",
      "stages.2.2.large_kernel.lkb_origin.bn.weight",
      "stages.2.2.large_kernel.lkb_origin.bn.bias",
      "stages.2.2.norm.weight",
      "stages.2.2.norm.bias",
      "stages.2.2.pwconv1.bias",
      "stages.2.2.pwconv2.bias",
      "stages.2.3.gamma",
      "stages.2.3.large_kernel.lkb_origin.bn.weight",
      "stages.2.3.large_kernel.lkb_origin.bn.bias",
      "stages.2.3.norm.weight",
      "stages.2.3.norm.bias",
      "stages.2.3.pwconv1.bias",
      "stages.2.3.pwconv2.bias",
      "stages.2.4.gamma",
      "stages.2.4.large_kernel.lkb_origin.bn.weight",
      "stages.2.4.large_kernel.lkb_origin.bn.bias",
      "stages.2.4.norm.weight",
      "stages.2.4.norm.bias",
      "stages.2.4.pwconv1.bias",
      "stages.2.4.pwconv2.bias",
      "stages.2.5.gamma",
      "stages.2.5.large_kernel.lkb_origin.bn.weight",
      "stages.2.5.large_kernel.lkb_origin.bn.bias",
      "stages.2.5.norm.weight",
      "stages.2.5.norm.bias",
      "stages.2.5.pwconv1.bias",
      "stages.2.5.pwconv2.bias",
      "stages.2.6.gamma",
      "stages.2.6.large_kernel.lkb_origin.bn.weight",
      "stages.2.6.large_kernel.lkb_origin.bn.bias",
      "stages.2.6.norm.weight",
      "stages.2.6.norm.bias",
      "stages.2.6.pwconv1.bias",
      "stages.2.6.pwconv2.bias",
      "stages.2.7.gamma",
      "stages.2.7.large_kernel.lkb_origin.bn.weight",
      "stages.2.7.large_kernel.lkb_origin.bn.bias",
      "stages.2.7.norm.weight",
      "stages.2.7.norm.bias",
      "stages.2.7.pwconv1.bias",
      "stages.2.7.pwconv2.bias",
      "stages.2.8.gamma",
      "stages.2.8.large_kernel.lkb_origin.bn.weight",
      "stages.2.8.large_kernel.lkb_origin.bn.bias",
      "stages.2.8.norm.weight",
      "stages.2.8.norm.bias",
      "stages.2.8.pwconv1.bias",
      "stages.2.8.pwconv2.bias",
      "stages.3.0.gamma",
      "stages.3.0.large_kernel.lkb_origin.bn.weight",
      "stages.3.0.large_kernel.lkb_origin.bn.bias",
      "stages.3.0.norm.weight",
      "stages.3.0.norm.bias",
      "stages.3.0.pwconv1.bias",
      "stages.3.0.pwconv2.bias",
      "stages.3.1.gamma",
      "stages.3.1.large_kernel.lkb_origin.bn.weight",
      "stages.3.1.large_kernel.lkb_origin.bn.bias",
      "stages.3.1.norm.weight",
      "stages.3.1.norm.bias",
      "stages.3.1.pwconv1.bias",
      "stages.3.1.pwconv2.bias",
      "stages.3.2.gamma",
      "stages.3.2.large_kernel.lkb_origin.bn.weight",
      "stages.3.2.large_kernel.lkb_origin.bn.bias",
      "stages.3.2.norm.weight",
      "stages.3.2.norm.bias",
      "stages.3.2.pwconv1.bias",
      "stages.3.2.pwconv2.bias",
      "norm.weight",
      "norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
criterion = SoftTargetCrossEntropy()
Traceback (most recent call last):
  File "main.py", line 561, in <module>
    main(args)
  File "main.py", line 446, in main
    utils.auto_load_model(
  File "/gpfs/work3/0/prjste21060/projects/datasets/TJ_RobustData/robustness/SLaK/utils.py", line 493, in auto_load_model
    checkpoint = torch.load(args.resume, map_location='cpu')
  File "/home/sliu/miniconda3/envs/slak/lib/python3.8/site-packages/torch/serialization.py", line 594, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/home/sliu/miniconda3/envs/slak/lib/python3.8/site-packages/torch/serialization.py", line 230, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/home/sliu/miniconda3/envs/slak/lib/python3.8/site-packages/torch/serialization.py", line 211, in __init__
    super(_open_file, self).__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: '/gpfs/work3/0/prjste21060/LoRA_LK/transfer/convnext_tiny/120epochs/checkpoint-best.pth'
SLak_kernel_distill_s_T.sh: line 148: source: deactivate: file not found
