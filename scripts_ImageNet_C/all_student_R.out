Not using distributed mode
Namespace(Decom=False, aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=64, bn=False, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/sliu/project_space/datasets/imagenet-r', data_set='IMNET', data_type='imagenet-r', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.1, enable_wandb=False, epochs=300, eval=True, eval_data_path=None, finetune='', fix=False, growth='random', head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, kernel_size=[7, 7, 7, 7, 100], layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=-1, log_dir=None, lr=0.004, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='resnet50', model_ema=False, model_ema_decay=0.9999, model_ema_eval=False, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_workers=10, only_L=False, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', parallel=True, pin_mem=True, project='SLaK', prune='magnitude', prune_rate=0.3, recount=1, redistribution='none', remode='pixel', reprob=0.25, resplit=False, resume='/gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_VitTeacher_STRN50_NKD/checkpoint-best.pth', save_ckpt=True, save_ckpt_freq=10, save_ckpt_num=3, seed=0, smoothing=0.1, sparse=False, sparse_init='snip', sparsity=0.4, start_epoch=0, train_interpolation='bicubic', update_freq=1, update_frequency=100, use_amp=False, verbose=False, wandb_ckpt=False, warmup_epochs=20, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, width_factor=1.0, world_size=1)
Transform = 
Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /home/sliu/project_space/datasets/imagenet-r
Number of the class = 1000
Mixup is activated!
Model = ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=True)
  (fc): Linear(in_features=2048, out_features=1000, bias=True)
)
number of params: 25557032
LR = 0.00400000
Batch size = 64
Update frequent = 1
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "conv1.weight",
      "layer1.0.conv1.weight",
      "layer1.0.conv2.weight",
      "layer1.0.conv3.weight",
      "layer1.0.downsample.0.weight",
      "layer1.1.conv1.weight",
      "layer1.1.conv2.weight",
      "layer1.1.conv3.weight",
      "layer1.2.conv1.weight",
      "layer1.2.conv2.weight",
      "layer1.2.conv3.weight",
      "layer2.0.conv1.weight",
      "layer2.0.conv2.weight",
      "layer2.0.conv3.weight",
      "layer2.0.downsample.0.weight",
      "layer2.1.conv1.weight",
      "layer2.1.conv2.weight",
      "layer2.1.conv3.weight",
      "layer2.2.conv1.weight",
      "layer2.2.conv2.weight",
      "layer2.2.conv3.weight",
      "layer2.3.conv1.weight",
      "layer2.3.conv2.weight",
      "layer2.3.conv3.weight",
      "layer3.0.conv1.weight",
      "layer3.0.conv2.weight",
      "layer3.0.conv3.weight",
      "layer3.0.downsample.0.weight",
      "layer3.1.conv1.weight",
      "layer3.1.conv2.weight",
      "layer3.1.conv3.weight",
      "layer3.2.conv1.weight",
      "layer3.2.conv2.weight",
      "layer3.2.conv3.weight",
      "layer3.3.conv1.weight",
      "layer3.3.conv2.weight",
      "layer3.3.conv3.weight",
      "layer3.4.conv1.weight",
      "layer3.4.conv2.weight",
      "layer3.4.conv3.weight",
      "layer3.5.conv1.weight",
      "layer3.5.conv2.weight",
      "layer3.5.conv3.weight",
      "layer4.0.conv1.weight",
      "layer4.0.conv2.weight",
      "layer4.0.conv3.weight",
      "layer4.0.downsample.0.weight",
      "layer4.1.conv1.weight",
      "layer4.1.conv2.weight",
      "layer4.1.conv3.weight",
      "layer4.2.conv1.weight",
      "layer4.2.conv2.weight",
      "layer4.2.conv3.weight",
      "fc.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "bn1.weight",
      "bn1.bias",
      "layer1.0.bn1.weight",
      "layer1.0.bn1.bias",
      "layer1.0.bn2.weight",
      "layer1.0.bn2.bias",
      "layer1.0.bn3.weight",
      "layer1.0.bn3.bias",
      "layer1.0.downsample.1.weight",
      "layer1.0.downsample.1.bias",
      "layer1.1.bn1.weight",
      "layer1.1.bn1.bias",
      "layer1.1.bn2.weight",
      "layer1.1.bn2.bias",
      "layer1.1.bn3.weight",
      "layer1.1.bn3.bias",
      "layer1.2.bn1.weight",
      "layer1.2.bn1.bias",
      "layer1.2.bn2.weight",
      "layer1.2.bn2.bias",
      "layer1.2.bn3.weight",
      "layer1.2.bn3.bias",
      "layer2.0.bn1.weight",
      "layer2.0.bn1.bias",
      "layer2.0.bn2.weight",
      "layer2.0.bn2.bias",
      "layer2.0.bn3.weight",
      "layer2.0.bn3.bias",
      "layer2.0.downsample.1.weight",
      "layer2.0.downsample.1.bias",
      "layer2.1.bn1.weight",
      "layer2.1.bn1.bias",
      "layer2.1.bn2.weight",
      "layer2.1.bn2.bias",
      "layer2.1.bn3.weight",
      "layer2.1.bn3.bias",
      "layer2.2.bn1.weight",
      "layer2.2.bn1.bias",
      "layer2.2.bn2.weight",
      "layer2.2.bn2.bias",
      "layer2.2.bn3.weight",
      "layer2.2.bn3.bias",
      "layer2.3.bn1.weight",
      "layer2.3.bn1.bias",
      "layer2.3.bn2.weight",
      "layer2.3.bn2.bias",
      "layer2.3.bn3.weight",
      "layer2.3.bn3.bias",
      "layer3.0.bn1.weight",
      "layer3.0.bn1.bias",
      "layer3.0.bn2.weight",
      "layer3.0.bn2.bias",
      "layer3.0.bn3.weight",
      "layer3.0.bn3.bias",
      "layer3.0.downsample.1.weight",
      "layer3.0.downsample.1.bias",
      "layer3.1.bn1.weight",
      "layer3.1.bn1.bias",
      "layer3.1.bn2.weight",
      "layer3.1.bn2.bias",
      "layer3.1.bn3.weight",
      "layer3.1.bn3.bias",
      "layer3.2.bn1.weight",
      "layer3.2.bn1.bias",
      "layer3.2.bn2.weight",
      "layer3.2.bn2.bias",
      "layer3.2.bn3.weight",
      "layer3.2.bn3.bias",
      "layer3.3.bn1.weight",
      "layer3.3.bn1.bias",
      "layer3.3.bn2.weight",
      "layer3.3.bn2.bias",
      "layer3.3.bn3.weight",
      "layer3.3.bn3.bias",
      "layer3.4.bn1.weight",
      "layer3.4.bn1.bias",
      "layer3.4.bn2.weight",
      "layer3.4.bn2.bias",
      "layer3.4.bn3.weight",
      "layer3.4.bn3.bias",
      "layer3.5.bn1.weight",
      "layer3.5.bn1.bias",
      "layer3.5.bn2.weight",
      "layer3.5.bn2.bias",
      "layer3.5.bn3.weight",
      "layer3.5.bn3.bias",
      "layer4.0.bn1.weight",
      "layer4.0.bn1.bias",
      "layer4.0.bn2.weight",
      "layer4.0.bn2.bias",
      "layer4.0.bn3.weight",
      "layer4.0.bn3.bias",
      "layer4.0.downsample.1.weight",
      "layer4.0.downsample.1.bias",
      "layer4.1.bn1.weight",
      "layer4.1.bn1.bias",
      "layer4.1.bn2.weight",
      "layer4.1.bn2.bias",
      "layer4.1.bn3.weight",
      "layer4.1.bn3.bias",
      "layer4.2.bn1.weight",
      "layer4.2.bn1.bias",
      "layer4.2.bn2.weight",
      "layer4.2.bn2.bias",
      "layer4.2.bn3.weight",
      "layer4.2.bn3.bias",
      "fc.bias"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
criterion = SoftTargetCrossEntropy()
dict_keys(['model', 'optimizer', 'epoch', 'scaler', 'args', 'model_ema'])
Resume checkpoint /gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_VitTeacher_STRN50_NKD/checkpoint-best.pth
With optim & sched!
conv1.weight density is 1.0
bn1.weight density is 1.0
bn1.bias density is 1.0
layer1.0.conv1.weight density is 1.0
layer1.0.bn1.weight density is 1.0
layer1.0.bn1.bias density is 1.0
layer1.0.conv2.weight density is 1.0
layer1.0.bn2.weight density is 1.0
layer1.0.bn2.bias density is 1.0
layer1.0.conv3.weight density is 1.0
layer1.0.bn3.weight density is 1.0
layer1.0.bn3.bias density is 1.0
layer1.0.downsample.0.weight density is 1.0
layer1.0.downsample.1.weight density is 1.0
layer1.0.downsample.1.bias density is 1.0
layer1.1.conv1.weight density is 1.0
layer1.1.bn1.weight density is 1.0
layer1.1.bn1.bias density is 1.0
layer1.1.conv2.weight density is 1.0
layer1.1.bn2.weight density is 1.0
layer1.1.bn2.bias density is 1.0
layer1.1.conv3.weight density is 1.0
layer1.1.bn3.weight density is 1.0
layer1.1.bn3.bias density is 1.0
layer1.2.conv1.weight density is 1.0
layer1.2.bn1.weight density is 1.0
layer1.2.bn1.bias density is 1.0
layer1.2.conv2.weight density is 1.0
layer1.2.bn2.weight density is 1.0
layer1.2.bn2.bias density is 1.0
layer1.2.conv3.weight density is 1.0
layer1.2.bn3.weight density is 1.0
layer1.2.bn3.bias density is 1.0
layer2.0.conv1.weight density is 1.0
layer2.0.bn1.weight density is 1.0
layer2.0.bn1.bias density is 1.0
layer2.0.conv2.weight density is 1.0
layer2.0.bn2.weight density is 1.0
layer2.0.bn2.bias density is 1.0
layer2.0.conv3.weight density is 1.0
layer2.0.bn3.weight density is 1.0
layer2.0.bn3.bias density is 1.0
layer2.0.downsample.0.weight density is 1.0
layer2.0.downsample.1.weight density is 1.0
layer2.0.downsample.1.bias density is 1.0
layer2.1.conv1.weight density is 1.0
layer2.1.bn1.weight density is 1.0
layer2.1.bn1.bias density is 1.0
layer2.1.conv2.weight density is 1.0
layer2.1.bn2.weight density is 1.0
layer2.1.bn2.bias density is 1.0
layer2.1.conv3.weight density is 1.0
layer2.1.bn3.weight density is 1.0
layer2.1.bn3.bias density is 1.0
layer2.2.conv1.weight density is 1.0
layer2.2.bn1.weight density is 1.0
layer2.2.bn1.bias density is 1.0
layer2.2.conv2.weight density is 1.0
layer2.2.bn2.weight density is 1.0
layer2.2.bn2.bias density is 1.0
layer2.2.conv3.weight density is 1.0
layer2.2.bn3.weight density is 1.0
layer2.2.bn3.bias density is 1.0
layer2.3.conv1.weight density is 1.0
layer2.3.bn1.weight density is 1.0
layer2.3.bn1.bias density is 1.0
layer2.3.conv2.weight density is 1.0
layer2.3.bn2.weight density is 1.0
layer2.3.bn2.bias density is 1.0
layer2.3.conv3.weight density is 1.0
layer2.3.bn3.weight density is 1.0
layer2.3.bn3.bias density is 1.0
layer3.0.conv1.weight density is 1.0
layer3.0.bn1.weight density is 1.0
layer3.0.bn1.bias density is 1.0
layer3.0.conv2.weight density is 1.0
layer3.0.bn2.weight density is 1.0
layer3.0.bn2.bias density is 1.0
layer3.0.conv3.weight density is 1.0
layer3.0.bn3.weight density is 1.0
layer3.0.bn3.bias density is 1.0
layer3.0.downsample.0.weight density is 1.0
layer3.0.downsample.1.weight density is 1.0
layer3.0.downsample.1.bias density is 1.0
layer3.1.conv1.weight density is 1.0
layer3.1.bn1.weight density is 1.0
layer3.1.bn1.bias density is 1.0
layer3.1.conv2.weight density is 1.0
layer3.1.bn2.weight density is 1.0
layer3.1.bn2.bias density is 1.0
layer3.1.conv3.weight density is 1.0
layer3.1.bn3.weight density is 1.0
layer3.1.bn3.bias density is 1.0
layer3.2.conv1.weight density is 1.0
layer3.2.bn1.weight density is 1.0
layer3.2.bn1.bias density is 1.0
layer3.2.conv2.weight density is 1.0
layer3.2.bn2.weight density is 1.0
layer3.2.bn2.bias density is 1.0
layer3.2.conv3.weight density is 1.0
layer3.2.bn3.weight density is 1.0
layer3.2.bn3.bias density is 1.0
layer3.3.conv1.weight density is 1.0
layer3.3.bn1.weight density is 1.0
layer3.3.bn1.bias density is 1.0
layer3.3.conv2.weight density is 1.0
layer3.3.bn2.weight density is 1.0
layer3.3.bn2.bias density is 1.0
layer3.3.conv3.weight density is 1.0
layer3.3.bn3.weight density is 1.0
layer3.3.bn3.bias density is 1.0
layer3.4.conv1.weight density is 1.0
layer3.4.bn1.weight density is 1.0
layer3.4.bn1.bias density is 1.0
layer3.4.conv2.weight density is 1.0
layer3.4.bn2.weight density is 1.0
layer3.4.bn2.bias density is 1.0
layer3.4.conv3.weight density is 1.0
layer3.4.bn3.weight density is 1.0
layer3.4.bn3.bias density is 1.0
layer3.5.conv1.weight density is 1.0
layer3.5.bn1.weight density is 1.0
layer3.5.bn1.bias density is 1.0
layer3.5.conv2.weight density is 1.0
layer3.5.bn2.weight density is 1.0
layer3.5.bn2.bias density is 1.0
layer3.5.conv3.weight density is 1.0
layer3.5.bn3.weight density is 1.0
layer3.5.bn3.bias density is 1.0
layer4.0.conv1.weight density is 1.0
layer4.0.bn1.weight density is 1.0
layer4.0.bn1.bias density is 1.0
layer4.0.conv2.weight density is 1.0
layer4.0.bn2.weight density is 1.0
layer4.0.bn2.bias density is 1.0
layer4.0.conv3.weight density is 1.0
layer4.0.bn3.weight density is 1.0
layer4.0.bn3.bias density is 1.0
layer4.0.downsample.0.weight density is 1.0
layer4.0.downsample.1.weight density is 1.0
layer4.0.downsample.1.bias density is 1.0
layer4.1.conv1.weight density is 1.0
layer4.1.bn1.weight density is 1.0
layer4.1.bn1.bias density is 1.0
layer4.1.conv2.weight density is 1.0
layer4.1.bn2.weight density is 1.0
layer4.1.bn2.bias density is 1.0
layer4.1.conv3.weight density is 1.0
layer4.1.bn3.weight density is 1.0
layer4.1.bn3.bias density is 1.0
layer4.2.conv1.weight density is 1.0
layer4.2.bn1.weight density is 1.0
layer4.2.bn1.bias density is 1.0
layer4.2.conv2.weight density is 1.0
layer4.2.bn2.weight density is 1.0
layer4.2.bn2.bias density is 1.0
layer4.2.conv3.weight density is 1.0
layer4.2.bn3.weight density is 1.0
layer4.2.bn3.bias density is 1.0
fc.weight density is 1.0
fc.bias density is 1.0
Test:  [  0/313]  eta: 0:17:17  loss: 4.3664 (4.3664)  acc1: 18.7500 (18.7500)  acc5: 29.1667 (29.1667)  time: 3.3143  data: 1.6740  max mem: 3726
Test:  [ 10/313]  eta: 0:01:43  loss: 3.9670 (3.8072)  acc1: 22.9167 (26.4205)  acc5: 37.5000 (42.4242)  time: 0.3403  data: 0.1523  max mem: 3726
Test:  [ 20/313]  eta: 0:00:58  loss: 3.5437 (3.4740)  acc1: 27.0833 (33.3333)  acc5: 44.7917 (47.6687)  time: 0.0426  data: 0.0001  max mem: 3726
Test:  [ 30/313]  eta: 0:00:46  loss: 2.9488 (3.2763)  acc1: 40.6250 (37.2312)  acc5: 56.2500 (51.7137)  time: 0.0649  data: 0.0224  max mem: 3726
Test:  [ 40/313]  eta: 0:00:37  loss: 2.6579 (3.1427)  acc1: 43.7500 (39.8374)  acc5: 62.5000 (53.8364)  time: 0.0746  data: 0.0320  max mem: 3726
Test:  [ 50/313]  eta: 0:00:35  loss: 3.6095 (3.3540)  acc1: 27.0833 (36.9077)  acc5: 45.8333 (50.3268)  time: 0.0892  data: 0.0469  max mem: 3726
Test:  [ 60/313]  eta: 0:00:32  loss: 4.2220 (3.4752)  acc1: 20.8333 (34.5799)  acc5: 34.3750 (48.2753)  time: 0.1095  data: 0.0670  max mem: 3726
Test:  [ 70/313]  eta: 0:00:30  loss: 3.9732 (3.4067)  acc1: 23.9583 (35.8275)  acc5: 37.5000 (49.4718)  time: 0.1028  data: 0.0605  max mem: 3726
Test:  [ 80/313]  eta: 0:00:28  loss: 2.3342 (3.2391)  acc1: 48.9583 (38.1816)  acc5: 68.7500 (52.4949)  time: 0.1050  data: 0.0627  max mem: 3726
Test:  [ 90/313]  eta: 0:00:28  loss: 2.0602 (3.0993)  acc1: 52.0833 (40.2358)  acc5: 73.9583 (55.0595)  time: 0.1279  data: 0.0855  max mem: 3726
Test:  [100/313]  eta: 0:00:26  loss: 2.0602 (3.0143)  acc1: 53.1250 (41.6151)  acc5: 72.9167 (56.4356)  time: 0.1199  data: 0.0776  max mem: 3726
Test:  [110/313]  eta: 0:00:24  loss: 2.4479 (3.0164)  acc1: 46.8750 (41.4977)  acc5: 62.5000 (56.3908)  time: 0.0956  data: 0.0534  max mem: 3726
Test:  [120/313]  eta: 0:00:23  loss: 3.1021 (3.0478)  acc1: 36.4583 (41.0554)  acc5: 54.1667 (55.8798)  time: 0.1117  data: 0.0694  max mem: 3726
Test:  [130/313]  eta: 0:00:21  loss: 3.5736 (3.1318)  acc1: 26.0417 (39.6469)  acc5: 44.7917 (54.5643)  time: 0.0948  data: 0.0520  max mem: 3726
Test:  [140/313]  eta: 0:00:19  loss: 3.6801 (3.1677)  acc1: 27.0833 (39.0145)  acc5: 44.7917 (54.0854)  time: 0.0681  data: 0.0252  max mem: 3726
Test:  [150/313]  eta: 0:00:18  loss: 3.2331 (3.1582)  acc1: 30.2083 (39.2384)  acc5: 50.0000 (54.1736)  time: 0.0904  data: 0.0478  max mem: 3726
Test:  [160/313]  eta: 0:00:17  loss: 3.1702 (3.1987)  acc1: 29.1667 (38.6193)  acc5: 43.7500 (53.4097)  time: 0.1053  data: 0.0628  max mem: 3726
Test:  [170/313]  eta: 0:00:15  loss: 3.8369 (3.2244)  acc1: 28.1250 (38.1213)  acc5: 40.6250 (52.9483)  time: 0.0932  data: 0.0509  max mem: 3726
Test:  [180/313]  eta: 0:00:14  loss: 3.8369 (3.2673)  acc1: 25.0000 (37.4252)  acc5: 39.5833 (52.2215)  time: 0.1001  data: 0.0577  max mem: 3726
Test:  [190/313]  eta: 0:00:13  loss: 2.9789 (3.2273)  acc1: 36.4583 (38.0236)  acc5: 54.1667 (53.0214)  time: 0.1007  data: 0.0583  max mem: 3726
Test:  [200/313]  eta: 0:00:12  loss: 2.9508 (3.2163)  acc1: 39.5833 (38.0908)  acc5: 63.5417 (53.3271)  time: 0.1316  data: 0.0894  max mem: 3726
Test:  [210/313]  eta: 0:00:11  loss: 3.1040 (3.2130)  acc1: 34.3750 (38.0282)  acc5: 58.3333 (53.5792)  time: 0.1268  data: 0.0846  max mem: 3726
Test:  [220/313]  eta: 0:00:10  loss: 3.0129 (3.1817)  acc1: 37.5000 (38.4945)  acc5: 59.3750 (54.1808)  time: 0.0759  data: 0.0336  max mem: 3726
Test:  [230/313]  eta: 0:00:09  loss: 2.5748 (3.1623)  acc1: 40.6250 (38.6183)  acc5: 65.6250 (54.6852)  time: 0.0809  data: 0.0387  max mem: 3726
Test:  [240/313]  eta: 0:00:07  loss: 2.9546 (3.1619)  acc1: 37.5000 (38.7232)  acc5: 55.2083 (54.7113)  time: 0.0837  data: 0.0414  max mem: 3726
Test:  [250/313]  eta: 0:00:06  loss: 2.5433 (3.1241)  acc1: 42.7083 (39.3177)  acc5: 66.6667 (55.4739)  time: 0.0979  data: 0.0556  max mem: 3726
Test:  [260/313]  eta: 0:00:05  loss: 2.4306 (3.1098)  acc1: 47.9167 (39.6392)  acc5: 70.8333 (55.7272)  time: 0.0897  data: 0.0474  max mem: 3726
Test:  [270/313]  eta: 0:00:04  loss: 2.2779 (3.0696)  acc1: 55.2083 (40.3944)  acc5: 71.8750 (56.4576)  time: 0.0844  data: 0.0421  max mem: 3726
Test:  [280/313]  eta: 0:00:03  loss: 2.7785 (3.0924)  acc1: 42.7083 (39.8799)  acc5: 58.3333 (55.9534)  time: 0.0855  data: 0.0432  max mem: 3726
Test:  [290/313]  eta: 0:00:02  loss: 3.8079 (3.1199)  acc1: 22.9167 (39.4294)  acc5: 37.5000 (55.4339)  time: 0.0742  data: 0.0319  max mem: 3726
Test:  [300/313]  eta: 0:00:01  loss: 3.2218 (3.1143)  acc1: 33.3333 (39.3999)  acc5: 54.1667 (55.6098)  time: 0.0865  data: 0.0442  max mem: 3726
Test:  [310/313]  eta: 0:00:00  loss: 3.2915 (3.1342)  acc1: 32.2917 (39.0206)  acc5: 51.0417 (55.2117)  time: 0.0804  data: 0.0382  max mem: 3726
Test:  [312/313]  eta: 0:00:00  loss: 3.2915 (3.1411)  acc1: 31.2500 (38.9133)  acc5: 50.0000 (55.0967)  time: 0.0993  data: 0.0504  max mem: 3726
Test: Total time: 0:00:32 (0.1039 s / it)
* Acc@1 38.913 Acc@5 55.097 loss 3.141
Accuracy of the network on 30000 test images: 38.91333%
Not using distributed mode
Namespace(Decom=False, aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=64, bn=False, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/sliu/project_space/datasets/imagenet-r', data_set='IMNET', data_type='imagenet-r', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.1, enable_wandb=False, epochs=300, eval=True, eval_data_path=None, finetune='', fix=False, growth='random', head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, kernel_size=[7, 7, 7, 7, 100], layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=-1, log_dir=None, lr=0.004, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='resnet50', model_ema=False, model_ema_decay=0.9999, model_ema_eval=False, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_workers=10, only_L=False, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', parallel=True, pin_mem=True, project='SLaK', prune='magnitude', prune_rate=0.3, recount=1, redistribution='none', remode='pixel', reprob=0.25, resplit=False, resume='/gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_SwinTeacher_STRN50_NKD/checkpoint-best.pth', save_ckpt=True, save_ckpt_freq=10, save_ckpt_num=3, seed=0, smoothing=0.1, sparse=False, sparse_init='snip', sparsity=0.4, start_epoch=0, train_interpolation='bicubic', update_freq=1, update_frequency=100, use_amp=False, verbose=False, wandb_ckpt=False, warmup_epochs=20, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, width_factor=1.0, world_size=1)
Transform = 
Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /home/sliu/project_space/datasets/imagenet-r
Number of the class = 1000
Mixup is activated!
Model = ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=True)
  (fc): Linear(in_features=2048, out_features=1000, bias=True)
)
number of params: 25557032
LR = 0.00400000
Batch size = 64
Update frequent = 1
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "conv1.weight",
      "layer1.0.conv1.weight",
      "layer1.0.conv2.weight",
      "layer1.0.conv3.weight",
      "layer1.0.downsample.0.weight",
      "layer1.1.conv1.weight",
      "layer1.1.conv2.weight",
      "layer1.1.conv3.weight",
      "layer1.2.conv1.weight",
      "layer1.2.conv2.weight",
      "layer1.2.conv3.weight",
      "layer2.0.conv1.weight",
      "layer2.0.conv2.weight",
      "layer2.0.conv3.weight",
      "layer2.0.downsample.0.weight",
      "layer2.1.conv1.weight",
      "layer2.1.conv2.weight",
      "layer2.1.conv3.weight",
      "layer2.2.conv1.weight",
      "layer2.2.conv2.weight",
      "layer2.2.conv3.weight",
      "layer2.3.conv1.weight",
      "layer2.3.conv2.weight",
      "layer2.3.conv3.weight",
      "layer3.0.conv1.weight",
      "layer3.0.conv2.weight",
      "layer3.0.conv3.weight",
      "layer3.0.downsample.0.weight",
      "layer3.1.conv1.weight",
      "layer3.1.conv2.weight",
      "layer3.1.conv3.weight",
      "layer3.2.conv1.weight",
      "layer3.2.conv2.weight",
      "layer3.2.conv3.weight",
      "layer3.3.conv1.weight",
      "layer3.3.conv2.weight",
      "layer3.3.conv3.weight",
      "layer3.4.conv1.weight",
      "layer3.4.conv2.weight",
      "layer3.4.conv3.weight",
      "layer3.5.conv1.weight",
      "layer3.5.conv2.weight",
      "layer3.5.conv3.weight",
      "layer4.0.conv1.weight",
      "layer4.0.conv2.weight",
      "layer4.0.conv3.weight",
      "layer4.0.downsample.0.weight",
      "layer4.1.conv1.weight",
      "layer4.1.conv2.weight",
      "layer4.1.conv3.weight",
      "layer4.2.conv1.weight",
      "layer4.2.conv2.weight",
      "layer4.2.conv3.weight",
      "fc.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "bn1.weight",
      "bn1.bias",
      "layer1.0.bn1.weight",
      "layer1.0.bn1.bias",
      "layer1.0.bn2.weight",
      "layer1.0.bn2.bias",
      "layer1.0.bn3.weight",
      "layer1.0.bn3.bias",
      "layer1.0.downsample.1.weight",
      "layer1.0.downsample.1.bias",
      "layer1.1.bn1.weight",
      "layer1.1.bn1.bias",
      "layer1.1.bn2.weight",
      "layer1.1.bn2.bias",
      "layer1.1.bn3.weight",
      "layer1.1.bn3.bias",
      "layer1.2.bn1.weight",
      "layer1.2.bn1.bias",
      "layer1.2.bn2.weight",
      "layer1.2.bn2.bias",
      "layer1.2.bn3.weight",
      "layer1.2.bn3.bias",
      "layer2.0.bn1.weight",
      "layer2.0.bn1.bias",
      "layer2.0.bn2.weight",
      "layer2.0.bn2.bias",
      "layer2.0.bn3.weight",
      "layer2.0.bn3.bias",
      "layer2.0.downsample.1.weight",
      "layer2.0.downsample.1.bias",
      "layer2.1.bn1.weight",
      "layer2.1.bn1.bias",
      "layer2.1.bn2.weight",
      "layer2.1.bn2.bias",
      "layer2.1.bn3.weight",
      "layer2.1.bn3.bias",
      "layer2.2.bn1.weight",
      "layer2.2.bn1.bias",
      "layer2.2.bn2.weight",
      "layer2.2.bn2.bias",
      "layer2.2.bn3.weight",
      "layer2.2.bn3.bias",
      "layer2.3.bn1.weight",
      "layer2.3.bn1.bias",
      "layer2.3.bn2.weight",
      "layer2.3.bn2.bias",
      "layer2.3.bn3.weight",
      "layer2.3.bn3.bias",
      "layer3.0.bn1.weight",
      "layer3.0.bn1.bias",
      "layer3.0.bn2.weight",
      "layer3.0.bn2.bias",
      "layer3.0.bn3.weight",
      "layer3.0.bn3.bias",
      "layer3.0.downsample.1.weight",
      "layer3.0.downsample.1.bias",
      "layer3.1.bn1.weight",
      "layer3.1.bn1.bias",
      "layer3.1.bn2.weight",
      "layer3.1.bn2.bias",
      "layer3.1.bn3.weight",
      "layer3.1.bn3.bias",
      "layer3.2.bn1.weight",
      "layer3.2.bn1.bias",
      "layer3.2.bn2.weight",
      "layer3.2.bn2.bias",
      "layer3.2.bn3.weight",
      "layer3.2.bn3.bias",
      "layer3.3.bn1.weight",
      "layer3.3.bn1.bias",
      "layer3.3.bn2.weight",
      "layer3.3.bn2.bias",
      "layer3.3.bn3.weight",
      "layer3.3.bn3.bias",
      "layer3.4.bn1.weight",
      "layer3.4.bn1.bias",
      "layer3.4.bn2.weight",
      "layer3.4.bn2.bias",
      "layer3.4.bn3.weight",
      "layer3.4.bn3.bias",
      "layer3.5.bn1.weight",
      "layer3.5.bn1.bias",
      "layer3.5.bn2.weight",
      "layer3.5.bn2.bias",
      "layer3.5.bn3.weight",
      "layer3.5.bn3.bias",
      "layer4.0.bn1.weight",
      "layer4.0.bn1.bias",
      "layer4.0.bn2.weight",
      "layer4.0.bn2.bias",
      "layer4.0.bn3.weight",
      "layer4.0.bn3.bias",
      "layer4.0.downsample.1.weight",
      "layer4.0.downsample.1.bias",
      "layer4.1.bn1.weight",
      "layer4.1.bn1.bias",
      "layer4.1.bn2.weight",
      "layer4.1.bn2.bias",
      "layer4.1.bn3.weight",
      "layer4.1.bn3.bias",
      "layer4.2.bn1.weight",
      "layer4.2.bn1.bias",
      "layer4.2.bn2.weight",
      "layer4.2.bn2.bias",
      "layer4.2.bn3.weight",
      "layer4.2.bn3.bias",
      "fc.bias"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
criterion = SoftTargetCrossEntropy()
dict_keys(['model', 'optimizer', 'epoch', 'scaler', 'args', 'model_ema'])
Resume checkpoint /gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_SwinTeacher_STRN50_NKD/checkpoint-best.pth
With optim & sched!
conv1.weight density is 1.0
bn1.weight density is 1.0
bn1.bias density is 1.0
layer1.0.conv1.weight density is 1.0
layer1.0.bn1.weight density is 1.0
layer1.0.bn1.bias density is 1.0
layer1.0.conv2.weight density is 1.0
layer1.0.bn2.weight density is 1.0
layer1.0.bn2.bias density is 1.0
layer1.0.conv3.weight density is 1.0
layer1.0.bn3.weight density is 1.0
layer1.0.bn3.bias density is 1.0
layer1.0.downsample.0.weight density is 1.0
layer1.0.downsample.1.weight density is 1.0
layer1.0.downsample.1.bias density is 1.0
layer1.1.conv1.weight density is 1.0
layer1.1.bn1.weight density is 1.0
layer1.1.bn1.bias density is 1.0
layer1.1.conv2.weight density is 1.0
layer1.1.bn2.weight density is 1.0
layer1.1.bn2.bias density is 1.0
layer1.1.conv3.weight density is 1.0
layer1.1.bn3.weight density is 1.0
layer1.1.bn3.bias density is 1.0
layer1.2.conv1.weight density is 1.0
layer1.2.bn1.weight density is 1.0
layer1.2.bn1.bias density is 1.0
layer1.2.conv2.weight density is 1.0
layer1.2.bn2.weight density is 1.0
layer1.2.bn2.bias density is 1.0
layer1.2.conv3.weight density is 1.0
layer1.2.bn3.weight density is 1.0
layer1.2.bn3.bias density is 1.0
layer2.0.conv1.weight density is 1.0
layer2.0.bn1.weight density is 1.0
layer2.0.bn1.bias density is 1.0
layer2.0.conv2.weight density is 1.0
layer2.0.bn2.weight density is 1.0
layer2.0.bn2.bias density is 1.0
layer2.0.conv3.weight density is 1.0
layer2.0.bn3.weight density is 1.0
layer2.0.bn3.bias density is 1.0
layer2.0.downsample.0.weight density is 1.0
layer2.0.downsample.1.weight density is 1.0
layer2.0.downsample.1.bias density is 1.0
layer2.1.conv1.weight density is 1.0
layer2.1.bn1.weight density is 1.0
layer2.1.bn1.bias density is 1.0
layer2.1.conv2.weight density is 1.0
layer2.1.bn2.weight density is 1.0
layer2.1.bn2.bias density is 1.0
layer2.1.conv3.weight density is 1.0
layer2.1.bn3.weight density is 1.0
layer2.1.bn3.bias density is 1.0
layer2.2.conv1.weight density is 1.0
layer2.2.bn1.weight density is 1.0
layer2.2.bn1.bias density is 1.0
layer2.2.conv2.weight density is 1.0
layer2.2.bn2.weight density is 1.0
layer2.2.bn2.bias density is 1.0
layer2.2.conv3.weight density is 1.0
layer2.2.bn3.weight density is 1.0
layer2.2.bn3.bias density is 1.0
layer2.3.conv1.weight density is 1.0
layer2.3.bn1.weight density is 1.0
layer2.3.bn1.bias density is 1.0
layer2.3.conv2.weight density is 1.0
layer2.3.bn2.weight density is 1.0
layer2.3.bn2.bias density is 1.0
layer2.3.conv3.weight density is 1.0
layer2.3.bn3.weight density is 1.0
layer2.3.bn3.bias density is 1.0
layer3.0.conv1.weight density is 1.0
layer3.0.bn1.weight density is 1.0
layer3.0.bn1.bias density is 1.0
layer3.0.conv2.weight density is 1.0
layer3.0.bn2.weight density is 1.0
layer3.0.bn2.bias density is 1.0
layer3.0.conv3.weight density is 1.0
layer3.0.bn3.weight density is 1.0
layer3.0.bn3.bias density is 1.0
layer3.0.downsample.0.weight density is 1.0
layer3.0.downsample.1.weight density is 1.0
layer3.0.downsample.1.bias density is 1.0
layer3.1.conv1.weight density is 1.0
layer3.1.bn1.weight density is 1.0
layer3.1.bn1.bias density is 1.0
layer3.1.conv2.weight density is 1.0
layer3.1.bn2.weight density is 1.0
layer3.1.bn2.bias density is 1.0
layer3.1.conv3.weight density is 1.0
layer3.1.bn3.weight density is 1.0
layer3.1.bn3.bias density is 1.0
layer3.2.conv1.weight density is 1.0
layer3.2.bn1.weight density is 1.0
layer3.2.bn1.bias density is 1.0
layer3.2.conv2.weight density is 1.0
layer3.2.bn2.weight density is 1.0
layer3.2.bn2.bias density is 1.0
layer3.2.conv3.weight density is 1.0
layer3.2.bn3.weight density is 1.0
layer3.2.bn3.bias density is 1.0
layer3.3.conv1.weight density is 1.0
layer3.3.bn1.weight density is 1.0
layer3.3.bn1.bias density is 1.0
layer3.3.conv2.weight density is 1.0
layer3.3.bn2.weight density is 1.0
layer3.3.bn2.bias density is 1.0
layer3.3.conv3.weight density is 1.0
layer3.3.bn3.weight density is 1.0
layer3.3.bn3.bias density is 1.0
layer3.4.conv1.weight density is 1.0
layer3.4.bn1.weight density is 1.0
layer3.4.bn1.bias density is 1.0
layer3.4.conv2.weight density is 1.0
layer3.4.bn2.weight density is 1.0
layer3.4.bn2.bias density is 1.0
layer3.4.conv3.weight density is 1.0
layer3.4.bn3.weight density is 1.0
layer3.4.bn3.bias density is 1.0
layer3.5.conv1.weight density is 1.0
layer3.5.bn1.weight density is 1.0
layer3.5.bn1.bias density is 1.0
layer3.5.conv2.weight density is 1.0
layer3.5.bn2.weight density is 1.0
layer3.5.bn2.bias density is 1.0
layer3.5.conv3.weight density is 1.0
layer3.5.bn3.weight density is 1.0
layer3.5.bn3.bias density is 1.0
layer4.0.conv1.weight density is 1.0
layer4.0.bn1.weight density is 1.0
layer4.0.bn1.bias density is 1.0
layer4.0.conv2.weight density is 1.0
layer4.0.bn2.weight density is 1.0
layer4.0.bn2.bias density is 1.0
layer4.0.conv3.weight density is 1.0
layer4.0.bn3.weight density is 1.0
layer4.0.bn3.bias density is 1.0
layer4.0.downsample.0.weight density is 1.0
layer4.0.downsample.1.weight density is 1.0
layer4.0.downsample.1.bias density is 1.0
layer4.1.conv1.weight density is 1.0
layer4.1.bn1.weight density is 1.0
layer4.1.bn1.bias density is 1.0
layer4.1.conv2.weight density is 1.0
layer4.1.bn2.weight density is 1.0
layer4.1.bn2.bias density is 1.0
layer4.1.conv3.weight density is 1.0
layer4.1.bn3.weight density is 1.0
layer4.1.bn3.bias density is 1.0
layer4.2.conv1.weight density is 1.0
layer4.2.bn1.weight density is 1.0
layer4.2.bn1.bias density is 1.0
layer4.2.conv2.weight density is 1.0
layer4.2.bn2.weight density is 1.0
layer4.2.bn2.bias density is 1.0
layer4.2.conv3.weight density is 1.0
layer4.2.bn3.weight density is 1.0
layer4.2.bn3.bias density is 1.0
fc.weight density is 1.0
fc.bias density is 1.0
Test:  [  0/313]  eta: 0:18:24  loss: 4.3739 (4.3739)  acc1: 21.8750 (21.8750)  acc5: 28.1250 (28.1250)  time: 3.5281  data: 2.0458  max mem: 3726
Test:  [ 10/313]  eta: 0:01:49  loss: 3.9848 (3.8449)  acc1: 23.9583 (26.4205)  acc5: 40.6250 (40.8144)  time: 0.3604  data: 0.1861  max mem: 3726
Test:  [ 20/313]  eta: 0:01:01  loss: 3.8059 (3.5796)  acc1: 25.0000 (31.4484)  acc5: 42.7083 (45.8829)  time: 0.0430  data: 0.0001  max mem: 3726
Test:  [ 30/313]  eta: 0:00:51  loss: 2.8201 (3.3382)  acc1: 40.6250 (35.7527)  acc5: 56.2500 (50.5040)  time: 0.0819  data: 0.0396  max mem: 3726
Test:  [ 40/313]  eta: 0:00:43  loss: 2.6865 (3.2030)  acc1: 46.8750 (38.2876)  acc5: 62.5000 (52.9726)  time: 0.1092  data: 0.0664  max mem: 3726
Test:  [ 50/313]  eta: 0:00:38  loss: 3.8458 (3.3823)  acc1: 27.0833 (35.8252)  acc5: 40.6250 (49.7958)  time: 0.0924  data: 0.0496  max mem: 3726
Test:  [ 60/313]  eta: 0:00:35  loss: 4.2122 (3.4953)  acc1: 19.7917 (33.6407)  acc5: 35.4167 (47.8654)  time: 0.0984  data: 0.0560  max mem: 3726
Test:  [ 70/313]  eta: 0:00:32  loss: 3.9439 (3.4418)  acc1: 23.9583 (34.6538)  acc5: 39.5833 (48.8556)  time: 0.1066  data: 0.0642  max mem: 3726
Test:  [ 80/313]  eta: 0:00:30  loss: 2.6646 (3.3132)  acc1: 44.7917 (36.6127)  acc5: 63.5417 (51.2989)  time: 0.1086  data: 0.0660  max mem: 3726
Test:  [ 90/313]  eta: 0:00:28  loss: 2.2373 (3.1911)  acc1: 50.0000 (38.5188)  acc5: 69.7917 (53.6287)  time: 0.1052  data: 0.0626  max mem: 3726
Test:  [100/313]  eta: 0:00:26  loss: 2.2373 (3.1298)  acc1: 48.9583 (39.4905)  acc5: 69.7917 (54.7339)  time: 0.0941  data: 0.0508  max mem: 3726
Test:  [110/313]  eta: 0:00:24  loss: 2.7384 (3.1294)  acc1: 41.6667 (39.4989)  acc5: 60.4167 (54.7297)  time: 0.0834  data: 0.0401  max mem: 3726
Test:  [120/313]  eta: 0:00:22  loss: 3.2011 (3.1560)  acc1: 37.5000 (39.0840)  acc5: 52.0833 (54.2786)  time: 0.0745  data: 0.0321  max mem: 3726
Test:  [130/313]  eta: 0:00:21  loss: 3.7609 (3.2320)  acc1: 27.0833 (37.7545)  acc5: 42.7083 (52.9024)  time: 0.0895  data: 0.0470  max mem: 3726
Test:  [140/313]  eta: 0:00:19  loss: 3.7609 (3.2604)  acc1: 26.0417 (37.2045)  acc5: 42.7083 (52.4749)  time: 0.0853  data: 0.0427  max mem: 3726
Test:  [150/313]  eta: 0:00:17  loss: 3.2800 (3.2451)  acc1: 32.2917 (37.5897)  acc5: 48.9583 (52.7042)  time: 0.0667  data: 0.0243  max mem: 3726
Test:  [160/313]  eta: 0:00:16  loss: 3.4740 (3.2849)  acc1: 30.2083 (36.9824)  acc5: 43.7500 (51.9345)  time: 0.0723  data: 0.0296  max mem: 3726
Test:  [170/313]  eta: 0:00:15  loss: 4.0127 (3.3159)  acc1: 21.8750 (36.4644)  acc5: 37.5000 (51.4620)  time: 0.0918  data: 0.0486  max mem: 3726
Test:  [180/313]  eta: 0:00:14  loss: 4.0127 (3.3558)  acc1: 23.9583 (35.8598)  acc5: 37.5000 (50.7597)  time: 0.0957  data: 0.0530  max mem: 3726
Test:  [190/313]  eta: 0:00:12  loss: 3.3799 (3.3243)  acc1: 36.4583 (36.3820)  acc5: 52.0833 (51.5271)  time: 0.0910  data: 0.0488  max mem: 3726
Test:  [200/313]  eta: 0:00:12  loss: 3.0391 (3.3153)  acc1: 37.5000 (36.5050)  acc5: 59.3750 (51.8709)  time: 0.1409  data: 0.0987  max mem: 3726
Test:  [210/313]  eta: 0:00:11  loss: 3.3673 (3.3150)  acc1: 34.3750 (36.4781)  acc5: 56.2500 (52.1426)  time: 0.1283  data: 0.0861  max mem: 3726
Test:  [220/313]  eta: 0:00:09  loss: 3.0866 (3.2864)  acc1: 37.5000 (37.0192)  acc5: 58.3333 (52.7715)  time: 0.0678  data: 0.0255  max mem: 3726
Test:  [230/313]  eta: 0:00:08  loss: 2.8088 (3.2677)  acc1: 42.7083 (37.2114)  acc5: 63.5417 (53.3730)  time: 0.0710  data: 0.0287  max mem: 3726
Test:  [240/313]  eta: 0:00:07  loss: 3.0852 (3.2680)  acc1: 37.5000 (37.2796)  acc5: 60.4167 (53.4578)  time: 0.0727  data: 0.0304  max mem: 3726
Test:  [250/313]  eta: 0:00:06  loss: 2.6670 (3.2292)  acc1: 40.6250 (37.9897)  acc5: 60.4167 (54.1833)  time: 0.0803  data: 0.0379  max mem: 3726
Test:  [260/313]  eta: 0:00:05  loss: 2.5009 (3.2086)  acc1: 51.0417 (38.3780)  acc5: 68.7500 (54.5299)  time: 0.0750  data: 0.0322  max mem: 3726
Test:  [270/313]  eta: 0:00:04  loss: 2.4040 (3.1700)  acc1: 53.1250 (39.0952)  acc5: 69.7917 (55.2698)  time: 0.0747  data: 0.0321  max mem: 3726
Test:  [280/313]  eta: 0:00:03  loss: 2.7832 (3.1979)  acc1: 40.6250 (38.5231)  acc5: 57.2917 (54.7079)  time: 0.0783  data: 0.0360  max mem: 3726
Test:  [290/313]  eta: 0:00:02  loss: 3.9424 (3.2230)  acc1: 25.0000 (38.0799)  acc5: 37.5000 (54.1631)  time: 0.0681  data: 0.0255  max mem: 3726
Test:  [300/313]  eta: 0:00:01  loss: 3.5247 (3.2218)  acc1: 28.1250 (37.9949)  acc5: 47.9167 (54.2601)  time: 0.0678  data: 0.0252  max mem: 3726
Test:  [310/313]  eta: 0:00:00  loss: 3.5128 (3.2379)  acc1: 31.2500 (37.6876)  acc5: 50.0000 (53.9054)  time: 0.0749  data: 0.0326  max mem: 3726
Test:  [312/313]  eta: 0:00:00  loss: 3.5128 (3.2450)  acc1: 31.2500 (37.5900)  acc5: 50.0000 (53.8000)  time: 0.0918  data: 0.0431  max mem: 3726
Test: Total time: 0:00:30 (0.0982 s / it)
* Acc@1 37.590 Acc@5 53.800 loss 3.245
Accuracy of the network on 30000 test images: 37.59000%
Not using distributed mode
Namespace(Decom=False, aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=64, bn=False, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/sliu/project_space/datasets/imagenet-r', data_set='IMNET', data_type='imagenet-r', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.1, enable_wandb=False, epochs=300, eval=True, eval_data_path=None, finetune='', fix=False, growth='random', head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, kernel_size=[7, 7, 7, 7, 100], layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=-1, log_dir=None, lr=0.004, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='resnet50', model_ema=False, model_ema_decay=0.9999, model_ema_eval=False, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_workers=10, only_L=False, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', parallel=True, pin_mem=True, project='SLaK', prune='magnitude', prune_rate=0.3, recount=1, redistribution='none', remode='pixel', reprob=0.25, resplit=False, resume='/gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_Teacherscswin_STRN50_NKD/checkpoint-best.pth', save_ckpt=True, save_ckpt_freq=10, save_ckpt_num=3, seed=0, smoothing=0.1, sparse=False, sparse_init='snip', sparsity=0.4, start_epoch=0, train_interpolation='bicubic', update_freq=1, update_frequency=100, use_amp=False, verbose=False, wandb_ckpt=False, warmup_epochs=20, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, width_factor=1.0, world_size=1)
Transform = 
Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /home/sliu/project_space/datasets/imagenet-r
Number of the class = 1000
Mixup is activated!
Model = ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=True)
  (fc): Linear(in_features=2048, out_features=1000, bias=True)
)
number of params: 25557032
LR = 0.00400000
Batch size = 64
Update frequent = 1
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "conv1.weight",
      "layer1.0.conv1.weight",
      "layer1.0.conv2.weight",
      "layer1.0.conv3.weight",
      "layer1.0.downsample.0.weight",
      "layer1.1.conv1.weight",
      "layer1.1.conv2.weight",
      "layer1.1.conv3.weight",
      "layer1.2.conv1.weight",
      "layer1.2.conv2.weight",
      "layer1.2.conv3.weight",
      "layer2.0.conv1.weight",
      "layer2.0.conv2.weight",
      "layer2.0.conv3.weight",
      "layer2.0.downsample.0.weight",
      "layer2.1.conv1.weight",
      "layer2.1.conv2.weight",
      "layer2.1.conv3.weight",
      "layer2.2.conv1.weight",
      "layer2.2.conv2.weight",
      "layer2.2.conv3.weight",
      "layer2.3.conv1.weight",
      "layer2.3.conv2.weight",
      "layer2.3.conv3.weight",
      "layer3.0.conv1.weight",
      "layer3.0.conv2.weight",
      "layer3.0.conv3.weight",
      "layer3.0.downsample.0.weight",
      "layer3.1.conv1.weight",
      "layer3.1.conv2.weight",
      "layer3.1.conv3.weight",
      "layer3.2.conv1.weight",
      "layer3.2.conv2.weight",
      "layer3.2.conv3.weight",
      "layer3.3.conv1.weight",
      "layer3.3.conv2.weight",
      "layer3.3.conv3.weight",
      "layer3.4.conv1.weight",
      "layer3.4.conv2.weight",
      "layer3.4.conv3.weight",
      "layer3.5.conv1.weight",
      "layer3.5.conv2.weight",
      "layer3.5.conv3.weight",
      "layer4.0.conv1.weight",
      "layer4.0.conv2.weight",
      "layer4.0.conv3.weight",
      "layer4.0.downsample.0.weight",
      "layer4.1.conv1.weight",
      "layer4.1.conv2.weight",
      "layer4.1.conv3.weight",
      "layer4.2.conv1.weight",
      "layer4.2.conv2.weight",
      "layer4.2.conv3.weight",
      "fc.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "bn1.weight",
      "bn1.bias",
      "layer1.0.bn1.weight",
      "layer1.0.bn1.bias",
      "layer1.0.bn2.weight",
      "layer1.0.bn2.bias",
      "layer1.0.bn3.weight",
      "layer1.0.bn3.bias",
      "layer1.0.downsample.1.weight",
      "layer1.0.downsample.1.bias",
      "layer1.1.bn1.weight",
      "layer1.1.bn1.bias",
      "layer1.1.bn2.weight",
      "layer1.1.bn2.bias",
      "layer1.1.bn3.weight",
      "layer1.1.bn3.bias",
      "layer1.2.bn1.weight",
      "layer1.2.bn1.bias",
      "layer1.2.bn2.weight",
      "layer1.2.bn2.bias",
      "layer1.2.bn3.weight",
      "layer1.2.bn3.bias",
      "layer2.0.bn1.weight",
      "layer2.0.bn1.bias",
      "layer2.0.bn2.weight",
      "layer2.0.bn2.bias",
      "layer2.0.bn3.weight",
      "layer2.0.bn3.bias",
      "layer2.0.downsample.1.weight",
      "layer2.0.downsample.1.bias",
      "layer2.1.bn1.weight",
      "layer2.1.bn1.bias",
      "layer2.1.bn2.weight",
      "layer2.1.bn2.bias",
      "layer2.1.bn3.weight",
      "layer2.1.bn3.bias",
      "layer2.2.bn1.weight",
      "layer2.2.bn1.bias",
      "layer2.2.bn2.weight",
      "layer2.2.bn2.bias",
      "layer2.2.bn3.weight",
      "layer2.2.bn3.bias",
      "layer2.3.bn1.weight",
      "layer2.3.bn1.bias",
      "layer2.3.bn2.weight",
      "layer2.3.bn2.bias",
      "layer2.3.bn3.weight",
      "layer2.3.bn3.bias",
      "layer3.0.bn1.weight",
      "layer3.0.bn1.bias",
      "layer3.0.bn2.weight",
      "layer3.0.bn2.bias",
      "layer3.0.bn3.weight",
      "layer3.0.bn3.bias",
      "layer3.0.downsample.1.weight",
      "layer3.0.downsample.1.bias",
      "layer3.1.bn1.weight",
      "layer3.1.bn1.bias",
      "layer3.1.bn2.weight",
      "layer3.1.bn2.bias",
      "layer3.1.bn3.weight",
      "layer3.1.bn3.bias",
      "layer3.2.bn1.weight",
      "layer3.2.bn1.bias",
      "layer3.2.bn2.weight",
      "layer3.2.bn2.bias",
      "layer3.2.bn3.weight",
      "layer3.2.bn3.bias",
      "layer3.3.bn1.weight",
      "layer3.3.bn1.bias",
      "layer3.3.bn2.weight",
      "layer3.3.bn2.bias",
      "layer3.3.bn3.weight",
      "layer3.3.bn3.bias",
      "layer3.4.bn1.weight",
      "layer3.4.bn1.bias",
      "layer3.4.bn2.weight",
      "layer3.4.bn2.bias",
      "layer3.4.bn3.weight",
      "layer3.4.bn3.bias",
      "layer3.5.bn1.weight",
      "layer3.5.bn1.bias",
      "layer3.5.bn2.weight",
      "layer3.5.bn2.bias",
      "layer3.5.bn3.weight",
      "layer3.5.bn3.bias",
      "layer4.0.bn1.weight",
      "layer4.0.bn1.bias",
      "layer4.0.bn2.weight",
      "layer4.0.bn2.bias",
      "layer4.0.bn3.weight",
      "layer4.0.bn3.bias",
      "layer4.0.downsample.1.weight",
      "layer4.0.downsample.1.bias",
      "layer4.1.bn1.weight",
      "layer4.1.bn1.bias",
      "layer4.1.bn2.weight",
      "layer4.1.bn2.bias",
      "layer4.1.bn3.weight",
      "layer4.1.bn3.bias",
      "layer4.2.bn1.weight",
      "layer4.2.bn1.bias",
      "layer4.2.bn2.weight",
      "layer4.2.bn2.bias",
      "layer4.2.bn3.weight",
      "layer4.2.bn3.bias",
      "fc.bias"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
criterion = SoftTargetCrossEntropy()
dict_keys(['model', 'optimizer', 'epoch', 'scaler', 'args', 'model_ema'])
Resume checkpoint /gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_Teacherscswin_STRN50_NKD/checkpoint-best.pth
With optim & sched!
conv1.weight density is 1.0
bn1.weight density is 1.0
bn1.bias density is 1.0
layer1.0.conv1.weight density is 1.0
layer1.0.bn1.weight density is 1.0
layer1.0.bn1.bias density is 1.0
layer1.0.conv2.weight density is 1.0
layer1.0.bn2.weight density is 1.0
layer1.0.bn2.bias density is 1.0
layer1.0.conv3.weight density is 1.0
layer1.0.bn3.weight density is 1.0
layer1.0.bn3.bias density is 1.0
layer1.0.downsample.0.weight density is 1.0
layer1.0.downsample.1.weight density is 1.0
layer1.0.downsample.1.bias density is 1.0
layer1.1.conv1.weight density is 1.0
layer1.1.bn1.weight density is 1.0
layer1.1.bn1.bias density is 1.0
layer1.1.conv2.weight density is 1.0
layer1.1.bn2.weight density is 1.0
layer1.1.bn2.bias density is 1.0
layer1.1.conv3.weight density is 1.0
layer1.1.bn3.weight density is 1.0
layer1.1.bn3.bias density is 1.0
layer1.2.conv1.weight density is 1.0
layer1.2.bn1.weight density is 1.0
layer1.2.bn1.bias density is 1.0
layer1.2.conv2.weight density is 1.0
layer1.2.bn2.weight density is 1.0
layer1.2.bn2.bias density is 1.0
layer1.2.conv3.weight density is 1.0
layer1.2.bn3.weight density is 1.0
layer1.2.bn3.bias density is 1.0
layer2.0.conv1.weight density is 1.0
layer2.0.bn1.weight density is 1.0
layer2.0.bn1.bias density is 1.0
layer2.0.conv2.weight density is 1.0
layer2.0.bn2.weight density is 1.0
layer2.0.bn2.bias density is 1.0
layer2.0.conv3.weight density is 1.0
layer2.0.bn3.weight density is 1.0
layer2.0.bn3.bias density is 1.0
layer2.0.downsample.0.weight density is 1.0
layer2.0.downsample.1.weight density is 1.0
layer2.0.downsample.1.bias density is 1.0
layer2.1.conv1.weight density is 1.0
layer2.1.bn1.weight density is 1.0
layer2.1.bn1.bias density is 1.0
layer2.1.conv2.weight density is 1.0
layer2.1.bn2.weight density is 1.0
layer2.1.bn2.bias density is 1.0
layer2.1.conv3.weight density is 1.0
layer2.1.bn3.weight density is 1.0
layer2.1.bn3.bias density is 1.0
layer2.2.conv1.weight density is 1.0
layer2.2.bn1.weight density is 1.0
layer2.2.bn1.bias density is 1.0
layer2.2.conv2.weight density is 1.0
layer2.2.bn2.weight density is 1.0
layer2.2.bn2.bias density is 1.0
layer2.2.conv3.weight density is 1.0
layer2.2.bn3.weight density is 1.0
layer2.2.bn3.bias density is 1.0
layer2.3.conv1.weight density is 1.0
layer2.3.bn1.weight density is 1.0
layer2.3.bn1.bias density is 1.0
layer2.3.conv2.weight density is 1.0
layer2.3.bn2.weight density is 1.0
layer2.3.bn2.bias density is 1.0
layer2.3.conv3.weight density is 1.0
layer2.3.bn3.weight density is 1.0
layer2.3.bn3.bias density is 1.0
layer3.0.conv1.weight density is 1.0
layer3.0.bn1.weight density is 1.0
layer3.0.bn1.bias density is 1.0
layer3.0.conv2.weight density is 1.0
layer3.0.bn2.weight density is 1.0
layer3.0.bn2.bias density is 1.0
layer3.0.conv3.weight density is 1.0
layer3.0.bn3.weight density is 1.0
layer3.0.bn3.bias density is 1.0
layer3.0.downsample.0.weight density is 1.0
layer3.0.downsample.1.weight density is 1.0
layer3.0.downsample.1.bias density is 1.0
layer3.1.conv1.weight density is 1.0
layer3.1.bn1.weight density is 1.0
layer3.1.bn1.bias density is 1.0
layer3.1.conv2.weight density is 1.0
layer3.1.bn2.weight density is 1.0
layer3.1.bn2.bias density is 1.0
layer3.1.conv3.weight density is 1.0
layer3.1.bn3.weight density is 1.0
layer3.1.bn3.bias density is 1.0
layer3.2.conv1.weight density is 1.0
layer3.2.bn1.weight density is 1.0
layer3.2.bn1.bias density is 1.0
layer3.2.conv2.weight density is 1.0
layer3.2.bn2.weight density is 1.0
layer3.2.bn2.bias density is 1.0
layer3.2.conv3.weight density is 1.0
layer3.2.bn3.weight density is 1.0
layer3.2.bn3.bias density is 1.0
layer3.3.conv1.weight density is 1.0
layer3.3.bn1.weight density is 1.0
layer3.3.bn1.bias density is 1.0
layer3.3.conv2.weight density is 1.0
layer3.3.bn2.weight density is 1.0
layer3.3.bn2.bias density is 1.0
layer3.3.conv3.weight density is 1.0
layer3.3.bn3.weight density is 1.0
layer3.3.bn3.bias density is 1.0
layer3.4.conv1.weight density is 1.0
layer3.4.bn1.weight density is 1.0
layer3.4.bn1.bias density is 1.0
layer3.4.conv2.weight density is 1.0
layer3.4.bn2.weight density is 1.0
layer3.4.bn2.bias density is 1.0
layer3.4.conv3.weight density is 1.0
layer3.4.bn3.weight density is 1.0
layer3.4.bn3.bias density is 1.0
layer3.5.conv1.weight density is 1.0
layer3.5.bn1.weight density is 1.0
layer3.5.bn1.bias density is 1.0
layer3.5.conv2.weight density is 1.0
layer3.5.bn2.weight density is 1.0
layer3.5.bn2.bias density is 1.0
layer3.5.conv3.weight density is 1.0
layer3.5.bn3.weight density is 1.0
layer3.5.bn3.bias density is 1.0
layer4.0.conv1.weight density is 1.0
layer4.0.bn1.weight density is 1.0
layer4.0.bn1.bias density is 1.0
layer4.0.conv2.weight density is 1.0
layer4.0.bn2.weight density is 1.0
layer4.0.bn2.bias density is 1.0
layer4.0.conv3.weight density is 1.0
layer4.0.bn3.weight density is 1.0
layer4.0.bn3.bias density is 1.0
layer4.0.downsample.0.weight density is 1.0
layer4.0.downsample.1.weight density is 1.0
layer4.0.downsample.1.bias density is 1.0
layer4.1.conv1.weight density is 1.0
layer4.1.bn1.weight density is 1.0
layer4.1.bn1.bias density is 1.0
layer4.1.conv2.weight density is 1.0
layer4.1.bn2.weight density is 1.0
layer4.1.bn2.bias density is 1.0
layer4.1.conv3.weight density is 1.0
layer4.1.bn3.weight density is 1.0
layer4.1.bn3.bias density is 1.0
layer4.2.conv1.weight density is 1.0
layer4.2.bn1.weight density is 1.0
layer4.2.bn1.bias density is 1.0
layer4.2.conv2.weight density is 1.0
layer4.2.bn2.weight density is 1.0
layer4.2.bn2.bias density is 1.0
layer4.2.conv3.weight density is 1.0
layer4.2.bn3.weight density is 1.0
layer4.2.bn3.bias density is 1.0
fc.weight density is 1.0
fc.bias density is 1.0
Test:  [  0/313]  eta: 0:21:28  loss: 4.3972 (4.3972)  acc1: 18.7500 (18.7500)  acc5: 28.1250 (28.1250)  time: 4.1166  data: 2.5927  max mem: 3726
Test:  [ 10/313]  eta: 0:02:05  loss: 3.8233 (3.7746)  acc1: 25.0000 (27.0833)  acc5: 41.6667 (42.5189)  time: 0.4137  data: 0.2358  max mem: 3726
Test:  [ 20/313]  eta: 0:01:09  loss: 3.6976 (3.5182)  acc1: 27.0833 (32.7877)  acc5: 43.7500 (47.0734)  time: 0.0429  data: 0.0001  max mem: 3726
Test:  [ 30/313]  eta: 0:00:52  loss: 2.8479 (3.3003)  acc1: 42.7083 (37.0632)  acc5: 58.3333 (51.2433)  time: 0.0599  data: 0.0176  max mem: 3726
Test:  [ 40/313]  eta: 0:00:42  loss: 2.7773 (3.1804)  acc1: 44.7917 (39.3801)  acc5: 60.4167 (52.9980)  time: 0.0686  data: 0.0263  max mem: 3726
Test:  [ 50/313]  eta: 0:00:36  loss: 3.6732 (3.3718)  acc1: 27.0833 (36.7647)  acc5: 43.7500 (49.9387)  time: 0.0641  data: 0.0218  max mem: 3726
Test:  [ 60/313]  eta: 0:00:32  loss: 4.1325 (3.4667)  acc1: 20.8333 (34.5970)  acc5: 34.3750 (48.2753)  time: 0.0783  data: 0.0360  max mem: 3726
Test:  [ 70/313]  eta: 0:00:29  loss: 3.8773 (3.4198)  acc1: 21.8750 (35.4167)  acc5: 40.6250 (49.0023)  time: 0.0839  data: 0.0411  max mem: 3726
Test:  [ 80/313]  eta: 0:00:28  loss: 2.5270 (3.2815)  acc1: 46.8750 (37.3585)  acc5: 65.6250 (51.7747)  time: 0.0948  data: 0.0517  max mem: 3726
Test:  [ 90/313]  eta: 0:00:25  loss: 2.1994 (3.1589)  acc1: 52.0833 (39.3544)  acc5: 69.7917 (54.0064)  time: 0.0842  data: 0.0415  max mem: 3726
Test:  [100/313]  eta: 0:00:23  loss: 2.1994 (3.0836)  acc1: 53.1250 (40.5631)  acc5: 69.7917 (55.3115)  time: 0.0734  data: 0.0308  max mem: 3726
Test:  [110/313]  eta: 0:00:21  loss: 2.6626 (3.0836)  acc1: 44.7917 (40.5875)  acc5: 64.5833 (55.2177)  time: 0.0764  data: 0.0337  max mem: 3726
Test:  [120/313]  eta: 0:00:20  loss: 3.1900 (3.1174)  acc1: 35.4167 (40.1601)  acc5: 53.1250 (54.6918)  time: 0.0664  data: 0.0241  max mem: 3726
Test:  [130/313]  eta: 0:00:19  loss: 3.7548 (3.2009)  acc1: 26.0417 (38.7405)  acc5: 41.6667 (53.0693)  time: 0.0993  data: 0.0560  max mem: 3726
Test:  [140/313]  eta: 0:00:17  loss: 3.7216 (3.2326)  acc1: 27.0833 (38.0541)  acc5: 42.7083 (52.6744)  time: 0.0924  data: 0.0492  max mem: 3726
Test:  [150/313]  eta: 0:00:16  loss: 3.4642 (3.2226)  acc1: 30.2083 (38.3278)  acc5: 50.0000 (52.8491)  time: 0.0640  data: 0.0215  max mem: 3726
Test:  [160/313]  eta: 0:00:15  loss: 3.3776 (3.2606)  acc1: 30.2083 (37.6423)  acc5: 41.6667 (52.0963)  time: 0.0715  data: 0.0289  max mem: 3726
Test:  [170/313]  eta: 0:00:13  loss: 3.9681 (3.2916)  acc1: 23.9583 (37.1589)  acc5: 39.5833 (51.5107)  time: 0.0717  data: 0.0294  max mem: 3726
Test:  [180/313]  eta: 0:00:12  loss: 3.9694 (3.3321)  acc1: 25.0000 (36.4756)  acc5: 38.5417 (50.7597)  time: 0.0740  data: 0.0318  max mem: 3726
Test:  [190/313]  eta: 0:00:11  loss: 3.2682 (3.2977)  acc1: 37.5000 (37.1728)  acc5: 55.2083 (51.6416)  time: 0.0775  data: 0.0351  max mem: 3726
Test:  [200/313]  eta: 0:00:11  loss: 3.0480 (3.2894)  acc1: 40.6250 (37.2979)  acc5: 61.4583 (52.0782)  time: 0.1137  data: 0.0712  max mem: 3726
Test:  [210/313]  eta: 0:00:09  loss: 3.2407 (3.2895)  acc1: 35.4167 (37.3519)  acc5: 58.3333 (52.3598)  time: 0.1031  data: 0.0608  max mem: 3726
Test:  [220/313]  eta: 0:00:08  loss: 3.2316 (3.2589)  acc1: 40.6250 (37.8959)  acc5: 58.3333 (53.0402)  time: 0.0686  data: 0.0263  max mem: 3726
Test:  [230/313]  eta: 0:00:07  loss: 2.8708 (3.2392)  acc1: 44.7917 (38.1944)  acc5: 65.6250 (53.6932)  time: 0.0734  data: 0.0309  max mem: 3726
Test:  [240/313]  eta: 0:00:06  loss: 2.9749 (3.2353)  acc1: 41.6667 (38.2737)  acc5: 60.4167 (53.8684)  time: 0.0602  data: 0.0172  max mem: 3726
Test:  [250/313]  eta: 0:00:05  loss: 2.6983 (3.1962)  acc1: 40.6250 (39.0023)  acc5: 65.6250 (54.6605)  time: 0.0600  data: 0.0167  max mem: 3726
Test:  [260/313]  eta: 0:00:04  loss: 2.4594 (3.1774)  acc1: 50.0000 (39.3718)  acc5: 68.7500 (55.0008)  time: 0.0695  data: 0.0263  max mem: 3726
Test:  [270/313]  eta: 0:00:03  loss: 2.4418 (3.1386)  acc1: 55.2083 (40.1330)  acc5: 70.8333 (55.8080)  time: 0.0661  data: 0.0232  max mem: 3726
Test:  [280/313]  eta: 0:00:02  loss: 2.5905 (3.1666)  acc1: 45.8333 (39.5648)  acc5: 61.4583 (55.2565)  time: 0.0667  data: 0.0241  max mem: 3726
Test:  [290/313]  eta: 0:00:02  loss: 3.9785 (3.1961)  acc1: 22.9167 (39.0643)  acc5: 36.4583 (54.6141)  time: 0.0693  data: 0.0269  max mem: 3726
Test:  [300/313]  eta: 0:00:01  loss: 3.5790 (3.2029)  acc1: 25.0000 (38.8497)  acc5: 44.7917 (54.5300)  time: 0.0600  data: 0.0176  max mem: 3726
Test:  [310/313]  eta: 0:00:00  loss: 3.4967 (3.2176)  acc1: 32.2917 (38.5417)  acc5: 47.9167 (54.2404)  time: 0.0731  data: 0.0309  max mem: 3726
Test:  [312/313]  eta: 0:00:00  loss: 3.4967 (3.2245)  acc1: 32.2917 (38.4333)  acc5: 46.8750 (54.1300)  time: 0.0907  data: 0.0420  max mem: 3726
Test: Total time: 0:00:27 (0.0886 s / it)
* Acc@1 38.433 Acc@5 54.130 loss 3.224
Accuracy of the network on 30000 test images: 38.43333%
Not using distributed mode
Namespace(Decom=False, aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=64, bn=False, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/sliu/project_space/datasets/imagenet-r', data_set='IMNET', data_type='imagenet-r', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.1, enable_wandb=False, epochs=300, eval=True, eval_data_path=None, finetune='', fix=False, growth='random', head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, kernel_size=[7, 7, 7, 7, 100], layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=-1, log_dir=None, lr=0.004, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='resnet50', model_ema=False, model_ema_decay=0.9999, model_ema_eval=False, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_workers=10, only_L=False, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', parallel=True, pin_mem=True, project='SLaK', prune='magnitude', prune_rate=0.3, recount=1, redistribution='none', remode='pixel', reprob=0.25, resplit=False, resume='/gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_ConvNeXtTeacher_STRN50_NKD/checkpoint-best.pth', save_ckpt=True, save_ckpt_freq=10, save_ckpt_num=3, seed=0, smoothing=0.1, sparse=False, sparse_init='snip', sparsity=0.4, start_epoch=0, train_interpolation='bicubic', update_freq=1, update_frequency=100, use_amp=False, verbose=False, wandb_ckpt=False, warmup_epochs=20, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, width_factor=1.0, world_size=1)
Transform = 
Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /home/sliu/project_space/datasets/imagenet-r
Number of the class = 1000
Mixup is activated!
Model = ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=True)
  (fc): Linear(in_features=2048, out_features=1000, bias=True)
)
number of params: 25557032
LR = 0.00400000
Batch size = 64
Update frequent = 1
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "conv1.weight",
      "layer1.0.conv1.weight",
      "layer1.0.conv2.weight",
      "layer1.0.conv3.weight",
      "layer1.0.downsample.0.weight",
      "layer1.1.conv1.weight",
      "layer1.1.conv2.weight",
      "layer1.1.conv3.weight",
      "layer1.2.conv1.weight",
      "layer1.2.conv2.weight",
      "layer1.2.conv3.weight",
      "layer2.0.conv1.weight",
      "layer2.0.conv2.weight",
      "layer2.0.conv3.weight",
      "layer2.0.downsample.0.weight",
      "layer2.1.conv1.weight",
      "layer2.1.conv2.weight",
      "layer2.1.conv3.weight",
      "layer2.2.conv1.weight",
      "layer2.2.conv2.weight",
      "layer2.2.conv3.weight",
      "layer2.3.conv1.weight",
      "layer2.3.conv2.weight",
      "layer2.3.conv3.weight",
      "layer3.0.conv1.weight",
      "layer3.0.conv2.weight",
      "layer3.0.conv3.weight",
      "layer3.0.downsample.0.weight",
      "layer3.1.conv1.weight",
      "layer3.1.conv2.weight",
      "layer3.1.conv3.weight",
      "layer3.2.conv1.weight",
      "layer3.2.conv2.weight",
      "layer3.2.conv3.weight",
      "layer3.3.conv1.weight",
      "layer3.3.conv2.weight",
      "layer3.3.conv3.weight",
      "layer3.4.conv1.weight",
      "layer3.4.conv2.weight",
      "layer3.4.conv3.weight",
      "layer3.5.conv1.weight",
      "layer3.5.conv2.weight",
      "layer3.5.conv3.weight",
      "layer4.0.conv1.weight",
      "layer4.0.conv2.weight",
      "layer4.0.conv3.weight",
      "layer4.0.downsample.0.weight",
      "layer4.1.conv1.weight",
      "layer4.1.conv2.weight",
      "layer4.1.conv3.weight",
      "layer4.2.conv1.weight",
      "layer4.2.conv2.weight",
      "layer4.2.conv3.weight",
      "fc.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "bn1.weight",
      "bn1.bias",
      "layer1.0.bn1.weight",
      "layer1.0.bn1.bias",
      "layer1.0.bn2.weight",
      "layer1.0.bn2.bias",
      "layer1.0.bn3.weight",
      "layer1.0.bn3.bias",
      "layer1.0.downsample.1.weight",
      "layer1.0.downsample.1.bias",
      "layer1.1.bn1.weight",
      "layer1.1.bn1.bias",
      "layer1.1.bn2.weight",
      "layer1.1.bn2.bias",
      "layer1.1.bn3.weight",
      "layer1.1.bn3.bias",
      "layer1.2.bn1.weight",
      "layer1.2.bn1.bias",
      "layer1.2.bn2.weight",
      "layer1.2.bn2.bias",
      "layer1.2.bn3.weight",
      "layer1.2.bn3.bias",
      "layer2.0.bn1.weight",
      "layer2.0.bn1.bias",
      "layer2.0.bn2.weight",
      "layer2.0.bn2.bias",
      "layer2.0.bn3.weight",
      "layer2.0.bn3.bias",
      "layer2.0.downsample.1.weight",
      "layer2.0.downsample.1.bias",
      "layer2.1.bn1.weight",
      "layer2.1.bn1.bias",
      "layer2.1.bn2.weight",
      "layer2.1.bn2.bias",
      "layer2.1.bn3.weight",
      "layer2.1.bn3.bias",
      "layer2.2.bn1.weight",
      "layer2.2.bn1.bias",
      "layer2.2.bn2.weight",
      "layer2.2.bn2.bias",
      "layer2.2.bn3.weight",
      "layer2.2.bn3.bias",
      "layer2.3.bn1.weight",
      "layer2.3.bn1.bias",
      "layer2.3.bn2.weight",
      "layer2.3.bn2.bias",
      "layer2.3.bn3.weight",
      "layer2.3.bn3.bias",
      "layer3.0.bn1.weight",
      "layer3.0.bn1.bias",
      "layer3.0.bn2.weight",
      "layer3.0.bn2.bias",
      "layer3.0.bn3.weight",
      "layer3.0.bn3.bias",
      "layer3.0.downsample.1.weight",
      "layer3.0.downsample.1.bias",
      "layer3.1.bn1.weight",
      "layer3.1.bn1.bias",
      "layer3.1.bn2.weight",
      "layer3.1.bn2.bias",
      "layer3.1.bn3.weight",
      "layer3.1.bn3.bias",
      "layer3.2.bn1.weight",
      "layer3.2.bn1.bias",
      "layer3.2.bn2.weight",
      "layer3.2.bn2.bias",
      "layer3.2.bn3.weight",
      "layer3.2.bn3.bias",
      "layer3.3.bn1.weight",
      "layer3.3.bn1.bias",
      "layer3.3.bn2.weight",
      "layer3.3.bn2.bias",
      "layer3.3.bn3.weight",
      "layer3.3.bn3.bias",
      "layer3.4.bn1.weight",
      "layer3.4.bn1.bias",
      "layer3.4.bn2.weight",
      "layer3.4.bn2.bias",
      "layer3.4.bn3.weight",
      "layer3.4.bn3.bias",
      "layer3.5.bn1.weight",
      "layer3.5.bn1.bias",
      "layer3.5.bn2.weight",
      "layer3.5.bn2.bias",
      "layer3.5.bn3.weight",
      "layer3.5.bn3.bias",
      "layer4.0.bn1.weight",
      "layer4.0.bn1.bias",
      "layer4.0.bn2.weight",
      "layer4.0.bn2.bias",
      "layer4.0.bn3.weight",
      "layer4.0.bn3.bias",
      "layer4.0.downsample.1.weight",
      "layer4.0.downsample.1.bias",
      "layer4.1.bn1.weight",
      "layer4.1.bn1.bias",
      "layer4.1.bn2.weight",
      "layer4.1.bn2.bias",
      "layer4.1.bn3.weight",
      "layer4.1.bn3.bias",
      "layer4.2.bn1.weight",
      "layer4.2.bn1.bias",
      "layer4.2.bn2.weight",
      "layer4.2.bn2.bias",
      "layer4.2.bn3.weight",
      "layer4.2.bn3.bias",
      "fc.bias"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
criterion = SoftTargetCrossEntropy()
dict_keys(['model', 'optimizer', 'epoch', 'scaler', 'args', 'model_ema'])
Resume checkpoint /gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_ConvNeXtTeacher_STRN50_NKD/checkpoint-best.pth
With optim & sched!
conv1.weight density is 1.0
bn1.weight density is 1.0
bn1.bias density is 1.0
layer1.0.conv1.weight density is 1.0
layer1.0.bn1.weight density is 1.0
layer1.0.bn1.bias density is 1.0
layer1.0.conv2.weight density is 1.0
layer1.0.bn2.weight density is 1.0
layer1.0.bn2.bias density is 1.0
layer1.0.conv3.weight density is 1.0
layer1.0.bn3.weight density is 1.0
layer1.0.bn3.bias density is 1.0
layer1.0.downsample.0.weight density is 1.0
layer1.0.downsample.1.weight density is 1.0
layer1.0.downsample.1.bias density is 1.0
layer1.1.conv1.weight density is 1.0
layer1.1.bn1.weight density is 1.0
layer1.1.bn1.bias density is 1.0
layer1.1.conv2.weight density is 1.0
layer1.1.bn2.weight density is 1.0
layer1.1.bn2.bias density is 1.0
layer1.1.conv3.weight density is 1.0
layer1.1.bn3.weight density is 1.0
layer1.1.bn3.bias density is 1.0
layer1.2.conv1.weight density is 1.0
layer1.2.bn1.weight density is 1.0
layer1.2.bn1.bias density is 1.0
layer1.2.conv2.weight density is 1.0
layer1.2.bn2.weight density is 1.0
layer1.2.bn2.bias density is 1.0
layer1.2.conv3.weight density is 1.0
layer1.2.bn3.weight density is 1.0
layer1.2.bn3.bias density is 1.0
layer2.0.conv1.weight density is 1.0
layer2.0.bn1.weight density is 1.0
layer2.0.bn1.bias density is 1.0
layer2.0.conv2.weight density is 1.0
layer2.0.bn2.weight density is 1.0
layer2.0.bn2.bias density is 1.0
layer2.0.conv3.weight density is 1.0
layer2.0.bn3.weight density is 1.0
layer2.0.bn3.bias density is 1.0
layer2.0.downsample.0.weight density is 1.0
layer2.0.downsample.1.weight density is 1.0
layer2.0.downsample.1.bias density is 1.0
layer2.1.conv1.weight density is 1.0
layer2.1.bn1.weight density is 1.0
layer2.1.bn1.bias density is 1.0
layer2.1.conv2.weight density is 1.0
layer2.1.bn2.weight density is 1.0
layer2.1.bn2.bias density is 1.0
layer2.1.conv3.weight density is 1.0
layer2.1.bn3.weight density is 1.0
layer2.1.bn3.bias density is 1.0
layer2.2.conv1.weight density is 1.0
layer2.2.bn1.weight density is 1.0
layer2.2.bn1.bias density is 1.0
layer2.2.conv2.weight density is 1.0
layer2.2.bn2.weight density is 1.0
layer2.2.bn2.bias density is 1.0
layer2.2.conv3.weight density is 1.0
layer2.2.bn3.weight density is 1.0
layer2.2.bn3.bias density is 1.0
layer2.3.conv1.weight density is 1.0
layer2.3.bn1.weight density is 1.0
layer2.3.bn1.bias density is 1.0
layer2.3.conv2.weight density is 1.0
layer2.3.bn2.weight density is 1.0
layer2.3.bn2.bias density is 1.0
layer2.3.conv3.weight density is 1.0
layer2.3.bn3.weight density is 1.0
layer2.3.bn3.bias density is 1.0
layer3.0.conv1.weight density is 1.0
layer3.0.bn1.weight density is 1.0
layer3.0.bn1.bias density is 1.0
layer3.0.conv2.weight density is 1.0
layer3.0.bn2.weight density is 1.0
layer3.0.bn2.bias density is 1.0
layer3.0.conv3.weight density is 1.0
layer3.0.bn3.weight density is 1.0
layer3.0.bn3.bias density is 1.0
layer3.0.downsample.0.weight density is 1.0
layer3.0.downsample.1.weight density is 1.0
layer3.0.downsample.1.bias density is 1.0
layer3.1.conv1.weight density is 1.0
layer3.1.bn1.weight density is 1.0
layer3.1.bn1.bias density is 1.0
layer3.1.conv2.weight density is 1.0
layer3.1.bn2.weight density is 1.0
layer3.1.bn2.bias density is 1.0
layer3.1.conv3.weight density is 1.0
layer3.1.bn3.weight density is 1.0
layer3.1.bn3.bias density is 1.0
layer3.2.conv1.weight density is 1.0
layer3.2.bn1.weight density is 1.0
layer3.2.bn1.bias density is 1.0
layer3.2.conv2.weight density is 1.0
layer3.2.bn2.weight density is 1.0
layer3.2.bn2.bias density is 1.0
layer3.2.conv3.weight density is 1.0
layer3.2.bn3.weight density is 1.0
layer3.2.bn3.bias density is 1.0
layer3.3.conv1.weight density is 1.0
layer3.3.bn1.weight density is 1.0
layer3.3.bn1.bias density is 1.0
layer3.3.conv2.weight density is 1.0
layer3.3.bn2.weight density is 1.0
layer3.3.bn2.bias density is 1.0
layer3.3.conv3.weight density is 1.0
layer3.3.bn3.weight density is 1.0
layer3.3.bn3.bias density is 1.0
layer3.4.conv1.weight density is 1.0
layer3.4.bn1.weight density is 1.0
layer3.4.bn1.bias density is 1.0
layer3.4.conv2.weight density is 1.0
layer3.4.bn2.weight density is 1.0
layer3.4.bn2.bias density is 1.0
layer3.4.conv3.weight density is 1.0
layer3.4.bn3.weight density is 1.0
layer3.4.bn3.bias density is 1.0
layer3.5.conv1.weight density is 1.0
layer3.5.bn1.weight density is 1.0
layer3.5.bn1.bias density is 1.0
layer3.5.conv2.weight density is 1.0
layer3.5.bn2.weight density is 1.0
layer3.5.bn2.bias density is 1.0
layer3.5.conv3.weight density is 1.0
layer3.5.bn3.weight density is 1.0
layer3.5.bn3.bias density is 1.0
layer4.0.conv1.weight density is 1.0
layer4.0.bn1.weight density is 1.0
layer4.0.bn1.bias density is 1.0
layer4.0.conv2.weight density is 1.0
layer4.0.bn2.weight density is 1.0
layer4.0.bn2.bias density is 1.0
layer4.0.conv3.weight density is 1.0
layer4.0.bn3.weight density is 1.0
layer4.0.bn3.bias density is 1.0
layer4.0.downsample.0.weight density is 1.0
layer4.0.downsample.1.weight density is 1.0
layer4.0.downsample.1.bias density is 1.0
layer4.1.conv1.weight density is 1.0
layer4.1.bn1.weight density is 1.0
layer4.1.bn1.bias density is 1.0
layer4.1.conv2.weight density is 1.0
layer4.1.bn2.weight density is 1.0
layer4.1.bn2.bias density is 1.0
layer4.1.conv3.weight density is 1.0
layer4.1.bn3.weight density is 1.0
layer4.1.bn3.bias density is 1.0
layer4.2.conv1.weight density is 1.0
layer4.2.bn1.weight density is 1.0
layer4.2.bn1.bias density is 1.0
layer4.2.conv2.weight density is 1.0
layer4.2.bn2.weight density is 1.0
layer4.2.bn2.bias density is 1.0
layer4.2.conv3.weight density is 1.0
layer4.2.bn3.weight density is 1.0
layer4.2.bn3.bias density is 1.0
fc.weight density is 1.0
fc.bias density is 1.0
Test:  [  0/313]  eta: 0:16:04  loss: 4.0038 (4.0038)  acc1: 20.8333 (20.8333)  acc5: 38.5417 (38.5417)  time: 3.0813  data: 1.5473  max mem: 3726
Test:  [ 10/313]  eta: 0:01:36  loss: 3.8074 (3.6921)  acc1: 25.0000 (28.5038)  acc5: 41.6667 (45.0758)  time: 0.3196  data: 0.1408  max mem: 3726
Test:  [ 20/313]  eta: 0:00:54  loss: 3.5737 (3.4530)  acc1: 29.1667 (33.5813)  acc5: 43.7500 (49.1071)  time: 0.0430  data: 0.0001  max mem: 3726
Test:  [ 30/313]  eta: 0:00:42  loss: 2.8089 (3.2520)  acc1: 42.7083 (37.3320)  acc5: 61.4583 (53.4610)  time: 0.0587  data: 0.0161  max mem: 3726
Test:  [ 40/313]  eta: 0:00:34  loss: 2.6835 (3.1134)  acc1: 45.8333 (39.6596)  acc5: 62.5000 (55.2591)  time: 0.0647  data: 0.0218  max mem: 3726
Test:  [ 50/313]  eta: 0:00:30  loss: 3.5452 (3.2937)  acc1: 29.1667 (37.0507)  acc5: 46.8750 (51.7974)  time: 0.0607  data: 0.0180  max mem: 3726
Test:  [ 60/313]  eta: 0:00:27  loss: 4.1053 (3.4056)  acc1: 22.9167 (34.9044)  acc5: 33.3333 (49.5389)  time: 0.0650  data: 0.0225  max mem: 3726
Test:  [ 70/313]  eta: 0:00:25  loss: 3.8567 (3.3562)  acc1: 26.0417 (35.9008)  acc5: 38.5417 (50.2934)  time: 0.0761  data: 0.0335  max mem: 3726
Test:  [ 80/313]  eta: 0:00:24  loss: 2.5403 (3.2220)  acc1: 47.9167 (37.9244)  acc5: 66.6667 (52.8807)  time: 0.0906  data: 0.0479  max mem: 3726
Test:  [ 90/313]  eta: 0:00:22  loss: 2.0667 (3.0984)  acc1: 54.1667 (40.0412)  acc5: 71.8750 (55.1625)  time: 0.0835  data: 0.0409  max mem: 3726
Test:  [100/313]  eta: 0:00:21  loss: 2.0667 (3.0278)  acc1: 54.1667 (41.2748)  acc5: 70.8333 (56.3531)  time: 0.0829  data: 0.0405  max mem: 3726
Test:  [110/313]  eta: 0:00:20  loss: 2.5964 (3.0285)  acc1: 46.8750 (41.4227)  acc5: 63.5417 (56.3626)  time: 0.0991  data: 0.0565  max mem: 3726
Test:  [120/313]  eta: 0:00:18  loss: 3.1285 (3.0604)  acc1: 39.5833 (40.9177)  acc5: 53.1250 (55.8282)  time: 0.0869  data: 0.0439  max mem: 3726
Test:  [130/313]  eta: 0:00:18  loss: 3.7960 (3.1465)  acc1: 26.0417 (39.4163)  acc5: 43.7500 (54.3257)  time: 0.1001  data: 0.0574  max mem: 3726
Test:  [140/313]  eta: 0:00:17  loss: 3.7461 (3.1784)  acc1: 26.0417 (38.7633)  acc5: 44.7917 (53.9303)  time: 0.1124  data: 0.0700  max mem: 3726
Test:  [150/313]  eta: 0:00:16  loss: 3.3215 (3.1697)  acc1: 32.2917 (39.0039)  acc5: 48.9583 (54.0287)  time: 0.1021  data: 0.0599  max mem: 3726
Test:  [160/313]  eta: 0:00:15  loss: 3.3215 (3.2104)  acc1: 35.4167 (38.3670)  acc5: 47.9167 (53.2544)  time: 0.0951  data: 0.0528  max mem: 3726
Test:  [170/313]  eta: 0:00:14  loss: 3.8398 (3.2379)  acc1: 26.0417 (37.8899)  acc5: 39.5833 (52.7230)  time: 0.0919  data: 0.0488  max mem: 3726
Test:  [180/313]  eta: 0:00:13  loss: 3.8558 (3.2736)  acc1: 26.0417 (37.1950)  acc5: 39.5833 (52.0776)  time: 0.1027  data: 0.0596  max mem: 3726
Test:  [190/313]  eta: 0:00:12  loss: 3.2106 (3.2421)  acc1: 36.4583 (37.8218)  acc5: 55.2083 (52.7705)  time: 0.1081  data: 0.0657  max mem: 3726
Test:  [200/313]  eta: 0:00:11  loss: 2.9853 (3.2342)  acc1: 41.6667 (37.9509)  acc5: 62.5000 (53.1613)  time: 0.1461  data: 0.1036  max mem: 3726
Test:  [210/313]  eta: 0:00:10  loss: 3.1880 (3.2359)  acc1: 37.5000 (37.9493)  acc5: 61.4583 (53.4261)  time: 0.1244  data: 0.0819  max mem: 3726
Test:  [220/313]  eta: 0:00:09  loss: 3.0889 (3.2060)  acc1: 38.5417 (38.5181)  acc5: 59.3750 (54.0535)  time: 0.0785  data: 0.0356  max mem: 3726
Test:  [230/313]  eta: 0:00:08  loss: 2.7852 (3.1875)  acc1: 44.7917 (38.7762)  acc5: 61.4583 (54.5860)  time: 0.0809  data: 0.0377  max mem: 3726
Test:  [240/313]  eta: 0:00:07  loss: 2.9147 (3.1838)  acc1: 41.6667 (38.8831)  acc5: 56.2500 (54.7372)  time: 0.0718  data: 0.0292  max mem: 3726
Test:  [250/313]  eta: 0:00:06  loss: 2.7424 (3.1471)  acc1: 43.7500 (39.6165)  acc5: 67.7083 (55.4449)  time: 0.0786  data: 0.0363  max mem: 3726
Test:  [260/313]  eta: 0:00:05  loss: 2.4191 (3.1284)  acc1: 54.1667 (40.0104)  acc5: 67.7083 (55.8190)  time: 0.0861  data: 0.0435  max mem: 3726
Test:  [270/313]  eta: 0:00:04  loss: 2.3571 (3.0891)  acc1: 54.1667 (40.7480)  acc5: 72.9167 (56.6152)  time: 0.0973  data: 0.0546  max mem: 3726
Test:  [280/313]  eta: 0:00:03  loss: 2.5400 (3.1177)  acc1: 45.8333 (40.2172)  acc5: 56.2500 (56.0090)  time: 0.0945  data: 0.0519  max mem: 3726
Test:  [290/313]  eta: 0:00:02  loss: 3.9517 (3.1459)  acc1: 23.9583 (39.7265)  acc5: 39.5833 (55.3981)  time: 0.0816  data: 0.0389  max mem: 3726
Test:  [300/313]  eta: 0:00:01  loss: 3.4931 (3.1518)  acc1: 27.0833 (39.4899)  acc5: 51.0417 (55.3883)  time: 0.0830  data: 0.0402  max mem: 3726
Test:  [310/313]  eta: 0:00:00  loss: 3.4931 (3.1693)  acc1: 29.1667 (39.1278)  acc5: 51.0417 (55.0610)  time: 0.0866  data: 0.0439  max mem: 3726
Test:  [312/313]  eta: 0:00:00  loss: 3.4931 (3.1757)  acc1: 29.1667 (39.0233)  acc5: 51.0417 (54.9633)  time: 0.0961  data: 0.0468  max mem: 3726
Test: Total time: 0:00:30 (0.0984 s / it)
* Acc@1 39.023 Acc@5 54.963 loss 3.176
Accuracy of the network on 30000 test images: 39.02333%
Not using distributed mode
Namespace(Decom=False, aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=64, bn=False, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/sliu/project_space/datasets/imagenet-r', data_set='IMNET', data_type='imagenet-r', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.1, enable_wandb=False, epochs=300, eval=True, eval_data_path=None, finetune='', fix=False, growth='random', head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, kernel_size=[7, 7, 7, 7, 100], layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=-1, log_dir=None, lr=0.004, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='resnet50', model_ema=False, model_ema_decay=0.9999, model_ema_eval=False, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_workers=10, only_L=False, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', parallel=True, pin_mem=True, project='SLaK', prune='magnitude', prune_rate=0.3, recount=1, redistribution='none', remode='pixel', reprob=0.25, resplit=False, resume='/gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_SLakTTeachers_STRN50_NKD/checkpoint-best.pth', save_ckpt=True, save_ckpt_freq=10, save_ckpt_num=3, seed=0, smoothing=0.1, sparse=False, sparse_init='snip', sparsity=0.4, start_epoch=0, train_interpolation='bicubic', update_freq=1, update_frequency=100, use_amp=False, verbose=False, wandb_ckpt=False, warmup_epochs=20, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, width_factor=1.0, world_size=1)
Transform = 
Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /home/sliu/project_space/datasets/imagenet-r
Number of the class = 1000
Mixup is activated!
Model = ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=True)
  (fc): Linear(in_features=2048, out_features=1000, bias=True)
)
number of params: 25557032
LR = 0.00400000
Batch size = 64
Update frequent = 1
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "conv1.weight",
      "layer1.0.conv1.weight",
      "layer1.0.conv2.weight",
      "layer1.0.conv3.weight",
      "layer1.0.downsample.0.weight",
      "layer1.1.conv1.weight",
      "layer1.1.conv2.weight",
      "layer1.1.conv3.weight",
      "layer1.2.conv1.weight",
      "layer1.2.conv2.weight",
      "layer1.2.conv3.weight",
      "layer2.0.conv1.weight",
      "layer2.0.conv2.weight",
      "layer2.0.conv3.weight",
      "layer2.0.downsample.0.weight",
      "layer2.1.conv1.weight",
      "layer2.1.conv2.weight",
      "layer2.1.conv3.weight",
      "layer2.2.conv1.weight",
      "layer2.2.conv2.weight",
      "layer2.2.conv3.weight",
      "layer2.3.conv1.weight",
      "layer2.3.conv2.weight",
      "layer2.3.conv3.weight",
      "layer3.0.conv1.weight",
      "layer3.0.conv2.weight",
      "layer3.0.conv3.weight",
      "layer3.0.downsample.0.weight",
      "layer3.1.conv1.weight",
      "layer3.1.conv2.weight",
      "layer3.1.conv3.weight",
      "layer3.2.conv1.weight",
      "layer3.2.conv2.weight",
      "layer3.2.conv3.weight",
      "layer3.3.conv1.weight",
      "layer3.3.conv2.weight",
      "layer3.3.conv3.weight",
      "layer3.4.conv1.weight",
      "layer3.4.conv2.weight",
      "layer3.4.conv3.weight",
      "layer3.5.conv1.weight",
      "layer3.5.conv2.weight",
      "layer3.5.conv3.weight",
      "layer4.0.conv1.weight",
      "layer4.0.conv2.weight",
      "layer4.0.conv3.weight",
      "layer4.0.downsample.0.weight",
      "layer4.1.conv1.weight",
      "layer4.1.conv2.weight",
      "layer4.1.conv3.weight",
      "layer4.2.conv1.weight",
      "layer4.2.conv2.weight",
      "layer4.2.conv3.weight",
      "fc.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "bn1.weight",
      "bn1.bias",
      "layer1.0.bn1.weight",
      "layer1.0.bn1.bias",
      "layer1.0.bn2.weight",
      "layer1.0.bn2.bias",
      "layer1.0.bn3.weight",
      "layer1.0.bn3.bias",
      "layer1.0.downsample.1.weight",
      "layer1.0.downsample.1.bias",
      "layer1.1.bn1.weight",
      "layer1.1.bn1.bias",
      "layer1.1.bn2.weight",
      "layer1.1.bn2.bias",
      "layer1.1.bn3.weight",
      "layer1.1.bn3.bias",
      "layer1.2.bn1.weight",
      "layer1.2.bn1.bias",
      "layer1.2.bn2.weight",
      "layer1.2.bn2.bias",
      "layer1.2.bn3.weight",
      "layer1.2.bn3.bias",
      "layer2.0.bn1.weight",
      "layer2.0.bn1.bias",
      "layer2.0.bn2.weight",
      "layer2.0.bn2.bias",
      "layer2.0.bn3.weight",
      "layer2.0.bn3.bias",
      "layer2.0.downsample.1.weight",
      "layer2.0.downsample.1.bias",
      "layer2.1.bn1.weight",
      "layer2.1.bn1.bias",
      "layer2.1.bn2.weight",
      "layer2.1.bn2.bias",
      "layer2.1.bn3.weight",
      "layer2.1.bn3.bias",
      "layer2.2.bn1.weight",
      "layer2.2.bn1.bias",
      "layer2.2.bn2.weight",
      "layer2.2.bn2.bias",
      "layer2.2.bn3.weight",
      "layer2.2.bn3.bias",
      "layer2.3.bn1.weight",
      "layer2.3.bn1.bias",
      "layer2.3.bn2.weight",
      "layer2.3.bn2.bias",
      "layer2.3.bn3.weight",
      "layer2.3.bn3.bias",
      "layer3.0.bn1.weight",
      "layer3.0.bn1.bias",
      "layer3.0.bn2.weight",
      "layer3.0.bn2.bias",
      "layer3.0.bn3.weight",
      "layer3.0.bn3.bias",
      "layer3.0.downsample.1.weight",
      "layer3.0.downsample.1.bias",
      "layer3.1.bn1.weight",
      "layer3.1.bn1.bias",
      "layer3.1.bn2.weight",
      "layer3.1.bn2.bias",
      "layer3.1.bn3.weight",
      "layer3.1.bn3.bias",
      "layer3.2.bn1.weight",
      "layer3.2.bn1.bias",
      "layer3.2.bn2.weight",
      "layer3.2.bn2.bias",
      "layer3.2.bn3.weight",
      "layer3.2.bn3.bias",
      "layer3.3.bn1.weight",
      "layer3.3.bn1.bias",
      "layer3.3.bn2.weight",
      "layer3.3.bn2.bias",
      "layer3.3.bn3.weight",
      "layer3.3.bn3.bias",
      "layer3.4.bn1.weight",
      "layer3.4.bn1.bias",
      "layer3.4.bn2.weight",
      "layer3.4.bn2.bias",
      "layer3.4.bn3.weight",
      "layer3.4.bn3.bias",
      "layer3.5.bn1.weight",
      "layer3.5.bn1.bias",
      "layer3.5.bn2.weight",
      "layer3.5.bn2.bias",
      "layer3.5.bn3.weight",
      "layer3.5.bn3.bias",
      "layer4.0.bn1.weight",
      "layer4.0.bn1.bias",
      "layer4.0.bn2.weight",
      "layer4.0.bn2.bias",
      "layer4.0.bn3.weight",
      "layer4.0.bn3.bias",
      "layer4.0.downsample.1.weight",
      "layer4.0.downsample.1.bias",
      "layer4.1.bn1.weight",
      "layer4.1.bn1.bias",
      "layer4.1.bn2.weight",
      "layer4.1.bn2.bias",
      "layer4.1.bn3.weight",
      "layer4.1.bn3.bias",
      "layer4.2.bn1.weight",
      "layer4.2.bn1.bias",
      "layer4.2.bn2.weight",
      "layer4.2.bn2.bias",
      "layer4.2.bn3.weight",
      "layer4.2.bn3.bias",
      "fc.bias"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
criterion = SoftTargetCrossEntropy()
dict_keys(['model', 'optimizer', 'epoch', 'scaler', 'args', 'model_ema'])
Resume checkpoint /gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_SLakTTeachers_STRN50_NKD/checkpoint-best.pth
With optim & sched!
conv1.weight density is 1.0
bn1.weight density is 1.0
bn1.bias density is 1.0
layer1.0.conv1.weight density is 1.0
layer1.0.bn1.weight density is 1.0
layer1.0.bn1.bias density is 1.0
layer1.0.conv2.weight density is 1.0
layer1.0.bn2.weight density is 1.0
layer1.0.bn2.bias density is 1.0
layer1.0.conv3.weight density is 1.0
layer1.0.bn3.weight density is 1.0
layer1.0.bn3.bias density is 1.0
layer1.0.downsample.0.weight density is 1.0
layer1.0.downsample.1.weight density is 1.0
layer1.0.downsample.1.bias density is 1.0
layer1.1.conv1.weight density is 1.0
layer1.1.bn1.weight density is 1.0
layer1.1.bn1.bias density is 1.0
layer1.1.conv2.weight density is 1.0
layer1.1.bn2.weight density is 1.0
layer1.1.bn2.bias density is 1.0
layer1.1.conv3.weight density is 1.0
layer1.1.bn3.weight density is 1.0
layer1.1.bn3.bias density is 1.0
layer1.2.conv1.weight density is 1.0
layer1.2.bn1.weight density is 1.0
layer1.2.bn1.bias density is 1.0
layer1.2.conv2.weight density is 1.0
layer1.2.bn2.weight density is 1.0
layer1.2.bn2.bias density is 1.0
layer1.2.conv3.weight density is 1.0
layer1.2.bn3.weight density is 1.0
layer1.2.bn3.bias density is 1.0
layer2.0.conv1.weight density is 1.0
layer2.0.bn1.weight density is 1.0
layer2.0.bn1.bias density is 1.0
layer2.0.conv2.weight density is 1.0
layer2.0.bn2.weight density is 1.0
layer2.0.bn2.bias density is 1.0
layer2.0.conv3.weight density is 1.0
layer2.0.bn3.weight density is 1.0
layer2.0.bn3.bias density is 1.0
layer2.0.downsample.0.weight density is 1.0
layer2.0.downsample.1.weight density is 1.0
layer2.0.downsample.1.bias density is 1.0
layer2.1.conv1.weight density is 1.0
layer2.1.bn1.weight density is 1.0
layer2.1.bn1.bias density is 1.0
layer2.1.conv2.weight density is 1.0
layer2.1.bn2.weight density is 1.0
layer2.1.bn2.bias density is 1.0
layer2.1.conv3.weight density is 1.0
layer2.1.bn3.weight density is 1.0
layer2.1.bn3.bias density is 1.0
layer2.2.conv1.weight density is 1.0
layer2.2.bn1.weight density is 1.0
layer2.2.bn1.bias density is 1.0
layer2.2.conv2.weight density is 1.0
layer2.2.bn2.weight density is 1.0
layer2.2.bn2.bias density is 1.0
layer2.2.conv3.weight density is 1.0
layer2.2.bn3.weight density is 1.0
layer2.2.bn3.bias density is 1.0
layer2.3.conv1.weight density is 1.0
layer2.3.bn1.weight density is 1.0
layer2.3.bn1.bias density is 1.0
layer2.3.conv2.weight density is 1.0
layer2.3.bn2.weight density is 1.0
layer2.3.bn2.bias density is 1.0
layer2.3.conv3.weight density is 1.0
layer2.3.bn3.weight density is 1.0
layer2.3.bn3.bias density is 1.0
layer3.0.conv1.weight density is 1.0
layer3.0.bn1.weight density is 1.0
layer3.0.bn1.bias density is 1.0
layer3.0.conv2.weight density is 1.0
layer3.0.bn2.weight density is 1.0
layer3.0.bn2.bias density is 1.0
layer3.0.conv3.weight density is 1.0
layer3.0.bn3.weight density is 1.0
layer3.0.bn3.bias density is 1.0
layer3.0.downsample.0.weight density is 1.0
layer3.0.downsample.1.weight density is 1.0
layer3.0.downsample.1.bias density is 1.0
layer3.1.conv1.weight density is 1.0
layer3.1.bn1.weight density is 1.0
layer3.1.bn1.bias density is 1.0
layer3.1.conv2.weight density is 1.0
layer3.1.bn2.weight density is 1.0
layer3.1.bn2.bias density is 1.0
layer3.1.conv3.weight density is 1.0
layer3.1.bn3.weight density is 1.0
layer3.1.bn3.bias density is 1.0
layer3.2.conv1.weight density is 1.0
layer3.2.bn1.weight density is 1.0
layer3.2.bn1.bias density is 1.0
layer3.2.conv2.weight density is 1.0
layer3.2.bn2.weight density is 1.0
layer3.2.bn2.bias density is 1.0
layer3.2.conv3.weight density is 1.0
layer3.2.bn3.weight density is 1.0
layer3.2.bn3.bias density is 1.0
layer3.3.conv1.weight density is 1.0
layer3.3.bn1.weight density is 1.0
layer3.3.bn1.bias density is 1.0
layer3.3.conv2.weight density is 1.0
layer3.3.bn2.weight density is 1.0
layer3.3.bn2.bias density is 1.0
layer3.3.conv3.weight density is 1.0
layer3.3.bn3.weight density is 1.0
layer3.3.bn3.bias density is 1.0
layer3.4.conv1.weight density is 1.0
layer3.4.bn1.weight density is 1.0
layer3.4.bn1.bias density is 1.0
layer3.4.conv2.weight density is 1.0
layer3.4.bn2.weight density is 1.0
layer3.4.bn2.bias density is 1.0
layer3.4.conv3.weight density is 1.0
layer3.4.bn3.weight density is 1.0
layer3.4.bn3.bias density is 1.0
layer3.5.conv1.weight density is 1.0
layer3.5.bn1.weight density is 1.0
layer3.5.bn1.bias density is 1.0
layer3.5.conv2.weight density is 1.0
layer3.5.bn2.weight density is 1.0
layer3.5.bn2.bias density is 1.0
layer3.5.conv3.weight density is 1.0
layer3.5.bn3.weight density is 1.0
layer3.5.bn3.bias density is 1.0
layer4.0.conv1.weight density is 1.0
layer4.0.bn1.weight density is 1.0
layer4.0.bn1.bias density is 1.0
layer4.0.conv2.weight density is 1.0
layer4.0.bn2.weight density is 1.0
layer4.0.bn2.bias density is 1.0
layer4.0.conv3.weight density is 1.0
layer4.0.bn3.weight density is 1.0
layer4.0.bn3.bias density is 1.0
layer4.0.downsample.0.weight density is 1.0
layer4.0.downsample.1.weight density is 1.0
layer4.0.downsample.1.bias density is 1.0
layer4.1.conv1.weight density is 1.0
layer4.1.bn1.weight density is 1.0
layer4.1.bn1.bias density is 1.0
layer4.1.conv2.weight density is 1.0
layer4.1.bn2.weight density is 1.0
layer4.1.bn2.bias density is 1.0
layer4.1.conv3.weight density is 1.0
layer4.1.bn3.weight density is 1.0
layer4.1.bn3.bias density is 1.0
layer4.2.conv1.weight density is 1.0
layer4.2.bn1.weight density is 1.0
layer4.2.bn1.bias density is 1.0
layer4.2.conv2.weight density is 1.0
layer4.2.bn2.weight density is 1.0
layer4.2.bn2.bias density is 1.0
layer4.2.conv3.weight density is 1.0
layer4.2.bn3.weight density is 1.0
layer4.2.bn3.bias density is 1.0
fc.weight density is 1.0
fc.bias density is 1.0
Test:  [  0/313]  eta: 0:17:39  loss: 4.2120 (4.2120)  acc1: 19.7917 (19.7917)  acc5: 37.5000 (37.5000)  time: 3.3860  data: 1.8696  max mem: 3726
Test:  [ 10/313]  eta: 0:01:45  loss: 3.9344 (3.7190)  acc1: 26.0417 (29.1667)  acc5: 43.7500 (45.7386)  time: 0.3473  data: 0.1701  max mem: 3726
Test:  [ 20/313]  eta: 0:00:59  loss: 3.6416 (3.4458)  acc1: 28.1250 (34.8214)  acc5: 45.8333 (49.6032)  time: 0.0429  data: 0.0001  max mem: 3726
Test:  [ 30/313]  eta: 0:00:47  loss: 2.8802 (3.2535)  acc1: 42.7083 (38.4073)  acc5: 58.3333 (53.7298)  time: 0.0689  data: 0.0266  max mem: 3726
Test:  [ 40/313]  eta: 0:00:40  loss: 2.7374 (3.0996)  acc1: 46.8750 (41.2348)  acc5: 62.5000 (55.8943)  time: 0.0906  data: 0.0480  max mem: 3726
Test:  [ 50/313]  eta: 0:00:36  loss: 3.5197 (3.2627)  acc1: 34.3750 (38.5825)  acc5: 46.8750 (52.6144)  time: 0.0925  data: 0.0499  max mem: 3726
Test:  [ 60/313]  eta: 0:00:34  loss: 3.8955 (3.3646)  acc1: 25.0000 (36.5266)  acc5: 41.6667 (50.8197)  time: 0.1103  data: 0.0680  max mem: 3726
Test:  [ 70/313]  eta: 0:00:31  loss: 3.7877 (3.3158)  acc1: 28.1250 (37.5587)  acc5: 45.8333 (51.4965)  time: 0.1089  data: 0.0666  max mem: 3726
Test:  [ 80/313]  eta: 0:00:29  loss: 2.4531 (3.1810)  acc1: 48.9583 (39.6862)  acc5: 67.7083 (54.1795)  time: 0.0973  data: 0.0544  max mem: 3726
Test:  [ 90/313]  eta: 0:00:27  loss: 2.0964 (3.0634)  acc1: 53.1250 (41.4950)  acc5: 73.9583 (56.3301)  time: 0.1098  data: 0.0669  max mem: 3726
Test:  [100/313]  eta: 0:00:25  loss: 2.0964 (2.9914)  acc1: 53.1250 (42.6774)  acc5: 71.8750 (57.5289)  time: 0.1019  data: 0.0592  max mem: 3726
Test:  [110/313]  eta: 0:00:23  loss: 2.5491 (2.9953)  acc1: 46.8750 (42.7083)  acc5: 64.5833 (57.4324)  time: 0.0827  data: 0.0398  max mem: 3726
Test:  [120/313]  eta: 0:00:22  loss: 3.1573 (3.0279)  acc1: 40.6250 (42.1832)  acc5: 52.0833 (56.7579)  time: 0.0793  data: 0.0366  max mem: 3726
Test:  [130/313]  eta: 0:00:20  loss: 3.6195 (3.0998)  acc1: 29.1667 (40.8874)  acc5: 45.8333 (55.4866)  time: 0.0871  data: 0.0440  max mem: 3726
Test:  [140/313]  eta: 0:00:19  loss: 3.6054 (3.1270)  acc1: 30.2083 (40.2852)  acc5: 47.9167 (55.1788)  time: 0.0842  data: 0.0413  max mem: 3726
Test:  [150/313]  eta: 0:00:17  loss: 3.2427 (3.1212)  acc1: 34.3750 (40.5422)  acc5: 48.9583 (55.2566)  time: 0.0686  data: 0.0261  max mem: 3726
Test:  [160/313]  eta: 0:00:16  loss: 3.3025 (3.1607)  acc1: 34.3750 (39.8098)  acc5: 44.7917 (54.4255)  time: 0.0765  data: 0.0337  max mem: 3726
Test:  [170/313]  eta: 0:00:14  loss: 3.6730 (3.1895)  acc1: 26.0417 (39.1691)  acc5: 40.6250 (53.8865)  time: 0.0746  data: 0.0319  max mem: 3726
Test:  [180/313]  eta: 0:00:13  loss: 3.7353 (3.2271)  acc1: 26.0417 (38.5244)  acc5: 40.6250 (53.1826)  time: 0.0734  data: 0.0311  max mem: 3726
Test:  [190/313]  eta: 0:00:12  loss: 3.3599 (3.1996)  acc1: 39.5833 (39.1089)  acc5: 55.2083 (53.9431)  time: 0.0849  data: 0.0425  max mem: 3726
Test:  [200/313]  eta: 0:00:11  loss: 3.0185 (3.1924)  acc1: 40.6250 (39.2724)  acc5: 63.5417 (54.3273)  time: 0.1106  data: 0.0683  max mem: 3726
Test:  [210/313]  eta: 0:00:10  loss: 3.1568 (3.1961)  acc1: 37.5000 (39.2674)  acc5: 60.4167 (54.4974)  time: 0.1037  data: 0.0610  max mem: 3726
Test:  [220/313]  eta: 0:00:09  loss: 3.0761 (3.1703)  acc1: 40.6250 (39.7577)  acc5: 60.4167 (55.1329)  time: 0.0634  data: 0.0206  max mem: 3726
Test:  [230/313]  eta: 0:00:08  loss: 2.8126 (3.1541)  acc1: 45.8333 (40.0298)  acc5: 62.5000 (55.6953)  time: 0.0679  data: 0.0255  max mem: 3726
Test:  [240/313]  eta: 0:00:07  loss: 2.9383 (3.1537)  acc1: 43.7500 (40.0934)  acc5: 59.3750 (55.8221)  time: 0.0686  data: 0.0262  max mem: 3726
Test:  [250/313]  eta: 0:00:06  loss: 2.5779 (3.1163)  acc1: 44.7917 (40.7744)  acc5: 62.5000 (56.5281)  time: 0.0679  data: 0.0254  max mem: 3726
Test:  [260/313]  eta: 0:00:05  loss: 2.3596 (3.1001)  acc1: 51.0417 (41.1159)  acc5: 72.9167 (56.7968)  time: 0.0754  data: 0.0328  max mem: 3726
Test:  [270/313]  eta: 0:00:04  loss: 2.2559 (3.0645)  acc1: 54.1667 (41.8243)  acc5: 73.9583 (57.5146)  time: 0.0758  data: 0.0326  max mem: 3726
Test:  [280/313]  eta: 0:00:03  loss: 2.6288 (3.0914)  acc1: 48.9583 (41.3145)  acc5: 63.5417 (57.0359)  time: 0.0662  data: 0.0232  max mem: 3726
Test:  [290/313]  eta: 0:00:02  loss: 3.7537 (3.1182)  acc1: 26.0417 (40.8469)  acc5: 35.4167 (56.4719)  time: 0.0644  data: 0.0220  max mem: 3726
Test:  [300/313]  eta: 0:00:01  loss: 3.3680 (3.1234)  acc1: 28.1250 (40.6423)  acc5: 50.0000 (56.4853)  time: 0.0659  data: 0.0236  max mem: 3726
Test:  [310/313]  eta: 0:00:00  loss: 3.5508 (3.1435)  acc1: 30.2083 (40.2599)  acc5: 47.9167 (56.0993)  time: 0.0698  data: 0.0275  max mem: 3726
Test:  [312/313]  eta: 0:00:00  loss: 3.5508 (3.1503)  acc1: 31.2500 (40.1633)  acc5: 47.9167 (55.9800)  time: 0.0851  data: 0.0365  max mem: 3726
Test: Total time: 0:00:28 (0.0925 s / it)
* Acc@1 40.163 Acc@5 55.980 loss 3.150
Accuracy of the network on 30000 test images: 40.16333%
Not using distributed mode
Namespace(Decom=False, aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=64, bn=True, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/sliu/project_space/datasets/imagenet-r', data_set='IMNET', data_type='imagenet-r', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.1, enable_wandb=False, epochs=300, eval=True, eval_data_path=None, finetune='', fix=False, growth='random', head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, kernel_size=[7, 7, 7, 7, 100], layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=-1, log_dir=None, lr=0.004, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='SLaK_tiny', model_ema=False, model_ema_decay=0.9999, model_ema_eval=False, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_workers=10, only_L=False, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', parallel=True, pin_mem=True, project='SLaK', prune='magnitude', prune_rate=0.3, recount=1, redistribution='none', remode='pixel', reprob=0.25, resplit=False, resume='/gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T3_bnTrue_Teacherswin_STConvNext_NKD/checkpoint-best.pth', save_ckpt=True, save_ckpt_freq=10, save_ckpt_num=3, seed=0, smoothing=0.1, sparse=False, sparse_init='snip', sparsity=0.4, start_epoch=0, train_interpolation='bicubic', update_freq=1, update_frequency=100, use_amp=False, verbose=False, wandb_ckpt=False, warmup_epochs=20, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, width_factor=1.0, world_size=1)
Transform = 
Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /home/sliu/project_space/datasets/imagenet-r
Number of the class = 1000
Mixup is activated!
Model = SLaK(
  (downsample_layers): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (1): LayerNorm()
    )
    (1): Sequential(
      (0): LayerNorm()
      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))
    )
    (2): Sequential(
      (0): LayerNorm()
      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))
    )
    (3): Sequential(
      (0): LayerNorm()
      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (stages): ModuleList(
    (0): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): Identity()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): DropPath()
      )
    )
    (1): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
    )
    (2): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (3): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (4): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (5): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (6): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (7): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (8): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
    )
    (3): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
number of params: 28595752
LR = 0.00400000
Batch size = 64
Update frequent = 1
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "downsample_layers.0.0.weight",
      "downsample_layers.1.1.weight",
      "downsample_layers.2.1.weight",
      "downsample_layers.3.1.weight",
      "stages.0.0.large_kernel.lkb_origin.conv.weight",
      "stages.0.0.pwconv1.weight",
      "stages.0.0.pwconv2.weight",
      "stages.0.1.large_kernel.lkb_origin.conv.weight",
      "stages.0.1.pwconv1.weight",
      "stages.0.1.pwconv2.weight",
      "stages.0.2.large_kernel.lkb_origin.conv.weight",
      "stages.0.2.pwconv1.weight",
      "stages.0.2.pwconv2.weight",
      "stages.1.0.large_kernel.lkb_origin.conv.weight",
      "stages.1.0.pwconv1.weight",
      "stages.1.0.pwconv2.weight",
      "stages.1.1.large_kernel.lkb_origin.conv.weight",
      "stages.1.1.pwconv1.weight",
      "stages.1.1.pwconv2.weight",
      "stages.1.2.large_kernel.lkb_origin.conv.weight",
      "stages.1.2.pwconv1.weight",
      "stages.1.2.pwconv2.weight",
      "stages.2.0.large_kernel.lkb_origin.conv.weight",
      "stages.2.0.pwconv1.weight",
      "stages.2.0.pwconv2.weight",
      "stages.2.1.large_kernel.lkb_origin.conv.weight",
      "stages.2.1.pwconv1.weight",
      "stages.2.1.pwconv2.weight",
      "stages.2.2.large_kernel.lkb_origin.conv.weight",
      "stages.2.2.pwconv1.weight",
      "stages.2.2.pwconv2.weight",
      "stages.2.3.large_kernel.lkb_origin.conv.weight",
      "stages.2.3.pwconv1.weight",
      "stages.2.3.pwconv2.weight",
      "stages.2.4.large_kernel.lkb_origin.conv.weight",
      "stages.2.4.pwconv1.weight",
      "stages.2.4.pwconv2.weight",
      "stages.2.5.large_kernel.lkb_origin.conv.weight",
      "stages.2.5.pwconv1.weight",
      "stages.2.5.pwconv2.weight",
      "stages.2.6.large_kernel.lkb_origin.conv.weight",
      "stages.2.6.pwconv1.weight",
      "stages.2.6.pwconv2.weight",
      "stages.2.7.large_kernel.lkb_origin.conv.weight",
      "stages.2.7.pwconv1.weight",
      "stages.2.7.pwconv2.weight",
      "stages.2.8.large_kernel.lkb_origin.conv.weight",
      "stages.2.8.pwconv1.weight",
      "stages.2.8.pwconv2.weight",
      "stages.3.0.large_kernel.lkb_origin.conv.weight",
      "stages.3.0.pwconv1.weight",
      "stages.3.0.pwconv2.weight",
      "stages.3.1.large_kernel.lkb_origin.conv.weight",
      "stages.3.1.pwconv1.weight",
      "stages.3.1.pwconv2.weight",
      "stages.3.2.large_kernel.lkb_origin.conv.weight",
      "stages.3.2.pwconv1.weight",
      "stages.3.2.pwconv2.weight",
      "head.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "downsample_layers.0.0.bias",
      "downsample_layers.0.1.weight",
      "downsample_layers.0.1.bias",
      "downsample_layers.1.0.weight",
      "downsample_layers.1.0.bias",
      "downsample_layers.1.1.bias",
      "downsample_layers.2.0.weight",
      "downsample_layers.2.0.bias",
      "downsample_layers.2.1.bias",
      "downsample_layers.3.0.weight",
      "downsample_layers.3.0.bias",
      "downsample_layers.3.1.bias",
      "stages.0.0.gamma",
      "stages.0.0.large_kernel.lkb_origin.bn.weight",
      "stages.0.0.large_kernel.lkb_origin.bn.bias",
      "stages.0.0.norm.weight",
      "stages.0.0.norm.bias",
      "stages.0.0.pwconv1.bias",
      "stages.0.0.pwconv2.bias",
      "stages.0.1.gamma",
      "stages.0.1.large_kernel.lkb_origin.bn.weight",
      "stages.0.1.large_kernel.lkb_origin.bn.bias",
      "stages.0.1.norm.weight",
      "stages.0.1.norm.bias",
      "stages.0.1.pwconv1.bias",
      "stages.0.1.pwconv2.bias",
      "stages.0.2.gamma",
      "stages.0.2.large_kernel.lkb_origin.bn.weight",
      "stages.0.2.large_kernel.lkb_origin.bn.bias",
      "stages.0.2.norm.weight",
      "stages.0.2.norm.bias",
      "stages.0.2.pwconv1.bias",
      "stages.0.2.pwconv2.bias",
      "stages.1.0.gamma",
      "stages.1.0.large_kernel.lkb_origin.bn.weight",
      "stages.1.0.large_kernel.lkb_origin.bn.bias",
      "stages.1.0.norm.weight",
      "stages.1.0.norm.bias",
      "stages.1.0.pwconv1.bias",
      "stages.1.0.pwconv2.bias",
      "stages.1.1.gamma",
      "stages.1.1.large_kernel.lkb_origin.bn.weight",
      "stages.1.1.large_kernel.lkb_origin.bn.bias",
      "stages.1.1.norm.weight",
      "stages.1.1.norm.bias",
      "stages.1.1.pwconv1.bias",
      "stages.1.1.pwconv2.bias",
      "stages.1.2.gamma",
      "stages.1.2.large_kernel.lkb_origin.bn.weight",
      "stages.1.2.large_kernel.lkb_origin.bn.bias",
      "stages.1.2.norm.weight",
      "stages.1.2.norm.bias",
      "stages.1.2.pwconv1.bias",
      "stages.1.2.pwconv2.bias",
      "stages.2.0.gamma",
      "stages.2.0.large_kernel.lkb_origin.bn.weight",
      "stages.2.0.large_kernel.lkb_origin.bn.bias",
      "stages.2.0.norm.weight",
      "stages.2.0.norm.bias",
      "stages.2.0.pwconv1.bias",
      "stages.2.0.pwconv2.bias",
      "stages.2.1.gamma",
      "stages.2.1.large_kernel.lkb_origin.bn.weight",
      "stages.2.1.large_kernel.lkb_origin.bn.bias",
      "stages.2.1.norm.weight",
      "stages.2.1.norm.bias",
      "stages.2.1.pwconv1.bias",
      "stages.2.1.pwconv2.bias",
      "stages.2.2.gamma",
      "stages.2.2.large_kernel.lkb_origin.bn.weight",
      "stages.2.2.large_kernel.lkb_origin.bn.bias",
      "stages.2.2.norm.weight",
      "stages.2.2.norm.bias",
      "stages.2.2.pwconv1.bias",
      "stages.2.2.pwconv2.bias",
      "stages.2.3.gamma",
      "stages.2.3.large_kernel.lkb_origin.bn.weight",
      "stages.2.3.large_kernel.lkb_origin.bn.bias",
      "stages.2.3.norm.weight",
      "stages.2.3.norm.bias",
      "stages.2.3.pwconv1.bias",
      "stages.2.3.pwconv2.bias",
      "stages.2.4.gamma",
      "stages.2.4.large_kernel.lkb_origin.bn.weight",
      "stages.2.4.large_kernel.lkb_origin.bn.bias",
      "stages.2.4.norm.weight",
      "stages.2.4.norm.bias",
      "stages.2.4.pwconv1.bias",
      "stages.2.4.pwconv2.bias",
      "stages.2.5.gamma",
      "stages.2.5.large_kernel.lkb_origin.bn.weight",
      "stages.2.5.large_kernel.lkb_origin.bn.bias",
      "stages.2.5.norm.weight",
      "stages.2.5.norm.bias",
      "stages.2.5.pwconv1.bias",
      "stages.2.5.pwconv2.bias",
      "stages.2.6.gamma",
      "stages.2.6.large_kernel.lkb_origin.bn.weight",
      "stages.2.6.large_kernel.lkb_origin.bn.bias",
      "stages.2.6.norm.weight",
      "stages.2.6.norm.bias",
      "stages.2.6.pwconv1.bias",
      "stages.2.6.pwconv2.bias",
      "stages.2.7.gamma",
      "stages.2.7.large_kernel.lkb_origin.bn.weight",
      "stages.2.7.large_kernel.lkb_origin.bn.bias",
      "stages.2.7.norm.weight",
      "stages.2.7.norm.bias",
      "stages.2.7.pwconv1.bias",
      "stages.2.7.pwconv2.bias",
      "stages.2.8.gamma",
      "stages.2.8.large_kernel.lkb_origin.bn.weight",
      "stages.2.8.large_kernel.lkb_origin.bn.bias",
      "stages.2.8.norm.weight",
      "stages.2.8.norm.bias",
      "stages.2.8.pwconv1.bias",
      "stages.2.8.pwconv2.bias",
      "stages.3.0.gamma",
      "stages.3.0.large_kernel.lkb_origin.bn.weight",
      "stages.3.0.large_kernel.lkb_origin.bn.bias",
      "stages.3.0.norm.weight",
      "stages.3.0.norm.bias",
      "stages.3.0.pwconv1.bias",
      "stages.3.0.pwconv2.bias",
      "stages.3.1.gamma",
      "stages.3.1.large_kernel.lkb_origin.bn.weight",
      "stages.3.1.large_kernel.lkb_origin.bn.bias",
      "stages.3.1.norm.weight",
      "stages.3.1.norm.bias",
      "stages.3.1.pwconv1.bias",
      "stages.3.1.pwconv2.bias",
      "stages.3.2.gamma",
      "stages.3.2.large_kernel.lkb_origin.bn.weight",
      "stages.3.2.large_kernel.lkb_origin.bn.bias",
      "stages.3.2.norm.weight",
      "stages.3.2.norm.bias",
      "stages.3.2.pwconv1.bias",
      "stages.3.2.pwconv2.bias",
      "norm.weight",
      "norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
criterion = SoftTargetCrossEntropy()
dict_keys(['model', 'optimizer', 'epoch', 'scaler', 'args', 'model_ema'])
Resume checkpoint /gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T3_bnTrue_Teacherswin_STConvNext_NKD/checkpoint-best.pth
With optim & sched!
downsample_layers.0.0.weight density is 1.0
downsample_layers.0.0.bias density is 1.0
downsample_layers.0.1.weight density is 1.0
downsample_layers.0.1.bias density is 1.0
downsample_layers.1.0.weight density is 1.0
downsample_layers.1.0.bias density is 1.0
downsample_layers.1.1.weight density is 1.0
downsample_layers.1.1.bias density is 1.0
downsample_layers.2.0.weight density is 1.0
downsample_layers.2.0.bias density is 1.0
downsample_layers.2.1.weight density is 1.0
downsample_layers.2.1.bias density is 1.0
downsample_layers.3.0.weight density is 1.0
downsample_layers.3.0.bias density is 1.0
downsample_layers.3.1.weight density is 1.0
downsample_layers.3.1.bias density is 1.0
stages.0.0.gamma density is 1.0
stages.0.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.0.norm.weight density is 1.0
stages.0.0.norm.bias density is 1.0
stages.0.0.pwconv1.weight density is 1.0
stages.0.0.pwconv1.bias density is 1.0
stages.0.0.pwconv2.weight density is 1.0
stages.0.0.pwconv2.bias density is 1.0
stages.0.1.gamma density is 1.0
stages.0.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.1.norm.weight density is 1.0
stages.0.1.norm.bias density is 1.0
stages.0.1.pwconv1.weight density is 1.0
stages.0.1.pwconv1.bias density is 1.0
stages.0.1.pwconv2.weight density is 1.0
stages.0.1.pwconv2.bias density is 1.0
stages.0.2.gamma density is 1.0
stages.0.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.2.norm.weight density is 1.0
stages.0.2.norm.bias density is 1.0
stages.0.2.pwconv1.weight density is 1.0
stages.0.2.pwconv1.bias density is 1.0
stages.0.2.pwconv2.weight density is 1.0
stages.0.2.pwconv2.bias density is 1.0
stages.1.0.gamma density is 1.0
stages.1.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.0.norm.weight density is 1.0
stages.1.0.norm.bias density is 1.0
stages.1.0.pwconv1.weight density is 1.0
stages.1.0.pwconv1.bias density is 1.0
stages.1.0.pwconv2.weight density is 1.0
stages.1.0.pwconv2.bias density is 1.0
stages.1.1.gamma density is 1.0
stages.1.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.1.norm.weight density is 1.0
stages.1.1.norm.bias density is 1.0
stages.1.1.pwconv1.weight density is 1.0
stages.1.1.pwconv1.bias density is 1.0
stages.1.1.pwconv2.weight density is 1.0
stages.1.1.pwconv2.bias density is 1.0
stages.1.2.gamma density is 1.0
stages.1.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.2.norm.weight density is 1.0
stages.1.2.norm.bias density is 1.0
stages.1.2.pwconv1.weight density is 1.0
stages.1.2.pwconv1.bias density is 1.0
stages.1.2.pwconv2.weight density is 1.0
stages.1.2.pwconv2.bias density is 1.0
stages.2.0.gamma density is 1.0
stages.2.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.0.norm.weight density is 1.0
stages.2.0.norm.bias density is 1.0
stages.2.0.pwconv1.weight density is 1.0
stages.2.0.pwconv1.bias density is 1.0
stages.2.0.pwconv2.weight density is 1.0
stages.2.0.pwconv2.bias density is 1.0
stages.2.1.gamma density is 1.0
stages.2.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.1.norm.weight density is 1.0
stages.2.1.norm.bias density is 1.0
stages.2.1.pwconv1.weight density is 1.0
stages.2.1.pwconv1.bias density is 1.0
stages.2.1.pwconv2.weight density is 1.0
stages.2.1.pwconv2.bias density is 1.0
stages.2.2.gamma density is 1.0
stages.2.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.2.norm.weight density is 1.0
stages.2.2.norm.bias density is 1.0
stages.2.2.pwconv1.weight density is 1.0
stages.2.2.pwconv1.bias density is 1.0
stages.2.2.pwconv2.weight density is 1.0
stages.2.2.pwconv2.bias density is 1.0
stages.2.3.gamma density is 1.0
stages.2.3.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.3.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.3.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.3.norm.weight density is 1.0
stages.2.3.norm.bias density is 1.0
stages.2.3.pwconv1.weight density is 1.0
stages.2.3.pwconv1.bias density is 1.0
stages.2.3.pwconv2.weight density is 1.0
stages.2.3.pwconv2.bias density is 1.0
stages.2.4.gamma density is 1.0
stages.2.4.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.4.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.4.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.4.norm.weight density is 1.0
stages.2.4.norm.bias density is 1.0
stages.2.4.pwconv1.weight density is 1.0
stages.2.4.pwconv1.bias density is 1.0
stages.2.4.pwconv2.weight density is 1.0
stages.2.4.pwconv2.bias density is 1.0
stages.2.5.gamma density is 1.0
stages.2.5.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.5.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.5.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.5.norm.weight density is 1.0
stages.2.5.norm.bias density is 1.0
stages.2.5.pwconv1.weight density is 1.0
stages.2.5.pwconv1.bias density is 1.0
stages.2.5.pwconv2.weight density is 1.0
stages.2.5.pwconv2.bias density is 1.0
stages.2.6.gamma density is 1.0
stages.2.6.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.6.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.6.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.6.norm.weight density is 1.0
stages.2.6.norm.bias density is 1.0
stages.2.6.pwconv1.weight density is 1.0
stages.2.6.pwconv1.bias density is 1.0
stages.2.6.pwconv2.weight density is 1.0
stages.2.6.pwconv2.bias density is 1.0
stages.2.7.gamma density is 1.0
stages.2.7.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.7.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.7.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.7.norm.weight density is 1.0
stages.2.7.norm.bias density is 1.0
stages.2.7.pwconv1.weight density is 1.0
stages.2.7.pwconv1.bias density is 1.0
stages.2.7.pwconv2.weight density is 1.0
stages.2.7.pwconv2.bias density is 1.0
stages.2.8.gamma density is 1.0
stages.2.8.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.8.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.8.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.8.norm.weight density is 1.0
stages.2.8.norm.bias density is 1.0
stages.2.8.pwconv1.weight density is 1.0
stages.2.8.pwconv1.bias density is 1.0
stages.2.8.pwconv2.weight density is 1.0
stages.2.8.pwconv2.bias density is 1.0
stages.3.0.gamma density is 1.0
stages.3.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.0.norm.weight density is 1.0
stages.3.0.norm.bias density is 1.0
stages.3.0.pwconv1.weight density is 1.0
stages.3.0.pwconv1.bias density is 1.0
stages.3.0.pwconv2.weight density is 1.0
stages.3.0.pwconv2.bias density is 1.0
stages.3.1.gamma density is 1.0
stages.3.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.1.norm.weight density is 1.0
stages.3.1.norm.bias density is 1.0
stages.3.1.pwconv1.weight density is 1.0
stages.3.1.pwconv1.bias density is 1.0
stages.3.1.pwconv2.weight density is 1.0
stages.3.1.pwconv2.bias density is 1.0
stages.3.2.gamma density is 1.0
stages.3.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.2.norm.weight density is 1.0
stages.3.2.norm.bias density is 1.0
stages.3.2.pwconv1.weight density is 1.0
stages.3.2.pwconv1.bias density is 1.0
stages.3.2.pwconv2.weight density is 1.0
stages.3.2.pwconv2.bias density is 1.0
norm.weight density is 1.0
norm.bias density is 1.0
head.weight density is 1.0
head.bias density is 1.0
Test:  [  0/313]  eta: 0:16:51  loss: 4.1208 (4.1208)  acc1: 21.8750 (21.8750)  acc5: 32.2917 (32.2917)  time: 3.2318  data: 2.0438  max mem: 2139
Test:  [ 10/313]  eta: 0:01:47  loss: 3.4964 (3.4907)  acc1: 28.1250 (31.3447)  acc5: 48.9583 (49.4318)  time: 0.3533  data: 0.1859  max mem: 2139
Test:  [ 20/313]  eta: 0:01:03  loss: 3.4646 (3.3181)  acc1: 29.1667 (36.1111)  acc5: 48.9583 (51.9345)  time: 0.0643  data: 0.0001  max mem: 2139
Test:  [ 30/313]  eta: 0:00:48  loss: 2.4173 (3.0460)  acc1: 46.8750 (40.5578)  acc5: 62.5000 (56.8884)  time: 0.0691  data: 0.0059  max mem: 2139
Test:  [ 40/313]  eta: 0:00:39  loss: 2.3295 (2.9226)  acc1: 51.0417 (42.8608)  acc5: 67.7083 (58.7398)  time: 0.0695  data: 0.0059  max mem: 2139
Test:  [ 50/313]  eta: 0:00:34  loss: 3.3339 (3.1165)  acc1: 36.4583 (40.1961)  acc5: 48.9583 (55.3513)  time: 0.0667  data: 0.0029  max mem: 2139
Test:  [ 60/313]  eta: 0:00:31  loss: 3.8504 (3.2039)  acc1: 23.9583 (38.3880)  acc5: 39.5833 (53.8593)  time: 0.0832  data: 0.0199  max mem: 2139
Test:  [ 70/313]  eta: 0:00:29  loss: 3.3108 (3.1445)  acc1: 34.3750 (39.7300)  acc5: 52.0833 (54.8709)  time: 0.0998  data: 0.0367  max mem: 2139
Test:  [ 80/313]  eta: 0:00:27  loss: 2.0590 (2.9840)  acc1: 54.1667 (42.1296)  acc5: 71.8750 (57.6518)  time: 0.1009  data: 0.0374  max mem: 2139
Test:  [ 90/313]  eta: 0:00:25  loss: 1.7000 (2.8447)  acc1: 57.2917 (44.4025)  acc5: 79.1667 (60.1419)  time: 0.0935  data: 0.0297  max mem: 2139
Test:  [100/313]  eta: 0:00:23  loss: 1.7560 (2.7596)  acc1: 57.2917 (45.8436)  acc5: 78.1250 (61.6234)  time: 0.0882  data: 0.0249  max mem: 2139
Test:  [110/313]  eta: 0:00:22  loss: 2.2800 (2.7580)  acc1: 51.0417 (46.0210)  acc5: 71.8750 (61.7023)  time: 0.0786  data: 0.0153  max mem: 2139
Test:  [120/313]  eta: 0:00:20  loss: 2.8965 (2.7899)  acc1: 45.8333 (45.4890)  acc5: 59.3750 (61.1829)  time: 0.0757  data: 0.0121  max mem: 2139
Test:  [130/313]  eta: 0:00:19  loss: 3.3053 (2.8688)  acc1: 34.3750 (44.0999)  acc5: 53.1250 (59.8282)  time: 0.0978  data: 0.0345  max mem: 2139
Test:  [140/313]  eta: 0:00:18  loss: 3.4261 (2.9002)  acc1: 31.2500 (43.4397)  acc5: 50.0000 (59.3602)  time: 0.0909  data: 0.0279  max mem: 2139
Test:  [150/313]  eta: 0:00:16  loss: 2.9103 (2.8918)  acc1: 41.6667 (43.7362)  acc5: 56.2500 (59.4302)  time: 0.0688  data: 0.0059  max mem: 2139
Test:  [160/313]  eta: 0:00:15  loss: 3.1346 (2.9326)  acc1: 39.5833 (42.9477)  acc5: 51.0417 (58.6051)  time: 0.0739  data: 0.0107  max mem: 2139
Test:  [170/313]  eta: 0:00:14  loss: 3.5902 (2.9604)  acc1: 29.1667 (42.4038)  acc5: 45.8333 (58.0836)  time: 0.0880  data: 0.0242  max mem: 2139
Test:  [180/313]  eta: 0:00:13  loss: 3.7380 (3.0040)  acc1: 29.1667 (41.6206)  acc5: 41.6667 (57.3320)  time: 0.0863  data: 0.0223  max mem: 2139
Test:  [190/313]  eta: 0:00:12  loss: 2.9880 (2.9715)  acc1: 41.6667 (42.2066)  acc5: 57.2917 (57.9679)  time: 0.0825  data: 0.0187  max mem: 2139
Test:  [200/313]  eta: 0:00:11  loss: 2.7458 (2.9644)  acc1: 43.7500 (42.2626)  acc5: 66.6667 (58.2763)  time: 0.1192  data: 0.0556  max mem: 2139
Test:  [210/313]  eta: 0:00:10  loss: 2.9329 (2.9663)  acc1: 39.5833 (42.2196)  acc5: 62.5000 (58.3827)  time: 0.1137  data: 0.0504  max mem: 2139
Test:  [220/313]  eta: 0:00:09  loss: 2.7969 (2.9364)  acc1: 45.8333 (42.7130)  acc5: 60.4167 (58.9508)  time: 0.0716  data: 0.0085  max mem: 2139
Test:  [230/313]  eta: 0:00:08  loss: 2.2443 (2.9121)  acc1: 48.9583 (43.0601)  acc5: 70.8333 (59.5869)  time: 0.0753  data: 0.0122  max mem: 2139
Test:  [240/313]  eta: 0:00:07  loss: 2.5025 (2.9135)  acc1: 47.9167 (43.1362)  acc5: 66.6667 (59.6300)  time: 0.0786  data: 0.0154  max mem: 2139
Test:  [250/313]  eta: 0:00:06  loss: 2.3326 (2.8737)  acc1: 48.9583 (43.9285)  acc5: 69.7917 (60.3295)  time: 0.0700  data: 0.0067  max mem: 2139
Test:  [260/313]  eta: 0:00:05  loss: 2.0280 (2.8593)  acc1: 57.2917 (44.2569)  acc5: 78.1250 (60.5484)  time: 0.0829  data: 0.0194  max mem: 2139
Test:  [270/313]  eta: 0:00:04  loss: 2.0280 (2.8172)  acc1: 58.3333 (45.0031)  acc5: 76.0417 (61.3238)  time: 0.0883  data: 0.0246  max mem: 2139
Test:  [280/313]  eta: 0:00:03  loss: 2.3794 (2.8425)  acc1: 46.8750 (44.5062)  acc5: 69.7917 (60.8949)  time: 0.0744  data: 0.0112  max mem: 2139
Test:  [290/313]  eta: 0:00:02  loss: 3.5690 (2.8737)  acc1: 28.1250 (43.9934)  acc5: 44.7917 (60.3093)  time: 0.0720  data: 0.0083  max mem: 2139
Test:  [300/313]  eta: 0:00:01  loss: 3.2276 (2.8767)  acc1: 33.3333 (43.8469)  acc5: 53.1250 (60.3405)  time: 0.0698  data: 0.0060  max mem: 2139
Test:  [310/313]  eta: 0:00:00  loss: 3.2019 (2.8990)  acc1: 36.4583 (43.4553)  acc5: 53.1250 (59.9176)  time: 0.0809  data: 0.0179  max mem: 2139
Test:  [312/313]  eta: 0:00:00  loss: 3.2019 (2.9059)  acc1: 36.4583 (43.3533)  acc5: 52.0833 (59.8033)  time: 0.0841  data: 0.0214  max mem: 2139
Test: Total time: 0:00:29 (0.0930 s / it)
* Acc@1 43.353 Acc@5 59.803 loss 2.906
Accuracy of the network on 30000 test images: 43.35333%
Not using distributed mode
Namespace(Decom=False, aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=64, bn=True, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/sliu/project_space/datasets/imagenet-r', data_set='IMNET', data_type='imagenet-r', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.1, enable_wandb=False, epochs=300, eval=True, eval_data_path=None, finetune='', fix=False, growth='random', head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, kernel_size=[7, 7, 7, 7, 100], layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=-1, log_dir=None, lr=0.004, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='SLaK_tiny', model_ema=False, model_ema_decay=0.9999, model_ema_eval=False, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_workers=10, only_L=False, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', parallel=True, pin_mem=True, project='SLaK', prune='magnitude', prune_rate=0.3, recount=1, redistribution='none', remode='pixel', reprob=0.25, resplit=False, resume='/gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T3_bnTrue_Teachercswin_STConvNext_NKD/checkpoint-best.pth', save_ckpt=True, save_ckpt_freq=10, save_ckpt_num=3, seed=0, smoothing=0.1, sparse=False, sparse_init='snip', sparsity=0.4, start_epoch=0, train_interpolation='bicubic', update_freq=1, update_frequency=100, use_amp=False, verbose=False, wandb_ckpt=False, warmup_epochs=20, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, width_factor=1.0, world_size=1)
Transform = 
Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /home/sliu/project_space/datasets/imagenet-r
Number of the class = 1000
Mixup is activated!
Model = SLaK(
  (downsample_layers): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (1): LayerNorm()
    )
    (1): Sequential(
      (0): LayerNorm()
      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))
    )
    (2): Sequential(
      (0): LayerNorm()
      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))
    )
    (3): Sequential(
      (0): LayerNorm()
      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (stages): ModuleList(
    (0): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): Identity()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): DropPath()
      )
    )
    (1): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
    )
    (2): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (3): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (4): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (5): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (6): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (7): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (8): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
    )
    (3): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
number of params: 28595752
LR = 0.00400000
Batch size = 64
Update frequent = 1
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "downsample_layers.0.0.weight",
      "downsample_layers.1.1.weight",
      "downsample_layers.2.1.weight",
      "downsample_layers.3.1.weight",
      "stages.0.0.large_kernel.lkb_origin.conv.weight",
      "stages.0.0.pwconv1.weight",
      "stages.0.0.pwconv2.weight",
      "stages.0.1.large_kernel.lkb_origin.conv.weight",
      "stages.0.1.pwconv1.weight",
      "stages.0.1.pwconv2.weight",
      "stages.0.2.large_kernel.lkb_origin.conv.weight",
      "stages.0.2.pwconv1.weight",
      "stages.0.2.pwconv2.weight",
      "stages.1.0.large_kernel.lkb_origin.conv.weight",
      "stages.1.0.pwconv1.weight",
      "stages.1.0.pwconv2.weight",
      "stages.1.1.large_kernel.lkb_origin.conv.weight",
      "stages.1.1.pwconv1.weight",
      "stages.1.1.pwconv2.weight",
      "stages.1.2.large_kernel.lkb_origin.conv.weight",
      "stages.1.2.pwconv1.weight",
      "stages.1.2.pwconv2.weight",
      "stages.2.0.large_kernel.lkb_origin.conv.weight",
      "stages.2.0.pwconv1.weight",
      "stages.2.0.pwconv2.weight",
      "stages.2.1.large_kernel.lkb_origin.conv.weight",
      "stages.2.1.pwconv1.weight",
      "stages.2.1.pwconv2.weight",
      "stages.2.2.large_kernel.lkb_origin.conv.weight",
      "stages.2.2.pwconv1.weight",
      "stages.2.2.pwconv2.weight",
      "stages.2.3.large_kernel.lkb_origin.conv.weight",
      "stages.2.3.pwconv1.weight",
      "stages.2.3.pwconv2.weight",
      "stages.2.4.large_kernel.lkb_origin.conv.weight",
      "stages.2.4.pwconv1.weight",
      "stages.2.4.pwconv2.weight",
      "stages.2.5.large_kernel.lkb_origin.conv.weight",
      "stages.2.5.pwconv1.weight",
      "stages.2.5.pwconv2.weight",
      "stages.2.6.large_kernel.lkb_origin.conv.weight",
      "stages.2.6.pwconv1.weight",
      "stages.2.6.pwconv2.weight",
      "stages.2.7.large_kernel.lkb_origin.conv.weight",
      "stages.2.7.pwconv1.weight",
      "stages.2.7.pwconv2.weight",
      "stages.2.8.large_kernel.lkb_origin.conv.weight",
      "stages.2.8.pwconv1.weight",
      "stages.2.8.pwconv2.weight",
      "stages.3.0.large_kernel.lkb_origin.conv.weight",
      "stages.3.0.pwconv1.weight",
      "stages.3.0.pwconv2.weight",
      "stages.3.1.large_kernel.lkb_origin.conv.weight",
      "stages.3.1.pwconv1.weight",
      "stages.3.1.pwconv2.weight",
      "stages.3.2.large_kernel.lkb_origin.conv.weight",
      "stages.3.2.pwconv1.weight",
      "stages.3.2.pwconv2.weight",
      "head.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "downsample_layers.0.0.bias",
      "downsample_layers.0.1.weight",
      "downsample_layers.0.1.bias",
      "downsample_layers.1.0.weight",
      "downsample_layers.1.0.bias",
      "downsample_layers.1.1.bias",
      "downsample_layers.2.0.weight",
      "downsample_layers.2.0.bias",
      "downsample_layers.2.1.bias",
      "downsample_layers.3.0.weight",
      "downsample_layers.3.0.bias",
      "downsample_layers.3.1.bias",
      "stages.0.0.gamma",
      "stages.0.0.large_kernel.lkb_origin.bn.weight",
      "stages.0.0.large_kernel.lkb_origin.bn.bias",
      "stages.0.0.norm.weight",
      "stages.0.0.norm.bias",
      "stages.0.0.pwconv1.bias",
      "stages.0.0.pwconv2.bias",
      "stages.0.1.gamma",
      "stages.0.1.large_kernel.lkb_origin.bn.weight",
      "stages.0.1.large_kernel.lkb_origin.bn.bias",
      "stages.0.1.norm.weight",
      "stages.0.1.norm.bias",
      "stages.0.1.pwconv1.bias",
      "stages.0.1.pwconv2.bias",
      "stages.0.2.gamma",
      "stages.0.2.large_kernel.lkb_origin.bn.weight",
      "stages.0.2.large_kernel.lkb_origin.bn.bias",
      "stages.0.2.norm.weight",
      "stages.0.2.norm.bias",
      "stages.0.2.pwconv1.bias",
      "stages.0.2.pwconv2.bias",
      "stages.1.0.gamma",
      "stages.1.0.large_kernel.lkb_origin.bn.weight",
      "stages.1.0.large_kernel.lkb_origin.bn.bias",
      "stages.1.0.norm.weight",
      "stages.1.0.norm.bias",
      "stages.1.0.pwconv1.bias",
      "stages.1.0.pwconv2.bias",
      "stages.1.1.gamma",
      "stages.1.1.large_kernel.lkb_origin.bn.weight",
      "stages.1.1.large_kernel.lkb_origin.bn.bias",
      "stages.1.1.norm.weight",
      "stages.1.1.norm.bias",
      "stages.1.1.pwconv1.bias",
      "stages.1.1.pwconv2.bias",
      "stages.1.2.gamma",
      "stages.1.2.large_kernel.lkb_origin.bn.weight",
      "stages.1.2.large_kernel.lkb_origin.bn.bias",
      "stages.1.2.norm.weight",
      "stages.1.2.norm.bias",
      "stages.1.2.pwconv1.bias",
      "stages.1.2.pwconv2.bias",
      "stages.2.0.gamma",
      "stages.2.0.large_kernel.lkb_origin.bn.weight",
      "stages.2.0.large_kernel.lkb_origin.bn.bias",
      "stages.2.0.norm.weight",
      "stages.2.0.norm.bias",
      "stages.2.0.pwconv1.bias",
      "stages.2.0.pwconv2.bias",
      "stages.2.1.gamma",
      "stages.2.1.large_kernel.lkb_origin.bn.weight",
      "stages.2.1.large_kernel.lkb_origin.bn.bias",
      "stages.2.1.norm.weight",
      "stages.2.1.norm.bias",
      "stages.2.1.pwconv1.bias",
      "stages.2.1.pwconv2.bias",
      "stages.2.2.gamma",
      "stages.2.2.large_kernel.lkb_origin.bn.weight",
      "stages.2.2.large_kernel.lkb_origin.bn.bias",
      "stages.2.2.norm.weight",
      "stages.2.2.norm.bias",
      "stages.2.2.pwconv1.bias",
      "stages.2.2.pwconv2.bias",
      "stages.2.3.gamma",
      "stages.2.3.large_kernel.lkb_origin.bn.weight",
      "stages.2.3.large_kernel.lkb_origin.bn.bias",
      "stages.2.3.norm.weight",
      "stages.2.3.norm.bias",
      "stages.2.3.pwconv1.bias",
      "stages.2.3.pwconv2.bias",
      "stages.2.4.gamma",
      "stages.2.4.large_kernel.lkb_origin.bn.weight",
      "stages.2.4.large_kernel.lkb_origin.bn.bias",
      "stages.2.4.norm.weight",
      "stages.2.4.norm.bias",
      "stages.2.4.pwconv1.bias",
      "stages.2.4.pwconv2.bias",
      "stages.2.5.gamma",
      "stages.2.5.large_kernel.lkb_origin.bn.weight",
      "stages.2.5.large_kernel.lkb_origin.bn.bias",
      "stages.2.5.norm.weight",
      "stages.2.5.norm.bias",
      "stages.2.5.pwconv1.bias",
      "stages.2.5.pwconv2.bias",
      "stages.2.6.gamma",
      "stages.2.6.large_kernel.lkb_origin.bn.weight",
      "stages.2.6.large_kernel.lkb_origin.bn.bias",
      "stages.2.6.norm.weight",
      "stages.2.6.norm.bias",
      "stages.2.6.pwconv1.bias",
      "stages.2.6.pwconv2.bias",
      "stages.2.7.gamma",
      "stages.2.7.large_kernel.lkb_origin.bn.weight",
      "stages.2.7.large_kernel.lkb_origin.bn.bias",
      "stages.2.7.norm.weight",
      "stages.2.7.norm.bias",
      "stages.2.7.pwconv1.bias",
      "stages.2.7.pwconv2.bias",
      "stages.2.8.gamma",
      "stages.2.8.large_kernel.lkb_origin.bn.weight",
      "stages.2.8.large_kernel.lkb_origin.bn.bias",
      "stages.2.8.norm.weight",
      "stages.2.8.norm.bias",
      "stages.2.8.pwconv1.bias",
      "stages.2.8.pwconv2.bias",
      "stages.3.0.gamma",
      "stages.3.0.large_kernel.lkb_origin.bn.weight",
      "stages.3.0.large_kernel.lkb_origin.bn.bias",
      "stages.3.0.norm.weight",
      "stages.3.0.norm.bias",
      "stages.3.0.pwconv1.bias",
      "stages.3.0.pwconv2.bias",
      "stages.3.1.gamma",
      "stages.3.1.large_kernel.lkb_origin.bn.weight",
      "stages.3.1.large_kernel.lkb_origin.bn.bias",
      "stages.3.1.norm.weight",
      "stages.3.1.norm.bias",
      "stages.3.1.pwconv1.bias",
      "stages.3.1.pwconv2.bias",
      "stages.3.2.gamma",
      "stages.3.2.large_kernel.lkb_origin.bn.weight",
      "stages.3.2.large_kernel.lkb_origin.bn.bias",
      "stages.3.2.norm.weight",
      "stages.3.2.norm.bias",
      "stages.3.2.pwconv1.bias",
      "stages.3.2.pwconv2.bias",
      "norm.weight",
      "norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
criterion = SoftTargetCrossEntropy()
dict_keys(['model', 'optimizer', 'epoch', 'scaler', 'args', 'model_ema'])
Resume checkpoint /gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T3_bnTrue_Teachercswin_STConvNext_NKD/checkpoint-best.pth
With optim & sched!
downsample_layers.0.0.weight density is 1.0
downsample_layers.0.0.bias density is 1.0
downsample_layers.0.1.weight density is 1.0
downsample_layers.0.1.bias density is 1.0
downsample_layers.1.0.weight density is 1.0
downsample_layers.1.0.bias density is 1.0
downsample_layers.1.1.weight density is 1.0
downsample_layers.1.1.bias density is 1.0
downsample_layers.2.0.weight density is 1.0
downsample_layers.2.0.bias density is 1.0
downsample_layers.2.1.weight density is 1.0
downsample_layers.2.1.bias density is 1.0
downsample_layers.3.0.weight density is 1.0
downsample_layers.3.0.bias density is 1.0
downsample_layers.3.1.weight density is 1.0
downsample_layers.3.1.bias density is 1.0
stages.0.0.gamma density is 1.0
stages.0.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.0.norm.weight density is 1.0
stages.0.0.norm.bias density is 1.0
stages.0.0.pwconv1.weight density is 1.0
stages.0.0.pwconv1.bias density is 1.0
stages.0.0.pwconv2.weight density is 1.0
stages.0.0.pwconv2.bias density is 1.0
stages.0.1.gamma density is 1.0
stages.0.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.1.norm.weight density is 1.0
stages.0.1.norm.bias density is 1.0
stages.0.1.pwconv1.weight density is 1.0
stages.0.1.pwconv1.bias density is 1.0
stages.0.1.pwconv2.weight density is 1.0
stages.0.1.pwconv2.bias density is 1.0
stages.0.2.gamma density is 1.0
stages.0.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.2.norm.weight density is 1.0
stages.0.2.norm.bias density is 1.0
stages.0.2.pwconv1.weight density is 1.0
stages.0.2.pwconv1.bias density is 1.0
stages.0.2.pwconv2.weight density is 1.0
stages.0.2.pwconv2.bias density is 1.0
stages.1.0.gamma density is 1.0
stages.1.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.0.norm.weight density is 1.0
stages.1.0.norm.bias density is 1.0
stages.1.0.pwconv1.weight density is 1.0
stages.1.0.pwconv1.bias density is 1.0
stages.1.0.pwconv2.weight density is 1.0
stages.1.0.pwconv2.bias density is 1.0
stages.1.1.gamma density is 1.0
stages.1.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.1.norm.weight density is 1.0
stages.1.1.norm.bias density is 1.0
stages.1.1.pwconv1.weight density is 1.0
stages.1.1.pwconv1.bias density is 1.0
stages.1.1.pwconv2.weight density is 1.0
stages.1.1.pwconv2.bias density is 1.0
stages.1.2.gamma density is 1.0
stages.1.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.2.norm.weight density is 1.0
stages.1.2.norm.bias density is 1.0
stages.1.2.pwconv1.weight density is 1.0
stages.1.2.pwconv1.bias density is 1.0
stages.1.2.pwconv2.weight density is 1.0
stages.1.2.pwconv2.bias density is 1.0
stages.2.0.gamma density is 1.0
stages.2.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.0.norm.weight density is 1.0
stages.2.0.norm.bias density is 1.0
stages.2.0.pwconv1.weight density is 1.0
stages.2.0.pwconv1.bias density is 1.0
stages.2.0.pwconv2.weight density is 1.0
stages.2.0.pwconv2.bias density is 1.0
stages.2.1.gamma density is 1.0
stages.2.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.1.norm.weight density is 1.0
stages.2.1.norm.bias density is 1.0
stages.2.1.pwconv1.weight density is 1.0
stages.2.1.pwconv1.bias density is 1.0
stages.2.1.pwconv2.weight density is 1.0
stages.2.1.pwconv2.bias density is 1.0
stages.2.2.gamma density is 1.0
stages.2.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.2.norm.weight density is 1.0
stages.2.2.norm.bias density is 1.0
stages.2.2.pwconv1.weight density is 1.0
stages.2.2.pwconv1.bias density is 1.0
stages.2.2.pwconv2.weight density is 1.0
stages.2.2.pwconv2.bias density is 1.0
stages.2.3.gamma density is 1.0
stages.2.3.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.3.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.3.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.3.norm.weight density is 1.0
stages.2.3.norm.bias density is 1.0
stages.2.3.pwconv1.weight density is 1.0
stages.2.3.pwconv1.bias density is 1.0
stages.2.3.pwconv2.weight density is 1.0
stages.2.3.pwconv2.bias density is 1.0
stages.2.4.gamma density is 1.0
stages.2.4.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.4.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.4.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.4.norm.weight density is 1.0
stages.2.4.norm.bias density is 1.0
stages.2.4.pwconv1.weight density is 1.0
stages.2.4.pwconv1.bias density is 1.0
stages.2.4.pwconv2.weight density is 1.0
stages.2.4.pwconv2.bias density is 1.0
stages.2.5.gamma density is 1.0
stages.2.5.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.5.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.5.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.5.norm.weight density is 1.0
stages.2.5.norm.bias density is 1.0
stages.2.5.pwconv1.weight density is 1.0
stages.2.5.pwconv1.bias density is 1.0
stages.2.5.pwconv2.weight density is 1.0
stages.2.5.pwconv2.bias density is 1.0
stages.2.6.gamma density is 1.0
stages.2.6.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.6.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.6.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.6.norm.weight density is 1.0
stages.2.6.norm.bias density is 1.0
stages.2.6.pwconv1.weight density is 1.0
stages.2.6.pwconv1.bias density is 1.0
stages.2.6.pwconv2.weight density is 1.0
stages.2.6.pwconv2.bias density is 1.0
stages.2.7.gamma density is 1.0
stages.2.7.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.7.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.7.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.7.norm.weight density is 1.0
stages.2.7.norm.bias density is 1.0
stages.2.7.pwconv1.weight density is 1.0
stages.2.7.pwconv1.bias density is 1.0
stages.2.7.pwconv2.weight density is 1.0
stages.2.7.pwconv2.bias density is 1.0
stages.2.8.gamma density is 1.0
stages.2.8.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.8.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.8.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.8.norm.weight density is 1.0
stages.2.8.norm.bias density is 1.0
stages.2.8.pwconv1.weight density is 1.0
stages.2.8.pwconv1.bias density is 1.0
stages.2.8.pwconv2.weight density is 1.0
stages.2.8.pwconv2.bias density is 1.0
stages.3.0.gamma density is 1.0
stages.3.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.0.norm.weight density is 1.0
stages.3.0.norm.bias density is 1.0
stages.3.0.pwconv1.weight density is 1.0
stages.3.0.pwconv1.bias density is 1.0
stages.3.0.pwconv2.weight density is 1.0
stages.3.0.pwconv2.bias density is 1.0
stages.3.1.gamma density is 1.0
stages.3.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.1.norm.weight density is 1.0
stages.3.1.norm.bias density is 1.0
stages.3.1.pwconv1.weight density is 1.0
stages.3.1.pwconv1.bias density is 1.0
stages.3.1.pwconv2.weight density is 1.0
stages.3.1.pwconv2.bias density is 1.0
stages.3.2.gamma density is 1.0
stages.3.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.2.norm.weight density is 1.0
stages.3.2.norm.bias density is 1.0
stages.3.2.pwconv1.weight density is 1.0
stages.3.2.pwconv1.bias density is 1.0
stages.3.2.pwconv2.weight density is 1.0
stages.3.2.pwconv2.bias density is 1.0
norm.weight density is 1.0
norm.bias density is 1.0
head.weight density is 1.0
head.bias density is 1.0
Test:  [  0/313]  eta: 0:16:17  loss: 4.3153 (4.3153)  acc1: 21.8750 (21.8750)  acc5: 32.2917 (32.2917)  time: 3.1236  data: 1.7811  max mem: 2139
Test:  [ 10/313]  eta: 0:01:43  loss: 3.6138 (3.5187)  acc1: 29.1667 (32.5758)  acc5: 46.8750 (48.3902)  time: 0.3422  data: 0.1620  max mem: 2139
Test:  [ 20/313]  eta: 0:01:01  loss: 3.3871 (3.2915)  acc1: 33.3333 (37.2024)  acc5: 46.8750 (52.4306)  time: 0.0638  data: 0.0001  max mem: 2139
Test:  [ 30/313]  eta: 0:00:45  loss: 2.4044 (3.0034)  acc1: 51.0417 (42.6411)  acc5: 63.5417 (57.6277)  time: 0.0633  data: 0.0001  max mem: 2139
Test:  [ 40/313]  eta: 0:00:37  loss: 2.3370 (2.8815)  acc1: 53.1250 (45.2998)  acc5: 68.7500 (59.5020)  time: 0.0637  data: 0.0007  max mem: 2139
Test:  [ 50/313]  eta: 0:00:33  loss: 3.3896 (3.0644)  acc1: 33.3333 (42.3407)  acc5: 50.0000 (56.1070)  time: 0.0715  data: 0.0084  max mem: 2139
Test:  [ 60/313]  eta: 0:00:29  loss: 3.9167 (3.1855)  acc1: 22.9167 (39.9590)  acc5: 34.3750 (53.5519)  time: 0.0756  data: 0.0121  max mem: 2139
Test:  [ 70/313]  eta: 0:00:27  loss: 3.5318 (3.1264)  acc1: 30.2083 (41.1678)  acc5: 45.8333 (54.5921)  time: 0.0720  data: 0.0087  max mem: 2139
Test:  [ 80/313]  eta: 0:00:26  loss: 2.0471 (2.9635)  acc1: 57.2917 (43.5828)  acc5: 75.0000 (57.7289)  time: 0.0948  data: 0.0318  max mem: 2139
Test:  [ 90/313]  eta: 0:00:24  loss: 1.7528 (2.8295)  acc1: 59.3750 (45.8104)  acc5: 81.2500 (60.1419)  time: 0.0973  data: 0.0343  max mem: 2139
Test:  [100/313]  eta: 0:00:22  loss: 1.8225 (2.7437)  acc1: 61.4583 (47.1947)  acc5: 78.1250 (61.6440)  time: 0.0797  data: 0.0163  max mem: 2139
Test:  [110/313]  eta: 0:00:20  loss: 2.1399 (2.7424)  acc1: 51.0417 (47.2128)  acc5: 72.9167 (61.6648)  time: 0.0788  data: 0.0154  max mem: 2139
Test:  [120/313]  eta: 0:00:19  loss: 2.8140 (2.7812)  acc1: 44.7917 (46.5393)  acc5: 61.4583 (61.1054)  time: 0.0782  data: 0.0152  max mem: 2139
Test:  [130/313]  eta: 0:00:18  loss: 3.4683 (2.8754)  acc1: 31.2500 (44.9109)  acc5: 48.9583 (59.5261)  time: 0.0979  data: 0.0346  max mem: 2139
Test:  [140/313]  eta: 0:00:17  loss: 3.3303 (2.8985)  acc1: 33.3333 (44.4666)  acc5: 48.9583 (59.2199)  time: 0.0888  data: 0.0253  max mem: 2139
Test:  [150/313]  eta: 0:00:15  loss: 2.9527 (2.8892)  acc1: 40.6250 (44.7572)  acc5: 58.3333 (59.2991)  time: 0.0692  data: 0.0059  max mem: 2139
Test:  [160/313]  eta: 0:00:14  loss: 3.1722 (2.9355)  acc1: 37.5000 (43.9959)  acc5: 50.0000 (58.4239)  time: 0.0691  data: 0.0059  max mem: 2139
Test:  [170/313]  eta: 0:00:13  loss: 3.5460 (2.9627)  acc1: 31.2500 (43.5429)  acc5: 44.7917 (57.9557)  time: 0.0638  data: 0.0001  max mem: 2139
Test:  [180/313]  eta: 0:00:12  loss: 3.5460 (3.0020)  acc1: 30.2083 (42.8177)  acc5: 43.7500 (57.2053)  time: 0.0726  data: 0.0085  max mem: 2139
Test:  [190/313]  eta: 0:00:11  loss: 2.9559 (2.9624)  acc1: 42.7083 (43.5755)  acc5: 59.3750 (58.0279)  time: 0.0840  data: 0.0205  max mem: 2139
Test:  [200/313]  eta: 0:00:10  loss: 2.6096 (2.9508)  acc1: 50.0000 (43.7448)  acc5: 67.7083 (58.3696)  time: 0.1031  data: 0.0398  max mem: 2139
Test:  [210/313]  eta: 0:00:09  loss: 2.9174 (2.9489)  acc1: 44.7917 (43.7697)  acc5: 64.5833 (58.6493)  time: 0.0915  data: 0.0279  max mem: 2139
Test:  [220/313]  eta: 0:00:08  loss: 2.7688 (2.9144)  acc1: 45.8333 (44.3250)  acc5: 67.7083 (59.3656)  time: 0.0691  data: 0.0054  max mem: 2139
Test:  [230/313]  eta: 0:00:07  loss: 2.2580 (2.8856)  acc1: 48.9583 (44.6789)  acc5: 75.0000 (60.1100)  time: 0.0726  data: 0.0086  max mem: 2139
Test:  [240/313]  eta: 0:00:06  loss: 2.6006 (2.8842)  acc1: 46.8750 (44.8435)  acc5: 64.5833 (60.1703)  time: 0.0673  data: 0.0033  max mem: 2139
Test:  [250/313]  eta: 0:00:05  loss: 2.0693 (2.8433)  acc1: 57.2917 (45.6134)  acc5: 71.8750 (60.8732)  time: 0.0884  data: 0.0253  max mem: 2139
Test:  [260/313]  eta: 0:00:04  loss: 2.0693 (2.8270)  acc1: 58.3333 (45.9531)  acc5: 72.9167 (61.1590)  time: 0.0943  data: 0.0313  max mem: 2139
Test:  [270/313]  eta: 0:00:03  loss: 1.9966 (2.7846)  acc1: 61.4583 (46.7136)  acc5: 75.0000 (61.9542)  time: 0.0854  data: 0.0216  max mem: 2139
Test:  [280/313]  eta: 0:00:02  loss: 2.3175 (2.8087)  acc1: 52.0833 (46.2782)  acc5: 69.7917 (61.5436)  time: 0.0810  data: 0.0172  max mem: 2139
Test:  [290/313]  eta: 0:00:02  loss: 3.5639 (2.8379)  acc1: 32.2917 (45.7653)  acc5: 50.0000 (60.9715)  time: 0.0763  data: 0.0133  max mem: 2139
Test:  [300/313]  eta: 0:00:01  loss: 3.1664 (2.8427)  acc1: 31.2500 (45.6430)  acc5: 55.2083 (60.9635)  time: 0.0841  data: 0.0211  max mem: 2139
Test:  [310/313]  eta: 0:00:00  loss: 2.9979 (2.8633)  acc1: 39.5833 (45.3008)  acc5: 55.2083 (60.5506)  time: 0.0889  data: 0.0259  max mem: 2139
Test:  [312/313]  eta: 0:00:00  loss: 2.9979 (2.8708)  acc1: 39.5833 (45.1967)  acc5: 55.2083 (60.4333)  time: 0.0828  data: 0.0204  max mem: 2139
Test: Total time: 0:00:28 (0.0897 s / it)
* Acc@1 45.197 Acc@5 60.433 loss 2.871
Accuracy of the network on 30000 test images: 45.19667%
Not using distributed mode
Namespace(Decom=False, aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=64, bn=True, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/sliu/project_space/datasets/imagenet-r', data_set='IMNET', data_type='imagenet-r', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.1, enable_wandb=False, epochs=300, eval=True, eval_data_path=None, finetune='', fix=False, growth='random', head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, kernel_size=[7, 7, 7, 7, 100], layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=-1, log_dir=None, lr=0.004, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='SLaK_tiny', model_ema=False, model_ema_decay=0.9999, model_ema_eval=False, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_workers=10, only_L=False, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', parallel=True, pin_mem=True, project='SLaK', prune='magnitude', prune_rate=0.3, recount=1, redistribution='none', remode='pixel', reprob=0.25, resplit=False, resume='/gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_ConvNeXtTeacher_STconvnextv1_NKD/checkpoint-best.pth', save_ckpt=True, save_ckpt_freq=10, save_ckpt_num=3, seed=0, smoothing=0.1, sparse=False, sparse_init='snip', sparsity=0.4, start_epoch=0, train_interpolation='bicubic', update_freq=1, update_frequency=100, use_amp=False, verbose=False, wandb_ckpt=False, warmup_epochs=20, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, width_factor=1.0, world_size=1)
Transform = 
Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /home/sliu/project_space/datasets/imagenet-r
Number of the class = 1000
Mixup is activated!
Model = SLaK(
  (downsample_layers): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (1): LayerNorm()
    )
    (1): Sequential(
      (0): LayerNorm()
      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))
    )
    (2): Sequential(
      (0): LayerNorm()
      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))
    )
    (3): Sequential(
      (0): LayerNorm()
      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (stages): ModuleList(
    (0): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): Identity()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): DropPath()
      )
    )
    (1): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
    )
    (2): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (3): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (4): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (5): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (6): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (7): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (8): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
    )
    (3): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
number of params: 28595752
LR = 0.00400000
Batch size = 64
Update frequent = 1
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "downsample_layers.0.0.weight",
      "downsample_layers.1.1.weight",
      "downsample_layers.2.1.weight",
      "downsample_layers.3.1.weight",
      "stages.0.0.large_kernel.lkb_origin.conv.weight",
      "stages.0.0.pwconv1.weight",
      "stages.0.0.pwconv2.weight",
      "stages.0.1.large_kernel.lkb_origin.conv.weight",
      "stages.0.1.pwconv1.weight",
      "stages.0.1.pwconv2.weight",
      "stages.0.2.large_kernel.lkb_origin.conv.weight",
      "stages.0.2.pwconv1.weight",
      "stages.0.2.pwconv2.weight",
      "stages.1.0.large_kernel.lkb_origin.conv.weight",
      "stages.1.0.pwconv1.weight",
      "stages.1.0.pwconv2.weight",
      "stages.1.1.large_kernel.lkb_origin.conv.weight",
      "stages.1.1.pwconv1.weight",
      "stages.1.1.pwconv2.weight",
      "stages.1.2.large_kernel.lkb_origin.conv.weight",
      "stages.1.2.pwconv1.weight",
      "stages.1.2.pwconv2.weight",
      "stages.2.0.large_kernel.lkb_origin.conv.weight",
      "stages.2.0.pwconv1.weight",
      "stages.2.0.pwconv2.weight",
      "stages.2.1.large_kernel.lkb_origin.conv.weight",
      "stages.2.1.pwconv1.weight",
      "stages.2.1.pwconv2.weight",
      "stages.2.2.large_kernel.lkb_origin.conv.weight",
      "stages.2.2.pwconv1.weight",
      "stages.2.2.pwconv2.weight",
      "stages.2.3.large_kernel.lkb_origin.conv.weight",
      "stages.2.3.pwconv1.weight",
      "stages.2.3.pwconv2.weight",
      "stages.2.4.large_kernel.lkb_origin.conv.weight",
      "stages.2.4.pwconv1.weight",
      "stages.2.4.pwconv2.weight",
      "stages.2.5.large_kernel.lkb_origin.conv.weight",
      "stages.2.5.pwconv1.weight",
      "stages.2.5.pwconv2.weight",
      "stages.2.6.large_kernel.lkb_origin.conv.weight",
      "stages.2.6.pwconv1.weight",
      "stages.2.6.pwconv2.weight",
      "stages.2.7.large_kernel.lkb_origin.conv.weight",
      "stages.2.7.pwconv1.weight",
      "stages.2.7.pwconv2.weight",
      "stages.2.8.large_kernel.lkb_origin.conv.weight",
      "stages.2.8.pwconv1.weight",
      "stages.2.8.pwconv2.weight",
      "stages.3.0.large_kernel.lkb_origin.conv.weight",
      "stages.3.0.pwconv1.weight",
      "stages.3.0.pwconv2.weight",
      "stages.3.1.large_kernel.lkb_origin.conv.weight",
      "stages.3.1.pwconv1.weight",
      "stages.3.1.pwconv2.weight",
      "stages.3.2.large_kernel.lkb_origin.conv.weight",
      "stages.3.2.pwconv1.weight",
      "stages.3.2.pwconv2.weight",
      "head.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "downsample_layers.0.0.bias",
      "downsample_layers.0.1.weight",
      "downsample_layers.0.1.bias",
      "downsample_layers.1.0.weight",
      "downsample_layers.1.0.bias",
      "downsample_layers.1.1.bias",
      "downsample_layers.2.0.weight",
      "downsample_layers.2.0.bias",
      "downsample_layers.2.1.bias",
      "downsample_layers.3.0.weight",
      "downsample_layers.3.0.bias",
      "downsample_layers.3.1.bias",
      "stages.0.0.gamma",
      "stages.0.0.large_kernel.lkb_origin.bn.weight",
      "stages.0.0.large_kernel.lkb_origin.bn.bias",
      "stages.0.0.norm.weight",
      "stages.0.0.norm.bias",
      "stages.0.0.pwconv1.bias",
      "stages.0.0.pwconv2.bias",
      "stages.0.1.gamma",
      "stages.0.1.large_kernel.lkb_origin.bn.weight",
      "stages.0.1.large_kernel.lkb_origin.bn.bias",
      "stages.0.1.norm.weight",
      "stages.0.1.norm.bias",
      "stages.0.1.pwconv1.bias",
      "stages.0.1.pwconv2.bias",
      "stages.0.2.gamma",
      "stages.0.2.large_kernel.lkb_origin.bn.weight",
      "stages.0.2.large_kernel.lkb_origin.bn.bias",
      "stages.0.2.norm.weight",
      "stages.0.2.norm.bias",
      "stages.0.2.pwconv1.bias",
      "stages.0.2.pwconv2.bias",
      "stages.1.0.gamma",
      "stages.1.0.large_kernel.lkb_origin.bn.weight",
      "stages.1.0.large_kernel.lkb_origin.bn.bias",
      "stages.1.0.norm.weight",
      "stages.1.0.norm.bias",
      "stages.1.0.pwconv1.bias",
      "stages.1.0.pwconv2.bias",
      "stages.1.1.gamma",
      "stages.1.1.large_kernel.lkb_origin.bn.weight",
      "stages.1.1.large_kernel.lkb_origin.bn.bias",
      "stages.1.1.norm.weight",
      "stages.1.1.norm.bias",
      "stages.1.1.pwconv1.bias",
      "stages.1.1.pwconv2.bias",
      "stages.1.2.gamma",
      "stages.1.2.large_kernel.lkb_origin.bn.weight",
      "stages.1.2.large_kernel.lkb_origin.bn.bias",
      "stages.1.2.norm.weight",
      "stages.1.2.norm.bias",
      "stages.1.2.pwconv1.bias",
      "stages.1.2.pwconv2.bias",
      "stages.2.0.gamma",
      "stages.2.0.large_kernel.lkb_origin.bn.weight",
      "stages.2.0.large_kernel.lkb_origin.bn.bias",
      "stages.2.0.norm.weight",
      "stages.2.0.norm.bias",
      "stages.2.0.pwconv1.bias",
      "stages.2.0.pwconv2.bias",
      "stages.2.1.gamma",
      "stages.2.1.large_kernel.lkb_origin.bn.weight",
      "stages.2.1.large_kernel.lkb_origin.bn.bias",
      "stages.2.1.norm.weight",
      "stages.2.1.norm.bias",
      "stages.2.1.pwconv1.bias",
      "stages.2.1.pwconv2.bias",
      "stages.2.2.gamma",
      "stages.2.2.large_kernel.lkb_origin.bn.weight",
      "stages.2.2.large_kernel.lkb_origin.bn.bias",
      "stages.2.2.norm.weight",
      "stages.2.2.norm.bias",
      "stages.2.2.pwconv1.bias",
      "stages.2.2.pwconv2.bias",
      "stages.2.3.gamma",
      "stages.2.3.large_kernel.lkb_origin.bn.weight",
      "stages.2.3.large_kernel.lkb_origin.bn.bias",
      "stages.2.3.norm.weight",
      "stages.2.3.norm.bias",
      "stages.2.3.pwconv1.bias",
      "stages.2.3.pwconv2.bias",
      "stages.2.4.gamma",
      "stages.2.4.large_kernel.lkb_origin.bn.weight",
      "stages.2.4.large_kernel.lkb_origin.bn.bias",
      "stages.2.4.norm.weight",
      "stages.2.4.norm.bias",
      "stages.2.4.pwconv1.bias",
      "stages.2.4.pwconv2.bias",
      "stages.2.5.gamma",
      "stages.2.5.large_kernel.lkb_origin.bn.weight",
      "stages.2.5.large_kernel.lkb_origin.bn.bias",
      "stages.2.5.norm.weight",
      "stages.2.5.norm.bias",
      "stages.2.5.pwconv1.bias",
      "stages.2.5.pwconv2.bias",
      "stages.2.6.gamma",
      "stages.2.6.large_kernel.lkb_origin.bn.weight",
      "stages.2.6.large_kernel.lkb_origin.bn.bias",
      "stages.2.6.norm.weight",
      "stages.2.6.norm.bias",
      "stages.2.6.pwconv1.bias",
      "stages.2.6.pwconv2.bias",
      "stages.2.7.gamma",
      "stages.2.7.large_kernel.lkb_origin.bn.weight",
      "stages.2.7.large_kernel.lkb_origin.bn.bias",
      "stages.2.7.norm.weight",
      "stages.2.7.norm.bias",
      "stages.2.7.pwconv1.bias",
      "stages.2.7.pwconv2.bias",
      "stages.2.8.gamma",
      "stages.2.8.large_kernel.lkb_origin.bn.weight",
      "stages.2.8.large_kernel.lkb_origin.bn.bias",
      "stages.2.8.norm.weight",
      "stages.2.8.norm.bias",
      "stages.2.8.pwconv1.bias",
      "stages.2.8.pwconv2.bias",
      "stages.3.0.gamma",
      "stages.3.0.large_kernel.lkb_origin.bn.weight",
      "stages.3.0.large_kernel.lkb_origin.bn.bias",
      "stages.3.0.norm.weight",
      "stages.3.0.norm.bias",
      "stages.3.0.pwconv1.bias",
      "stages.3.0.pwconv2.bias",
      "stages.3.1.gamma",
      "stages.3.1.large_kernel.lkb_origin.bn.weight",
      "stages.3.1.large_kernel.lkb_origin.bn.bias",
      "stages.3.1.norm.weight",
      "stages.3.1.norm.bias",
      "stages.3.1.pwconv1.bias",
      "stages.3.1.pwconv2.bias",
      "stages.3.2.gamma",
      "stages.3.2.large_kernel.lkb_origin.bn.weight",
      "stages.3.2.large_kernel.lkb_origin.bn.bias",
      "stages.3.2.norm.weight",
      "stages.3.2.norm.bias",
      "stages.3.2.pwconv1.bias",
      "stages.3.2.pwconv2.bias",
      "norm.weight",
      "norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
criterion = SoftTargetCrossEntropy()
dict_keys(['model', 'optimizer', 'epoch', 'scaler', 'args', 'model_ema'])
Resume checkpoint /gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_ConvNeXtTeacher_STconvnextv1_NKD/checkpoint-best.pth
With optim & sched!
downsample_layers.0.0.weight density is 1.0
downsample_layers.0.0.bias density is 1.0
downsample_layers.0.1.weight density is 1.0
downsample_layers.0.1.bias density is 1.0
downsample_layers.1.0.weight density is 1.0
downsample_layers.1.0.bias density is 1.0
downsample_layers.1.1.weight density is 1.0
downsample_layers.1.1.bias density is 1.0
downsample_layers.2.0.weight density is 1.0
downsample_layers.2.0.bias density is 1.0
downsample_layers.2.1.weight density is 1.0
downsample_layers.2.1.bias density is 1.0
downsample_layers.3.0.weight density is 1.0
downsample_layers.3.0.bias density is 1.0
downsample_layers.3.1.weight density is 1.0
downsample_layers.3.1.bias density is 1.0
stages.0.0.gamma density is 1.0
stages.0.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.0.norm.weight density is 1.0
stages.0.0.norm.bias density is 1.0
stages.0.0.pwconv1.weight density is 1.0
stages.0.0.pwconv1.bias density is 1.0
stages.0.0.pwconv2.weight density is 1.0
stages.0.0.pwconv2.bias density is 1.0
stages.0.1.gamma density is 1.0
stages.0.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.1.norm.weight density is 1.0
stages.0.1.norm.bias density is 1.0
stages.0.1.pwconv1.weight density is 1.0
stages.0.1.pwconv1.bias density is 1.0
stages.0.1.pwconv2.weight density is 1.0
stages.0.1.pwconv2.bias density is 1.0
stages.0.2.gamma density is 1.0
stages.0.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.2.norm.weight density is 1.0
stages.0.2.norm.bias density is 1.0
stages.0.2.pwconv1.weight density is 1.0
stages.0.2.pwconv1.bias density is 1.0
stages.0.2.pwconv2.weight density is 1.0
stages.0.2.pwconv2.bias density is 1.0
stages.1.0.gamma density is 1.0
stages.1.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.0.norm.weight density is 1.0
stages.1.0.norm.bias density is 1.0
stages.1.0.pwconv1.weight density is 1.0
stages.1.0.pwconv1.bias density is 1.0
stages.1.0.pwconv2.weight density is 1.0
stages.1.0.pwconv2.bias density is 1.0
stages.1.1.gamma density is 1.0
stages.1.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.1.norm.weight density is 1.0
stages.1.1.norm.bias density is 1.0
stages.1.1.pwconv1.weight density is 1.0
stages.1.1.pwconv1.bias density is 1.0
stages.1.1.pwconv2.weight density is 1.0
stages.1.1.pwconv2.bias density is 1.0
stages.1.2.gamma density is 1.0
stages.1.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.2.norm.weight density is 1.0
stages.1.2.norm.bias density is 1.0
stages.1.2.pwconv1.weight density is 1.0
stages.1.2.pwconv1.bias density is 1.0
stages.1.2.pwconv2.weight density is 1.0
stages.1.2.pwconv2.bias density is 1.0
stages.2.0.gamma density is 1.0
stages.2.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.0.norm.weight density is 1.0
stages.2.0.norm.bias density is 1.0
stages.2.0.pwconv1.weight density is 1.0
stages.2.0.pwconv1.bias density is 1.0
stages.2.0.pwconv2.weight density is 1.0
stages.2.0.pwconv2.bias density is 1.0
stages.2.1.gamma density is 1.0
stages.2.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.1.norm.weight density is 1.0
stages.2.1.norm.bias density is 1.0
stages.2.1.pwconv1.weight density is 1.0
stages.2.1.pwconv1.bias density is 1.0
stages.2.1.pwconv2.weight density is 1.0
stages.2.1.pwconv2.bias density is 1.0
stages.2.2.gamma density is 1.0
stages.2.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.2.norm.weight density is 1.0
stages.2.2.norm.bias density is 1.0
stages.2.2.pwconv1.weight density is 1.0
stages.2.2.pwconv1.bias density is 1.0
stages.2.2.pwconv2.weight density is 1.0
stages.2.2.pwconv2.bias density is 1.0
stages.2.3.gamma density is 1.0
stages.2.3.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.3.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.3.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.3.norm.weight density is 1.0
stages.2.3.norm.bias density is 1.0
stages.2.3.pwconv1.weight density is 1.0
stages.2.3.pwconv1.bias density is 1.0
stages.2.3.pwconv2.weight density is 1.0
stages.2.3.pwconv2.bias density is 1.0
stages.2.4.gamma density is 1.0
stages.2.4.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.4.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.4.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.4.norm.weight density is 1.0
stages.2.4.norm.bias density is 1.0
stages.2.4.pwconv1.weight density is 1.0
stages.2.4.pwconv1.bias density is 1.0
stages.2.4.pwconv2.weight density is 1.0
stages.2.4.pwconv2.bias density is 1.0
stages.2.5.gamma density is 1.0
stages.2.5.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.5.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.5.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.5.norm.weight density is 1.0
stages.2.5.norm.bias density is 1.0
stages.2.5.pwconv1.weight density is 1.0
stages.2.5.pwconv1.bias density is 1.0
stages.2.5.pwconv2.weight density is 1.0
stages.2.5.pwconv2.bias density is 1.0
stages.2.6.gamma density is 1.0
stages.2.6.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.6.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.6.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.6.norm.weight density is 1.0
stages.2.6.norm.bias density is 1.0
stages.2.6.pwconv1.weight density is 1.0
stages.2.6.pwconv1.bias density is 1.0
stages.2.6.pwconv2.weight density is 1.0
stages.2.6.pwconv2.bias density is 1.0
stages.2.7.gamma density is 1.0
stages.2.7.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.7.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.7.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.7.norm.weight density is 1.0
stages.2.7.norm.bias density is 1.0
stages.2.7.pwconv1.weight density is 1.0
stages.2.7.pwconv1.bias density is 1.0
stages.2.7.pwconv2.weight density is 1.0
stages.2.7.pwconv2.bias density is 1.0
stages.2.8.gamma density is 1.0
stages.2.8.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.8.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.8.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.8.norm.weight density is 1.0
stages.2.8.norm.bias density is 1.0
stages.2.8.pwconv1.weight density is 1.0
stages.2.8.pwconv1.bias density is 1.0
stages.2.8.pwconv2.weight density is 1.0
stages.2.8.pwconv2.bias density is 1.0
stages.3.0.gamma density is 1.0
stages.3.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.0.norm.weight density is 1.0
stages.3.0.norm.bias density is 1.0
stages.3.0.pwconv1.weight density is 1.0
stages.3.0.pwconv1.bias density is 1.0
stages.3.0.pwconv2.weight density is 1.0
stages.3.0.pwconv2.bias density is 1.0
stages.3.1.gamma density is 1.0
stages.3.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.1.norm.weight density is 1.0
stages.3.1.norm.bias density is 1.0
stages.3.1.pwconv1.weight density is 1.0
stages.3.1.pwconv1.bias density is 1.0
stages.3.1.pwconv2.weight density is 1.0
stages.3.1.pwconv2.bias density is 1.0
stages.3.2.gamma density is 1.0
stages.3.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.2.norm.weight density is 1.0
stages.3.2.norm.bias density is 1.0
stages.3.2.pwconv1.weight density is 1.0
stages.3.2.pwconv1.bias density is 1.0
stages.3.2.pwconv2.weight density is 1.0
stages.3.2.pwconv2.bias density is 1.0
norm.weight density is 1.0
norm.bias density is 1.0
head.weight density is 1.0
head.bias density is 1.0
Test:  [  0/313]  eta: 0:16:44  loss: 3.9790 (3.9790)  acc1: 25.0000 (25.0000)  acc5: 42.7083 (42.7083)  time: 3.2099  data: 2.0468  max mem: 2139
Test:  [ 10/313]  eta: 0:01:46  loss: 3.4847 (3.2456)  acc1: 34.3750 (37.3106)  acc5: 51.0417 (54.1667)  time: 0.3499  data: 0.1862  max mem: 2139
Test:  [ 20/313]  eta: 0:01:02  loss: 3.1591 (3.0805)  acc1: 34.3750 (41.0218)  acc5: 52.0833 (56.5476)  time: 0.0635  data: 0.0001  max mem: 2139
Test:  [ 30/313]  eta: 0:00:48  loss: 2.3904 (2.8261)  acc1: 52.0833 (45.7661)  acc5: 69.7917 (60.9879)  time: 0.0710  data: 0.0078  max mem: 2139
Test:  [ 40/313]  eta: 0:00:40  loss: 2.1289 (2.7010)  acc1: 56.2500 (48.2215)  acc5: 71.8750 (62.8811)  time: 0.0771  data: 0.0138  max mem: 2139
Test:  [ 50/313]  eta: 0:00:36  loss: 3.0332 (2.9100)  acc1: 40.6250 (44.8938)  acc5: 56.2500 (59.1912)  time: 0.0873  data: 0.0243  max mem: 2139
Test:  [ 60/313]  eta: 0:00:33  loss: 3.7267 (3.0233)  acc1: 26.0417 (42.3839)  acc5: 42.7083 (57.1209)  time: 0.1073  data: 0.0440  max mem: 2139
Test:  [ 70/313]  eta: 0:00:31  loss: 3.3774 (2.9728)  acc1: 31.2500 (43.3685)  acc5: 51.0417 (57.9079)  time: 0.1077  data: 0.0445  max mem: 2139
Test:  [ 80/313]  eta: 0:00:30  loss: 1.9226 (2.8235)  acc1: 57.2917 (45.5247)  acc5: 77.0833 (60.6739)  time: 0.1203  data: 0.0573  max mem: 2139
Test:  [ 90/313]  eta: 0:00:27  loss: 1.5370 (2.6982)  acc1: 60.4167 (47.6190)  acc5: 82.2917 (62.8892)  time: 0.1113  data: 0.0481  max mem: 2139
Test:  [100/313]  eta: 0:00:25  loss: 1.6778 (2.6213)  acc1: 61.4583 (49.0305)  acc5: 79.1667 (64.2224)  time: 0.0826  data: 0.0193  max mem: 2139
Test:  [110/313]  eta: 0:00:24  loss: 2.0890 (2.6253)  acc1: 54.1667 (48.9865)  acc5: 75.0000 (64.2080)  time: 0.1038  data: 0.0408  max mem: 2139
Test:  [120/313]  eta: 0:00:23  loss: 2.8175 (2.6643)  acc1: 43.7500 (48.3299)  acc5: 62.5000 (63.6708)  time: 0.1143  data: 0.0510  max mem: 2139
Test:  [130/313]  eta: 0:00:21  loss: 3.3092 (2.7594)  acc1: 34.3750 (46.7080)  acc5: 51.0417 (61.8877)  time: 0.1126  data: 0.0493  max mem: 2139
Test:  [140/313]  eta: 0:00:20  loss: 3.1685 (2.7799)  acc1: 35.4167 (46.2175)  acc5: 52.0833 (61.6504)  time: 0.0925  data: 0.0293  max mem: 2139
Test:  [150/313]  eta: 0:00:18  loss: 2.8436 (2.7719)  acc1: 42.7083 (46.4887)  acc5: 61.4583 (61.6860)  time: 0.0764  data: 0.0127  max mem: 2139
Test:  [160/313]  eta: 0:00:17  loss: 2.9386 (2.8121)  acc1: 40.6250 (45.7622)  acc5: 55.2083 (60.9407)  time: 0.0891  data: 0.0255  max mem: 2139
Test:  [170/313]  eta: 0:00:16  loss: 3.4165 (2.8353)  acc1: 32.2917 (45.3643)  acc5: 50.0000 (60.6238)  time: 0.1004  data: 0.0374  max mem: 2139
Test:  [180/313]  eta: 0:00:14  loss: 3.4165 (2.8762)  acc1: 31.2500 (44.5787)  acc5: 50.0000 (59.9217)  time: 0.1106  data: 0.0471  max mem: 2139
Test:  [190/313]  eta: 0:00:13  loss: 2.9264 (2.8420)  acc1: 42.7083 (45.1789)  acc5: 59.3750 (60.6130)  time: 0.1088  data: 0.0454  max mem: 2139
Test:  [200/313]  eta: 0:00:13  loss: 2.4941 (2.8303)  acc1: 47.9167 (45.4084)  acc5: 70.8333 (61.0541)  time: 0.1395  data: 0.0766  max mem: 2139
Test:  [210/313]  eta: 0:00:11  loss: 2.8771 (2.8327)  acc1: 43.7500 (45.3495)  acc5: 64.5833 (61.1868)  time: 0.1266  data: 0.0633  max mem: 2139
Test:  [220/313]  eta: 0:00:10  loss: 2.6944 (2.8001)  acc1: 45.8333 (45.9135)  acc5: 65.6250 (61.8024)  time: 0.0733  data: 0.0100  max mem: 2139
Test:  [230/313]  eta: 0:00:09  loss: 2.1850 (2.7736)  acc1: 55.2083 (46.3249)  acc5: 76.0417 (62.4098)  time: 0.0708  data: 0.0077  max mem: 2139
Test:  [240/313]  eta: 0:00:07  loss: 2.3206 (2.7757)  acc1: 51.0417 (46.3520)  acc5: 66.6667 (62.4827)  time: 0.0706  data: 0.0073  max mem: 2139
Test:  [250/313]  eta: 0:00:06  loss: 2.1210 (2.7342)  acc1: 57.2917 (47.1199)  acc5: 73.9583 (63.2138)  time: 0.0667  data: 0.0035  max mem: 2139
Test:  [260/313]  eta: 0:00:05  loss: 1.8417 (2.7218)  acc1: 61.4583 (47.3819)  acc5: 79.1667 (63.3940)  time: 0.0811  data: 0.0179  max mem: 2139
Test:  [270/313]  eta: 0:00:04  loss: 1.8473 (2.6818)  acc1: 65.6250 (48.1396)  acc5: 80.2083 (64.1413)  time: 0.0806  data: 0.0174  max mem: 2139
Test:  [280/313]  eta: 0:00:03  loss: 2.5580 (2.7104)  acc1: 51.0417 (47.6275)  acc5: 67.7083 (63.6195)  time: 0.0717  data: 0.0085  max mem: 2139
Test:  [290/313]  eta: 0:00:02  loss: 3.6725 (2.7413)  acc1: 31.2500 (47.1041)  acc5: 48.9583 (62.9832)  time: 0.0696  data: 0.0056  max mem: 2139
Test:  [300/313]  eta: 0:00:01  loss: 3.1243 (2.7442)  acc1: 33.3333 (46.9650)  acc5: 57.2917 (63.0191)  time: 0.0639  data: 0.0001  max mem: 2139
Test:  [310/313]  eta: 0:00:00  loss: 3.0190 (2.7669)  acc1: 39.5833 (46.5903)  acc5: 57.2917 (62.5904)  time: 0.0638  data: 0.0008  max mem: 2139
Test:  [312/313]  eta: 0:00:00  loss: 3.0190 (2.7735)  acc1: 39.5833 (46.4900)  acc5: 57.2917 (62.4933)  time: 0.0632  data: 0.0008  max mem: 2139
Test: Total time: 0:00:31 (0.0997 s / it)
* Acc@1 46.490 Acc@5 62.493 loss 2.774
Accuracy of the network on 30000 test images: 46.49000%
Not using distributed mode
Namespace(Decom=False, aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=64, bn=True, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/sliu/project_space/datasets/imagenet-r', data_set='IMNET', data_type='imagenet-r', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.1, enable_wandb=False, epochs=300, eval=True, eval_data_path=None, finetune='', fix=False, growth='random', head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, kernel_size=[7, 7, 7, 7, 100], layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=-1, log_dir=None, lr=0.004, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='SLaK_tiny', model_ema=False, model_ema_decay=0.9999, model_ema_eval=False, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_workers=10, only_L=False, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', parallel=True, pin_mem=True, project='SLaK', prune='magnitude', prune_rate=0.3, recount=1, redistribution='none', remode='pixel', reprob=0.25, resplit=False, resume='/gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_SlakT_Teacher_STConvNext_NKD/checkpoint-best.pth', save_ckpt=True, save_ckpt_freq=10, save_ckpt_num=3, seed=0, smoothing=0.1, sparse=False, sparse_init='snip', sparsity=0.4, start_epoch=0, train_interpolation='bicubic', update_freq=1, update_frequency=100, use_amp=False, verbose=False, wandb_ckpt=False, warmup_epochs=20, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, width_factor=1.0, world_size=1)
Transform = 
Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /home/sliu/project_space/datasets/imagenet-r
Number of the class = 1000
Mixup is activated!
Model = SLaK(
  (downsample_layers): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (1): LayerNorm()
    )
    (1): Sequential(
      (0): LayerNorm()
      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))
    )
    (2): Sequential(
      (0): LayerNorm()
      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))
    )
    (3): Sequential(
      (0): LayerNorm()
      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (stages): ModuleList(
    (0): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): Identity()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): DropPath()
      )
    )
    (1): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
    )
    (2): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (3): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (4): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (5): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (6): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (7): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (8): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
    )
    (3): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
number of params: 28595752
LR = 0.00400000
Batch size = 64
Update frequent = 1
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "downsample_layers.0.0.weight",
      "downsample_layers.1.1.weight",
      "downsample_layers.2.1.weight",
      "downsample_layers.3.1.weight",
      "stages.0.0.large_kernel.lkb_origin.conv.weight",
      "stages.0.0.pwconv1.weight",
      "stages.0.0.pwconv2.weight",
      "stages.0.1.large_kernel.lkb_origin.conv.weight",
      "stages.0.1.pwconv1.weight",
      "stages.0.1.pwconv2.weight",
      "stages.0.2.large_kernel.lkb_origin.conv.weight",
      "stages.0.2.pwconv1.weight",
      "stages.0.2.pwconv2.weight",
      "stages.1.0.large_kernel.lkb_origin.conv.weight",
      "stages.1.0.pwconv1.weight",
      "stages.1.0.pwconv2.weight",
      "stages.1.1.large_kernel.lkb_origin.conv.weight",
      "stages.1.1.pwconv1.weight",
      "stages.1.1.pwconv2.weight",
      "stages.1.2.large_kernel.lkb_origin.conv.weight",
      "stages.1.2.pwconv1.weight",
      "stages.1.2.pwconv2.weight",
      "stages.2.0.large_kernel.lkb_origin.conv.weight",
      "stages.2.0.pwconv1.weight",
      "stages.2.0.pwconv2.weight",
      "stages.2.1.large_kernel.lkb_origin.conv.weight",
      "stages.2.1.pwconv1.weight",
      "stages.2.1.pwconv2.weight",
      "stages.2.2.large_kernel.lkb_origin.conv.weight",
      "stages.2.2.pwconv1.weight",
      "stages.2.2.pwconv2.weight",
      "stages.2.3.large_kernel.lkb_origin.conv.weight",
      "stages.2.3.pwconv1.weight",
      "stages.2.3.pwconv2.weight",
      "stages.2.4.large_kernel.lkb_origin.conv.weight",
      "stages.2.4.pwconv1.weight",
      "stages.2.4.pwconv2.weight",
      "stages.2.5.large_kernel.lkb_origin.conv.weight",
      "stages.2.5.pwconv1.weight",
      "stages.2.5.pwconv2.weight",
      "stages.2.6.large_kernel.lkb_origin.conv.weight",
      "stages.2.6.pwconv1.weight",
      "stages.2.6.pwconv2.weight",
      "stages.2.7.large_kernel.lkb_origin.conv.weight",
      "stages.2.7.pwconv1.weight",
      "stages.2.7.pwconv2.weight",
      "stages.2.8.large_kernel.lkb_origin.conv.weight",
      "stages.2.8.pwconv1.weight",
      "stages.2.8.pwconv2.weight",
      "stages.3.0.large_kernel.lkb_origin.conv.weight",
      "stages.3.0.pwconv1.weight",
      "stages.3.0.pwconv2.weight",
      "stages.3.1.large_kernel.lkb_origin.conv.weight",
      "stages.3.1.pwconv1.weight",
      "stages.3.1.pwconv2.weight",
      "stages.3.2.large_kernel.lkb_origin.conv.weight",
      "stages.3.2.pwconv1.weight",
      "stages.3.2.pwconv2.weight",
      "head.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "downsample_layers.0.0.bias",
      "downsample_layers.0.1.weight",
      "downsample_layers.0.1.bias",
      "downsample_layers.1.0.weight",
      "downsample_layers.1.0.bias",
      "downsample_layers.1.1.bias",
      "downsample_layers.2.0.weight",
      "downsample_layers.2.0.bias",
      "downsample_layers.2.1.bias",
      "downsample_layers.3.0.weight",
      "downsample_layers.3.0.bias",
      "downsample_layers.3.1.bias",
      "stages.0.0.gamma",
      "stages.0.0.large_kernel.lkb_origin.bn.weight",
      "stages.0.0.large_kernel.lkb_origin.bn.bias",
      "stages.0.0.norm.weight",
      "stages.0.0.norm.bias",
      "stages.0.0.pwconv1.bias",
      "stages.0.0.pwconv2.bias",
      "stages.0.1.gamma",
      "stages.0.1.large_kernel.lkb_origin.bn.weight",
      "stages.0.1.large_kernel.lkb_origin.bn.bias",
      "stages.0.1.norm.weight",
      "stages.0.1.norm.bias",
      "stages.0.1.pwconv1.bias",
      "stages.0.1.pwconv2.bias",
      "stages.0.2.gamma",
      "stages.0.2.large_kernel.lkb_origin.bn.weight",
      "stages.0.2.large_kernel.lkb_origin.bn.bias",
      "stages.0.2.norm.weight",
      "stages.0.2.norm.bias",
      "stages.0.2.pwconv1.bias",
      "stages.0.2.pwconv2.bias",
      "stages.1.0.gamma",
      "stages.1.0.large_kernel.lkb_origin.bn.weight",
      "stages.1.0.large_kernel.lkb_origin.bn.bias",
      "stages.1.0.norm.weight",
      "stages.1.0.norm.bias",
      "stages.1.0.pwconv1.bias",
      "stages.1.0.pwconv2.bias",
      "stages.1.1.gamma",
      "stages.1.1.large_kernel.lkb_origin.bn.weight",
      "stages.1.1.large_kernel.lkb_origin.bn.bias",
      "stages.1.1.norm.weight",
      "stages.1.1.norm.bias",
      "stages.1.1.pwconv1.bias",
      "stages.1.1.pwconv2.bias",
      "stages.1.2.gamma",
      "stages.1.2.large_kernel.lkb_origin.bn.weight",
      "stages.1.2.large_kernel.lkb_origin.bn.bias",
      "stages.1.2.norm.weight",
      "stages.1.2.norm.bias",
      "stages.1.2.pwconv1.bias",
      "stages.1.2.pwconv2.bias",
      "stages.2.0.gamma",
      "stages.2.0.large_kernel.lkb_origin.bn.weight",
      "stages.2.0.large_kernel.lkb_origin.bn.bias",
      "stages.2.0.norm.weight",
      "stages.2.0.norm.bias",
      "stages.2.0.pwconv1.bias",
      "stages.2.0.pwconv2.bias",
      "stages.2.1.gamma",
      "stages.2.1.large_kernel.lkb_origin.bn.weight",
      "stages.2.1.large_kernel.lkb_origin.bn.bias",
      "stages.2.1.norm.weight",
      "stages.2.1.norm.bias",
      "stages.2.1.pwconv1.bias",
      "stages.2.1.pwconv2.bias",
      "stages.2.2.gamma",
      "stages.2.2.large_kernel.lkb_origin.bn.weight",
      "stages.2.2.large_kernel.lkb_origin.bn.bias",
      "stages.2.2.norm.weight",
      "stages.2.2.norm.bias",
      "stages.2.2.pwconv1.bias",
      "stages.2.2.pwconv2.bias",
      "stages.2.3.gamma",
      "stages.2.3.large_kernel.lkb_origin.bn.weight",
      "stages.2.3.large_kernel.lkb_origin.bn.bias",
      "stages.2.3.norm.weight",
      "stages.2.3.norm.bias",
      "stages.2.3.pwconv1.bias",
      "stages.2.3.pwconv2.bias",
      "stages.2.4.gamma",
      "stages.2.4.large_kernel.lkb_origin.bn.weight",
      "stages.2.4.large_kernel.lkb_origin.bn.bias",
      "stages.2.4.norm.weight",
      "stages.2.4.norm.bias",
      "stages.2.4.pwconv1.bias",
      "stages.2.4.pwconv2.bias",
      "stages.2.5.gamma",
      "stages.2.5.large_kernel.lkb_origin.bn.weight",
      "stages.2.5.large_kernel.lkb_origin.bn.bias",
      "stages.2.5.norm.weight",
      "stages.2.5.norm.bias",
      "stages.2.5.pwconv1.bias",
      "stages.2.5.pwconv2.bias",
      "stages.2.6.gamma",
      "stages.2.6.large_kernel.lkb_origin.bn.weight",
      "stages.2.6.large_kernel.lkb_origin.bn.bias",
      "stages.2.6.norm.weight",
      "stages.2.6.norm.bias",
      "stages.2.6.pwconv1.bias",
      "stages.2.6.pwconv2.bias",
      "stages.2.7.gamma",
      "stages.2.7.large_kernel.lkb_origin.bn.weight",
      "stages.2.7.large_kernel.lkb_origin.bn.bias",
      "stages.2.7.norm.weight",
      "stages.2.7.norm.bias",
      "stages.2.7.pwconv1.bias",
      "stages.2.7.pwconv2.bias",
      "stages.2.8.gamma",
      "stages.2.8.large_kernel.lkb_origin.bn.weight",
      "stages.2.8.large_kernel.lkb_origin.bn.bias",
      "stages.2.8.norm.weight",
      "stages.2.8.norm.bias",
      "stages.2.8.pwconv1.bias",
      "stages.2.8.pwconv2.bias",
      "stages.3.0.gamma",
      "stages.3.0.large_kernel.lkb_origin.bn.weight",
      "stages.3.0.large_kernel.lkb_origin.bn.bias",
      "stages.3.0.norm.weight",
      "stages.3.0.norm.bias",
      "stages.3.0.pwconv1.bias",
      "stages.3.0.pwconv2.bias",
      "stages.3.1.gamma",
      "stages.3.1.large_kernel.lkb_origin.bn.weight",
      "stages.3.1.large_kernel.lkb_origin.bn.bias",
      "stages.3.1.norm.weight",
      "stages.3.1.norm.bias",
      "stages.3.1.pwconv1.bias",
      "stages.3.1.pwconv2.bias",
      "stages.3.2.gamma",
      "stages.3.2.large_kernel.lkb_origin.bn.weight",
      "stages.3.2.large_kernel.lkb_origin.bn.bias",
      "stages.3.2.norm.weight",
      "stages.3.2.norm.bias",
      "stages.3.2.pwconv1.bias",
      "stages.3.2.pwconv2.bias",
      "norm.weight",
      "norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
criterion = SoftTargetCrossEntropy()
dict_keys(['model', 'optimizer', 'epoch', 'scaler', 'args', 'model_ema'])
Resume checkpoint /gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_SlakT_Teacher_STConvNext_NKD/checkpoint-best.pth
With optim & sched!
downsample_layers.0.0.weight density is 1.0
downsample_layers.0.0.bias density is 1.0
downsample_layers.0.1.weight density is 1.0
downsample_layers.0.1.bias density is 1.0
downsample_layers.1.0.weight density is 1.0
downsample_layers.1.0.bias density is 1.0
downsample_layers.1.1.weight density is 1.0
downsample_layers.1.1.bias density is 1.0
downsample_layers.2.0.weight density is 1.0
downsample_layers.2.0.bias density is 1.0
downsample_layers.2.1.weight density is 1.0
downsample_layers.2.1.bias density is 1.0
downsample_layers.3.0.weight density is 1.0
downsample_layers.3.0.bias density is 1.0
downsample_layers.3.1.weight density is 1.0
downsample_layers.3.1.bias density is 1.0
stages.0.0.gamma density is 1.0
stages.0.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.0.norm.weight density is 1.0
stages.0.0.norm.bias density is 1.0
stages.0.0.pwconv1.weight density is 1.0
stages.0.0.pwconv1.bias density is 1.0
stages.0.0.pwconv2.weight density is 1.0
stages.0.0.pwconv2.bias density is 1.0
stages.0.1.gamma density is 1.0
stages.0.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.1.norm.weight density is 1.0
stages.0.1.norm.bias density is 1.0
stages.0.1.pwconv1.weight density is 1.0
stages.0.1.pwconv1.bias density is 1.0
stages.0.1.pwconv2.weight density is 1.0
stages.0.1.pwconv2.bias density is 1.0
stages.0.2.gamma density is 1.0
stages.0.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.2.norm.weight density is 1.0
stages.0.2.norm.bias density is 1.0
stages.0.2.pwconv1.weight density is 1.0
stages.0.2.pwconv1.bias density is 1.0
stages.0.2.pwconv2.weight density is 1.0
stages.0.2.pwconv2.bias density is 1.0
stages.1.0.gamma density is 1.0
stages.1.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.0.norm.weight density is 1.0
stages.1.0.norm.bias density is 1.0
stages.1.0.pwconv1.weight density is 1.0
stages.1.0.pwconv1.bias density is 1.0
stages.1.0.pwconv2.weight density is 1.0
stages.1.0.pwconv2.bias density is 1.0
stages.1.1.gamma density is 1.0
stages.1.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.1.norm.weight density is 1.0
stages.1.1.norm.bias density is 1.0
stages.1.1.pwconv1.weight density is 1.0
stages.1.1.pwconv1.bias density is 1.0
stages.1.1.pwconv2.weight density is 1.0
stages.1.1.pwconv2.bias density is 1.0
stages.1.2.gamma density is 1.0
stages.1.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.2.norm.weight density is 1.0
stages.1.2.norm.bias density is 1.0
stages.1.2.pwconv1.weight density is 1.0
stages.1.2.pwconv1.bias density is 1.0
stages.1.2.pwconv2.weight density is 1.0
stages.1.2.pwconv2.bias density is 1.0
stages.2.0.gamma density is 1.0
stages.2.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.0.norm.weight density is 1.0
stages.2.0.norm.bias density is 1.0
stages.2.0.pwconv1.weight density is 1.0
stages.2.0.pwconv1.bias density is 1.0
stages.2.0.pwconv2.weight density is 1.0
stages.2.0.pwconv2.bias density is 1.0
stages.2.1.gamma density is 1.0
stages.2.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.1.norm.weight density is 1.0
stages.2.1.norm.bias density is 1.0
stages.2.1.pwconv1.weight density is 1.0
stages.2.1.pwconv1.bias density is 1.0
stages.2.1.pwconv2.weight density is 1.0
stages.2.1.pwconv2.bias density is 1.0
stages.2.2.gamma density is 1.0
stages.2.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.2.norm.weight density is 1.0
stages.2.2.norm.bias density is 1.0
stages.2.2.pwconv1.weight density is 1.0
stages.2.2.pwconv1.bias density is 1.0
stages.2.2.pwconv2.weight density is 1.0
stages.2.2.pwconv2.bias density is 1.0
stages.2.3.gamma density is 1.0
stages.2.3.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.3.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.3.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.3.norm.weight density is 1.0
stages.2.3.norm.bias density is 1.0
stages.2.3.pwconv1.weight density is 1.0
stages.2.3.pwconv1.bias density is 1.0
stages.2.3.pwconv2.weight density is 1.0
stages.2.3.pwconv2.bias density is 1.0
stages.2.4.gamma density is 1.0
stages.2.4.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.4.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.4.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.4.norm.weight density is 1.0
stages.2.4.norm.bias density is 1.0
stages.2.4.pwconv1.weight density is 1.0
stages.2.4.pwconv1.bias density is 1.0
stages.2.4.pwconv2.weight density is 1.0
stages.2.4.pwconv2.bias density is 1.0
stages.2.5.gamma density is 1.0
stages.2.5.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.5.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.5.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.5.norm.weight density is 1.0
stages.2.5.norm.bias density is 1.0
stages.2.5.pwconv1.weight density is 1.0
stages.2.5.pwconv1.bias density is 1.0
stages.2.5.pwconv2.weight density is 1.0
stages.2.5.pwconv2.bias density is 1.0
stages.2.6.gamma density is 1.0
stages.2.6.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.6.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.6.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.6.norm.weight density is 1.0
stages.2.6.norm.bias density is 1.0
stages.2.6.pwconv1.weight density is 1.0
stages.2.6.pwconv1.bias density is 1.0
stages.2.6.pwconv2.weight density is 1.0
stages.2.6.pwconv2.bias density is 1.0
stages.2.7.gamma density is 1.0
stages.2.7.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.7.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.7.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.7.norm.weight density is 1.0
stages.2.7.norm.bias density is 1.0
stages.2.7.pwconv1.weight density is 1.0
stages.2.7.pwconv1.bias density is 1.0
stages.2.7.pwconv2.weight density is 1.0
stages.2.7.pwconv2.bias density is 1.0
stages.2.8.gamma density is 1.0
stages.2.8.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.8.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.8.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.8.norm.weight density is 1.0
stages.2.8.norm.bias density is 1.0
stages.2.8.pwconv1.weight density is 1.0
stages.2.8.pwconv1.bias density is 1.0
stages.2.8.pwconv2.weight density is 1.0
stages.2.8.pwconv2.bias density is 1.0
stages.3.0.gamma density is 1.0
stages.3.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.0.norm.weight density is 1.0
stages.3.0.norm.bias density is 1.0
stages.3.0.pwconv1.weight density is 1.0
stages.3.0.pwconv1.bias density is 1.0
stages.3.0.pwconv2.weight density is 1.0
stages.3.0.pwconv2.bias density is 1.0
stages.3.1.gamma density is 1.0
stages.3.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.1.norm.weight density is 1.0
stages.3.1.norm.bias density is 1.0
stages.3.1.pwconv1.weight density is 1.0
stages.3.1.pwconv1.bias density is 1.0
stages.3.1.pwconv2.weight density is 1.0
stages.3.1.pwconv2.bias density is 1.0
stages.3.2.gamma density is 1.0
stages.3.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.2.norm.weight density is 1.0
stages.3.2.norm.bias density is 1.0
stages.3.2.pwconv1.weight density is 1.0
stages.3.2.pwconv1.bias density is 1.0
stages.3.2.pwconv2.weight density is 1.0
stages.3.2.pwconv2.bias density is 1.0
norm.weight density is 1.0
norm.bias density is 1.0
head.weight density is 1.0
head.bias density is 1.0
Test:  [  0/313]  eta: 0:15:24  loss: 3.9298 (3.9298)  acc1: 23.9583 (23.9583)  acc5: 42.7083 (42.7083)  time: 2.9543  data: 1.6106  max mem: 2139
Test:  [ 10/313]  eta: 0:01:39  loss: 3.4756 (3.3261)  acc1: 33.3333 (35.8902)  acc5: 50.0000 (52.5568)  time: 0.3276  data: 0.1465  max mem: 2139
Test:  [ 20/313]  eta: 0:00:59  loss: 3.2618 (3.1062)  acc1: 37.5000 (40.8234)  acc5: 51.0417 (56.0516)  time: 0.0640  data: 0.0001  max mem: 2139
Test:  [ 30/313]  eta: 0:00:44  loss: 2.1256 (2.8155)  acc1: 58.3333 (46.3038)  acc5: 71.8750 (61.9960)  time: 0.0648  data: 0.0018  max mem: 2139
Test:  [ 40/313]  eta: 0:00:37  loss: 2.1256 (2.7248)  acc1: 59.3750 (48.4756)  acc5: 73.9583 (63.4146)  time: 0.0664  data: 0.0034  max mem: 2139
Test:  [ 50/313]  eta: 0:00:33  loss: 3.1914 (2.9225)  acc1: 34.3750 (45.1389)  acc5: 51.0417 (59.7426)  time: 0.0763  data: 0.0133  max mem: 2139
Test:  [ 60/313]  eta: 0:00:31  loss: 3.6066 (3.0232)  acc1: 28.1250 (43.0669)  acc5: 46.8750 (58.1113)  time: 0.0997  data: 0.0365  max mem: 2139
Test:  [ 70/313]  eta: 0:00:29  loss: 3.2089 (2.9761)  acc1: 36.4583 (44.0581)  acc5: 56.2500 (58.7735)  time: 0.1091  data: 0.0456  max mem: 2139
Test:  [ 80/313]  eta: 0:00:27  loss: 1.9647 (2.8206)  acc1: 58.3333 (46.4763)  acc5: 77.0833 (61.7027)  time: 0.1007  data: 0.0370  max mem: 2139
Test:  [ 90/313]  eta: 0:00:25  loss: 1.4664 (2.6848)  acc1: 63.5417 (48.8324)  acc5: 84.3750 (64.1026)  time: 0.0952  data: 0.0318  max mem: 2139
Test:  [100/313]  eta: 0:00:23  loss: 1.4793 (2.5897)  acc1: 66.6667 (50.5157)  acc5: 82.2917 (65.7178)  time: 0.0843  data: 0.0209  max mem: 2139
Test:  [110/313]  eta: 0:00:22  loss: 2.0134 (2.5949)  acc1: 57.2917 (50.3378)  acc5: 78.1250 (65.7282)  time: 0.0852  data: 0.0219  max mem: 2139
Test:  [120/313]  eta: 0:00:20  loss: 2.6735 (2.6283)  acc1: 45.8333 (49.7676)  acc5: 63.5417 (65.0396)  time: 0.0880  data: 0.0247  max mem: 2139
Test:  [130/313]  eta: 0:00:19  loss: 3.2528 (2.7209)  acc1: 35.4167 (48.1711)  acc5: 54.1667 (63.3588)  time: 0.1030  data: 0.0391  max mem: 2139
Test:  [140/313]  eta: 0:00:18  loss: 3.1967 (2.7482)  acc1: 36.4583 (47.5694)  acc5: 55.2083 (63.0319)  time: 0.0970  data: 0.0330  max mem: 2139
Test:  [150/313]  eta: 0:00:16  loss: 2.8103 (2.7404)  acc1: 42.7083 (47.7580)  acc5: 58.3333 (63.0312)  time: 0.0670  data: 0.0036  max mem: 2139
Test:  [160/313]  eta: 0:00:15  loss: 2.9487 (2.7819)  acc1: 41.6667 (47.0432)  acc5: 52.0833 (62.2089)  time: 0.0690  data: 0.0059  max mem: 2139
Test:  [170/313]  eta: 0:00:14  loss: 3.3627 (2.8018)  acc1: 35.4167 (46.6374)  acc5: 47.9167 (61.8238)  time: 0.0855  data: 0.0225  max mem: 2139
Test:  [180/313]  eta: 0:00:13  loss: 3.3627 (2.8439)  acc1: 29.1667 (45.8506)  acc5: 45.8333 (61.0267)  time: 0.0892  data: 0.0257  max mem: 2139
Test:  [190/313]  eta: 0:00:12  loss: 3.0171 (2.8135)  acc1: 42.7083 (46.4714)  acc5: 60.4167 (61.6329)  time: 0.0928  data: 0.0293  max mem: 2139
Test:  [200/313]  eta: 0:00:11  loss: 2.7193 (2.8100)  acc1: 46.8750 (46.6107)  acc5: 67.7083 (61.9092)  time: 0.1227  data: 0.0595  max mem: 2139
Test:  [210/313]  eta: 0:00:10  loss: 2.9477 (2.8178)  acc1: 43.7500 (46.4949)  acc5: 65.6250 (62.0113)  time: 0.1056  data: 0.0426  max mem: 2139
Test:  [220/313]  eta: 0:00:09  loss: 2.8203 (2.7913)  acc1: 47.9167 (46.9976)  acc5: 67.7083 (62.5801)  time: 0.0810  data: 0.0176  max mem: 2139
Test:  [230/313]  eta: 0:00:08  loss: 2.2122 (2.7666)  acc1: 57.2917 (47.4522)  acc5: 76.0417 (63.2305)  time: 0.0838  data: 0.0202  max mem: 2139
Test:  [240/313]  eta: 0:00:07  loss: 2.2230 (2.7687)  acc1: 52.0833 (47.4974)  acc5: 70.8333 (63.2305)  time: 0.0705  data: 0.0072  max mem: 2139
Test:  [250/313]  eta: 0:00:06  loss: 2.0712 (2.7302)  acc1: 52.0833 (48.2445)  acc5: 69.7917 (63.8654)  time: 0.0775  data: 0.0141  max mem: 2139
Test:  [260/313]  eta: 0:00:05  loss: 1.8721 (2.7197)  acc1: 61.4583 (48.4834)  acc5: 79.1667 (64.0565)  time: 0.0790  data: 0.0155  max mem: 2139
Test:  [270/313]  eta: 0:00:04  loss: 1.8507 (2.6805)  acc1: 65.6250 (49.2697)  acc5: 79.1667 (64.7947)  time: 0.0716  data: 0.0087  max mem: 2139
Test:  [280/313]  eta: 0:00:03  loss: 2.3594 (2.7052)  acc1: 52.0833 (48.8101)  acc5: 67.7083 (64.3461)  time: 0.0778  data: 0.0144  max mem: 2139
Test:  [290/313]  eta: 0:00:02  loss: 3.2957 (2.7363)  acc1: 35.4167 (48.2603)  acc5: 54.1667 (63.7385)  time: 0.0743  data: 0.0110  max mem: 2139
Test:  [300/313]  eta: 0:00:01  loss: 3.2857 (2.7432)  acc1: 34.3750 (48.0620)  acc5: 57.2917 (63.7320)  time: 0.0705  data: 0.0072  max mem: 2139
Test:  [310/313]  eta: 0:00:00  loss: 3.2006 (2.7656)  acc1: 39.5833 (47.6755)  acc5: 57.2917 (63.3173)  time: 0.0836  data: 0.0203  max mem: 2139
Test:  [312/313]  eta: 0:00:00  loss: 3.2006 (2.7716)  acc1: 40.6250 (47.5800)  acc5: 57.2917 (63.2100)  time: 0.0817  data: 0.0186  max mem: 2139
Test: Total time: 0:00:29 (0.0938 s / it)
* Acc@1 47.580 Acc@5 63.210 loss 2.772
Accuracy of the network on 30000 test images: 47.58000%
SLak_kernel_distill_R.sh: line 162: source: deactivate: file not found
