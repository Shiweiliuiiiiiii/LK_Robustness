Not using distributed mode
Namespace(Decom=False, aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=64, bn=False, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/sliu/project_space/datasets/sketch', data_set='IMNET', data_type='imagenet-s', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.1, enable_wandb=False, epochs=300, eval=True, eval_data_path=None, finetune='', fix=False, growth='random', head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, kernel_size=[7, 7, 7, 7, 100], layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=-1, log_dir=None, lr=0.004, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='resnet50', model_ema=False, model_ema_decay=0.9999, model_ema_eval=False, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_workers=10, only_L=False, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', parallel=True, pin_mem=True, project='SLaK', prune='magnitude', prune_rate=0.3, recount=1, redistribution='none', remode='pixel', reprob=0.25, resplit=False, resume='/gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_VitTeacher_STRN50_NKD/checkpoint-best.pth', save_ckpt=True, save_ckpt_freq=10, save_ckpt_num=3, seed=0, smoothing=0.1, sparse=False, sparse_init='snip', sparsity=0.4, start_epoch=0, train_interpolation='bicubic', update_freq=1, update_frequency=100, use_amp=False, verbose=False, wandb_ckpt=False, warmup_epochs=20, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, width_factor=1.0, world_size=1)
Transform = 
Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /home/sliu/project_space/datasets/sketch
Number of the class = 1000
Mixup is activated!
Model = ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=True)
  (fc): Linear(in_features=2048, out_features=1000, bias=True)
)
number of params: 25557032
LR = 0.00400000
Batch size = 64
Update frequent = 1
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "conv1.weight",
      "layer1.0.conv1.weight",
      "layer1.0.conv2.weight",
      "layer1.0.conv3.weight",
      "layer1.0.downsample.0.weight",
      "layer1.1.conv1.weight",
      "layer1.1.conv2.weight",
      "layer1.1.conv3.weight",
      "layer1.2.conv1.weight",
      "layer1.2.conv2.weight",
      "layer1.2.conv3.weight",
      "layer2.0.conv1.weight",
      "layer2.0.conv2.weight",
      "layer2.0.conv3.weight",
      "layer2.0.downsample.0.weight",
      "layer2.1.conv1.weight",
      "layer2.1.conv2.weight",
      "layer2.1.conv3.weight",
      "layer2.2.conv1.weight",
      "layer2.2.conv2.weight",
      "layer2.2.conv3.weight",
      "layer2.3.conv1.weight",
      "layer2.3.conv2.weight",
      "layer2.3.conv3.weight",
      "layer3.0.conv1.weight",
      "layer3.0.conv2.weight",
      "layer3.0.conv3.weight",
      "layer3.0.downsample.0.weight",
      "layer3.1.conv1.weight",
      "layer3.1.conv2.weight",
      "layer3.1.conv3.weight",
      "layer3.2.conv1.weight",
      "layer3.2.conv2.weight",
      "layer3.2.conv3.weight",
      "layer3.3.conv1.weight",
      "layer3.3.conv2.weight",
      "layer3.3.conv3.weight",
      "layer3.4.conv1.weight",
      "layer3.4.conv2.weight",
      "layer3.4.conv3.weight",
      "layer3.5.conv1.weight",
      "layer3.5.conv2.weight",
      "layer3.5.conv3.weight",
      "layer4.0.conv1.weight",
      "layer4.0.conv2.weight",
      "layer4.0.conv3.weight",
      "layer4.0.downsample.0.weight",
      "layer4.1.conv1.weight",
      "layer4.1.conv2.weight",
      "layer4.1.conv3.weight",
      "layer4.2.conv1.weight",
      "layer4.2.conv2.weight",
      "layer4.2.conv3.weight",
      "fc.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "bn1.weight",
      "bn1.bias",
      "layer1.0.bn1.weight",
      "layer1.0.bn1.bias",
      "layer1.0.bn2.weight",
      "layer1.0.bn2.bias",
      "layer1.0.bn3.weight",
      "layer1.0.bn3.bias",
      "layer1.0.downsample.1.weight",
      "layer1.0.downsample.1.bias",
      "layer1.1.bn1.weight",
      "layer1.1.bn1.bias",
      "layer1.1.bn2.weight",
      "layer1.1.bn2.bias",
      "layer1.1.bn3.weight",
      "layer1.1.bn3.bias",
      "layer1.2.bn1.weight",
      "layer1.2.bn1.bias",
      "layer1.2.bn2.weight",
      "layer1.2.bn2.bias",
      "layer1.2.bn3.weight",
      "layer1.2.bn3.bias",
      "layer2.0.bn1.weight",
      "layer2.0.bn1.bias",
      "layer2.0.bn2.weight",
      "layer2.0.bn2.bias",
      "layer2.0.bn3.weight",
      "layer2.0.bn3.bias",
      "layer2.0.downsample.1.weight",
      "layer2.0.downsample.1.bias",
      "layer2.1.bn1.weight",
      "layer2.1.bn1.bias",
      "layer2.1.bn2.weight",
      "layer2.1.bn2.bias",
      "layer2.1.bn3.weight",
      "layer2.1.bn3.bias",
      "layer2.2.bn1.weight",
      "layer2.2.bn1.bias",
      "layer2.2.bn2.weight",
      "layer2.2.bn2.bias",
      "layer2.2.bn3.weight",
      "layer2.2.bn3.bias",
      "layer2.3.bn1.weight",
      "layer2.3.bn1.bias",
      "layer2.3.bn2.weight",
      "layer2.3.bn2.bias",
      "layer2.3.bn3.weight",
      "layer2.3.bn3.bias",
      "layer3.0.bn1.weight",
      "layer3.0.bn1.bias",
      "layer3.0.bn2.weight",
      "layer3.0.bn2.bias",
      "layer3.0.bn3.weight",
      "layer3.0.bn3.bias",
      "layer3.0.downsample.1.weight",
      "layer3.0.downsample.1.bias",
      "layer3.1.bn1.weight",
      "layer3.1.bn1.bias",
      "layer3.1.bn2.weight",
      "layer3.1.bn2.bias",
      "layer3.1.bn3.weight",
      "layer3.1.bn3.bias",
      "layer3.2.bn1.weight",
      "layer3.2.bn1.bias",
      "layer3.2.bn2.weight",
      "layer3.2.bn2.bias",
      "layer3.2.bn3.weight",
      "layer3.2.bn3.bias",
      "layer3.3.bn1.weight",
      "layer3.3.bn1.bias",
      "layer3.3.bn2.weight",
      "layer3.3.bn2.bias",
      "layer3.3.bn3.weight",
      "layer3.3.bn3.bias",
      "layer3.4.bn1.weight",
      "layer3.4.bn1.bias",
      "layer3.4.bn2.weight",
      "layer3.4.bn2.bias",
      "layer3.4.bn3.weight",
      "layer3.4.bn3.bias",
      "layer3.5.bn1.weight",
      "layer3.5.bn1.bias",
      "layer3.5.bn2.weight",
      "layer3.5.bn2.bias",
      "layer3.5.bn3.weight",
      "layer3.5.bn3.bias",
      "layer4.0.bn1.weight",
      "layer4.0.bn1.bias",
      "layer4.0.bn2.weight",
      "layer4.0.bn2.bias",
      "layer4.0.bn3.weight",
      "layer4.0.bn3.bias",
      "layer4.0.downsample.1.weight",
      "layer4.0.downsample.1.bias",
      "layer4.1.bn1.weight",
      "layer4.1.bn1.bias",
      "layer4.1.bn2.weight",
      "layer4.1.bn2.bias",
      "layer4.1.bn3.weight",
      "layer4.1.bn3.bias",
      "layer4.2.bn1.weight",
      "layer4.2.bn1.bias",
      "layer4.2.bn2.weight",
      "layer4.2.bn2.bias",
      "layer4.2.bn3.weight",
      "layer4.2.bn3.bias",
      "fc.bias"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
criterion = SoftTargetCrossEntropy()
dict_keys(['model', 'optimizer', 'epoch', 'scaler', 'args', 'model_ema'])
Resume checkpoint /gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_VitTeacher_STRN50_NKD/checkpoint-best.pth
With optim & sched!
conv1.weight density is 1.0
bn1.weight density is 1.0
bn1.bias density is 1.0
layer1.0.conv1.weight density is 1.0
layer1.0.bn1.weight density is 1.0
layer1.0.bn1.bias density is 1.0
layer1.0.conv2.weight density is 1.0
layer1.0.bn2.weight density is 1.0
layer1.0.bn2.bias density is 1.0
layer1.0.conv3.weight density is 1.0
layer1.0.bn3.weight density is 1.0
layer1.0.bn3.bias density is 1.0
layer1.0.downsample.0.weight density is 1.0
layer1.0.downsample.1.weight density is 1.0
layer1.0.downsample.1.bias density is 1.0
layer1.1.conv1.weight density is 1.0
layer1.1.bn1.weight density is 1.0
layer1.1.bn1.bias density is 1.0
layer1.1.conv2.weight density is 1.0
layer1.1.bn2.weight density is 1.0
layer1.1.bn2.bias density is 1.0
layer1.1.conv3.weight density is 1.0
layer1.1.bn3.weight density is 1.0
layer1.1.bn3.bias density is 1.0
layer1.2.conv1.weight density is 1.0
layer1.2.bn1.weight density is 1.0
layer1.2.bn1.bias density is 1.0
layer1.2.conv2.weight density is 1.0
layer1.2.bn2.weight density is 1.0
layer1.2.bn2.bias density is 1.0
layer1.2.conv3.weight density is 1.0
layer1.2.bn3.weight density is 1.0
layer1.2.bn3.bias density is 1.0
layer2.0.conv1.weight density is 1.0
layer2.0.bn1.weight density is 1.0
layer2.0.bn1.bias density is 1.0
layer2.0.conv2.weight density is 1.0
layer2.0.bn2.weight density is 1.0
layer2.0.bn2.bias density is 1.0
layer2.0.conv3.weight density is 1.0
layer2.0.bn3.weight density is 1.0
layer2.0.bn3.bias density is 1.0
layer2.0.downsample.0.weight density is 1.0
layer2.0.downsample.1.weight density is 1.0
layer2.0.downsample.1.bias density is 1.0
layer2.1.conv1.weight density is 1.0
layer2.1.bn1.weight density is 1.0
layer2.1.bn1.bias density is 1.0
layer2.1.conv2.weight density is 1.0
layer2.1.bn2.weight density is 1.0
layer2.1.bn2.bias density is 1.0
layer2.1.conv3.weight density is 1.0
layer2.1.bn3.weight density is 1.0
layer2.1.bn3.bias density is 1.0
layer2.2.conv1.weight density is 1.0
layer2.2.bn1.weight density is 1.0
layer2.2.bn1.bias density is 1.0
layer2.2.conv2.weight density is 1.0
layer2.2.bn2.weight density is 1.0
layer2.2.bn2.bias density is 1.0
layer2.2.conv3.weight density is 1.0
layer2.2.bn3.weight density is 1.0
layer2.2.bn3.bias density is 1.0
layer2.3.conv1.weight density is 1.0
layer2.3.bn1.weight density is 1.0
layer2.3.bn1.bias density is 1.0
layer2.3.conv2.weight density is 1.0
layer2.3.bn2.weight density is 1.0
layer2.3.bn2.bias density is 1.0
layer2.3.conv3.weight density is 1.0
layer2.3.bn3.weight density is 1.0
layer2.3.bn3.bias density is 1.0
layer3.0.conv1.weight density is 1.0
layer3.0.bn1.weight density is 1.0
layer3.0.bn1.bias density is 1.0
layer3.0.conv2.weight density is 1.0
layer3.0.bn2.weight density is 1.0
layer3.0.bn2.bias density is 1.0
layer3.0.conv3.weight density is 1.0
layer3.0.bn3.weight density is 1.0
layer3.0.bn3.bias density is 1.0
layer3.0.downsample.0.weight density is 1.0
layer3.0.downsample.1.weight density is 1.0
layer3.0.downsample.1.bias density is 1.0
layer3.1.conv1.weight density is 1.0
layer3.1.bn1.weight density is 1.0
layer3.1.bn1.bias density is 1.0
layer3.1.conv2.weight density is 1.0
layer3.1.bn2.weight density is 1.0
layer3.1.bn2.bias density is 1.0
layer3.1.conv3.weight density is 1.0
layer3.1.bn3.weight density is 1.0
layer3.1.bn3.bias density is 1.0
layer3.2.conv1.weight density is 1.0
layer3.2.bn1.weight density is 1.0
layer3.2.bn1.bias density is 1.0
layer3.2.conv2.weight density is 1.0
layer3.2.bn2.weight density is 1.0
layer3.2.bn2.bias density is 1.0
layer3.2.conv3.weight density is 1.0
layer3.2.bn3.weight density is 1.0
layer3.2.bn3.bias density is 1.0
layer3.3.conv1.weight density is 1.0
layer3.3.bn1.weight density is 1.0
layer3.3.bn1.bias density is 1.0
layer3.3.conv2.weight density is 1.0
layer3.3.bn2.weight density is 1.0
layer3.3.bn2.bias density is 1.0
layer3.3.conv3.weight density is 1.0
layer3.3.bn3.weight density is 1.0
layer3.3.bn3.bias density is 1.0
layer3.4.conv1.weight density is 1.0
layer3.4.bn1.weight density is 1.0
layer3.4.bn1.bias density is 1.0
layer3.4.conv2.weight density is 1.0
layer3.4.bn2.weight density is 1.0
layer3.4.bn2.bias density is 1.0
layer3.4.conv3.weight density is 1.0
layer3.4.bn3.weight density is 1.0
layer3.4.bn3.bias density is 1.0
layer3.5.conv1.weight density is 1.0
layer3.5.bn1.weight density is 1.0
layer3.5.bn1.bias density is 1.0
layer3.5.conv2.weight density is 1.0
layer3.5.bn2.weight density is 1.0
layer3.5.bn2.bias density is 1.0
layer3.5.conv3.weight density is 1.0
layer3.5.bn3.weight density is 1.0
layer3.5.bn3.bias density is 1.0
layer4.0.conv1.weight density is 1.0
layer4.0.bn1.weight density is 1.0
layer4.0.bn1.bias density is 1.0
layer4.0.conv2.weight density is 1.0
layer4.0.bn2.weight density is 1.0
layer4.0.bn2.bias density is 1.0
layer4.0.conv3.weight density is 1.0
layer4.0.bn3.weight density is 1.0
layer4.0.bn3.bias density is 1.0
layer4.0.downsample.0.weight density is 1.0
layer4.0.downsample.1.weight density is 1.0
layer4.0.downsample.1.bias density is 1.0
layer4.1.conv1.weight density is 1.0
layer4.1.bn1.weight density is 1.0
layer4.1.bn1.bias density is 1.0
layer4.1.conv2.weight density is 1.0
layer4.1.bn2.weight density is 1.0
layer4.1.bn2.bias density is 1.0
layer4.1.conv3.weight density is 1.0
layer4.1.bn3.weight density is 1.0
layer4.1.bn3.bias density is 1.0
layer4.2.conv1.weight density is 1.0
layer4.2.bn1.weight density is 1.0
layer4.2.bn1.bias density is 1.0
layer4.2.conv2.weight density is 1.0
layer4.2.bn2.weight density is 1.0
layer4.2.bn2.bias density is 1.0
layer4.2.conv3.weight density is 1.0
layer4.2.bn3.weight density is 1.0
layer4.2.bn3.bias density is 1.0
fc.weight density is 1.0
fc.bias density is 1.0
Test:  [  0/531]  eta: 0:32:00  loss: 5.5861 (5.5861)  acc1: 12.5000 (12.5000)  acc5: 30.2083 (30.2083)  time: 3.6159  data: 1.9812  max mem: 3726
Test:  [ 10/531]  eta: 0:03:14  loss: 4.7125 (4.4559)  acc1: 23.9583 (27.9356)  acc5: 38.5417 (44.6023)  time: 0.3733  data: 0.1802  max mem: 3726
Test:  [ 20/531]  eta: 0:01:57  loss: 3.9834 (4.3573)  acc1: 23.9583 (25.7937)  acc5: 44.7917 (46.7262)  time: 0.0601  data: 0.0119  max mem: 3726
Test:  [ 30/531]  eta: 0:01:38  loss: 4.7492 (4.8139)  acc1: 20.8333 (22.6815)  acc5: 40.6250 (41.5995)  time: 0.1000  data: 0.0524  max mem: 3726
Test:  [ 40/531]  eta: 0:01:40  loss: 5.1847 (4.8590)  acc1: 13.5417 (21.8242)  acc5: 32.2917 (41.1077)  time: 0.1791  data: 0.1315  max mem: 3726
Test:  [ 50/531]  eta: 0:01:36  loss: 4.3320 (4.7120)  acc1: 23.9583 (23.5294)  acc5: 44.7917 (42.7288)  time: 0.2038  data: 0.1574  max mem: 3726
Test:  [ 60/531]  eta: 0:01:34  loss: 4.4039 (4.7333)  acc1: 28.1250 (23.8900)  acc5: 44.7917 (42.2473)  time: 0.1931  data: 0.1322  max mem: 3726
Test:  [ 70/531]  eta: 0:01:43  loss: 4.6069 (4.7504)  acc1: 26.0417 (23.4302)  acc5: 41.6667 (41.6373)  time: 0.2867  data: 0.2072  max mem: 3726
Test:  [ 80/531]  eta: 0:01:42  loss: 4.2618 (4.7127)  acc1: 20.8333 (23.9712)  acc5: 42.7083 (41.7181)  time: 0.3107  data: 0.2394  max mem: 3726
Test:  [ 90/531]  eta: 0:01:35  loss: 3.8124 (4.5910)  acc1: 28.1250 (24.9199)  acc5: 51.0417 (43.5440)  time: 0.1905  data: 0.1354  max mem: 3726
Test:  [100/531]  eta: 0:01:30  loss: 3.1738 (4.4787)  acc1: 35.4167 (26.1757)  acc5: 62.5000 (45.1630)  time: 0.1378  data: 0.0765  max mem: 3726
Test:  [110/531]  eta: 0:01:25  loss: 3.1738 (4.3560)  acc1: 38.5417 (27.1209)  acc5: 64.5833 (47.1471)  time: 0.1366  data: 0.0726  max mem: 3726
Test:  [120/531]  eta: 0:01:20  loss: 3.0475 (4.2703)  acc1: 37.5000 (27.8753)  acc5: 67.7083 (48.5795)  time: 0.1251  data: 0.0589  max mem: 3726
Test:  [130/531]  eta: 0:01:18  loss: 3.4139 (4.2292)  acc1: 33.3333 (28.1250)  acc5: 60.4167 (49.2287)  time: 0.1556  data: 0.0880  max mem: 3726
Test:  [140/531]  eta: 0:01:16  loss: 3.3825 (4.1661)  acc1: 30.2083 (28.8638)  acc5: 61.4583 (50.2364)  time: 0.1913  data: 0.1293  max mem: 3726
Test:  [150/531]  eta: 0:01:14  loss: 3.3673 (4.1723)  acc1: 26.0417 (28.8424)  acc5: 61.4583 (50.3035)  time: 0.1978  data: 0.1328  max mem: 3726
Test:  [160/531]  eta: 0:01:14  loss: 3.8978 (4.1718)  acc1: 26.0417 (29.2055)  acc5: 53.1250 (50.3300)  time: 0.2405  data: 0.1830  max mem: 3726
Test:  [170/531]  eta: 0:01:11  loss: 4.4209 (4.1997)  acc1: 26.0417 (28.9352)  acc5: 46.8750 (50.0975)  time: 0.2282  data: 0.1582  max mem: 3726
Test:  [180/531]  eta: 0:01:10  loss: 4.6107 (4.1927)  acc1: 25.0000 (29.4026)  acc5: 44.7917 (50.1727)  time: 0.1911  data: 0.0828  max mem: 3726
Test:  [190/531]  eta: 0:01:09  loss: 3.8777 (4.1942)  acc1: 28.1250 (29.5430)  acc5: 48.9583 (50.2400)  time: 0.2384  data: 0.1327  max mem: 3726
Test:  [200/531]  eta: 0:01:06  loss: 3.8777 (4.1991)  acc1: 28.1250 (29.5657)  acc5: 48.9583 (49.9896)  time: 0.2030  data: 0.1324  max mem: 3726
Test:  [210/531]  eta: 0:01:04  loss: 4.7489 (4.2603)  acc1: 16.6667 (29.0877)  acc5: 40.6250 (49.2052)  time: 0.1887  data: 0.1370  max mem: 3726
Test:  [220/531]  eta: 0:01:03  loss: 4.6589 (4.2697)  acc1: 26.0417 (29.2562)  acc5: 40.6250 (48.9489)  time: 0.2268  data: 0.1784  max mem: 3726
Test:  [230/531]  eta: 0:01:00  loss: 4.0777 (4.2711)  acc1: 27.0833 (29.0900)  acc5: 44.7917 (48.6697)  time: 0.2001  data: 0.1513  max mem: 3726
Test:  [240/531]  eta: 0:00:59  loss: 4.1507 (4.2786)  acc1: 25.0000 (28.9549)  acc5: 44.7917 (48.4483)  time: 0.2413  data: 0.1945  max mem: 3726
Test:  [250/531]  eta: 0:00:58  loss: 4.4992 (4.3163)  acc1: 15.6250 (28.4612)  acc5: 38.5417 (47.7424)  time: 0.2897  data: 0.2361  max mem: 3726
Test:  [260/531]  eta: 0:00:56  loss: 4.5465 (4.3181)  acc1: 15.6250 (28.1849)  acc5: 33.3333 (47.4737)  time: 0.2413  data: 0.1548  max mem: 3726
Test:  [270/531]  eta: 0:00:54  loss: 4.2366 (4.3104)  acc1: 25.0000 (28.2788)  acc5: 38.5417 (47.5284)  time: 0.2240  data: 0.1236  max mem: 3726
Test:  [280/531]  eta: 0:00:53  loss: 3.6770 (4.2913)  acc1: 30.2083 (28.5142)  acc5: 55.2083 (47.7165)  time: 0.2459  data: 0.1573  max mem: 3726
Test:  [290/531]  eta: 0:00:50  loss: 3.7518 (4.2884)  acc1: 27.0833 (28.5080)  acc5: 50.0000 (47.5981)  time: 0.2018  data: 0.1232  max mem: 3726
Test:  [300/531]  eta: 0:00:47  loss: 4.0988 (4.2814)  acc1: 26.0417 (28.5922)  acc5: 46.8750 (47.5844)  time: 0.1486  data: 0.0927  max mem: 3726
Test:  [310/531]  eta: 0:00:45  loss: 4.0988 (4.2808)  acc1: 32.2917 (28.7212)  acc5: 46.8750 (47.5348)  time: 0.1690  data: 0.1236  max mem: 3726
Test:  [320/531]  eta: 0:00:43  loss: 3.8260 (4.2702)  acc1: 33.3333 (28.8324)  acc5: 43.7500 (47.5370)  time: 0.2109  data: 0.1535  max mem: 3726
Test:  [330/531]  eta: 0:00:41  loss: 3.8260 (4.2653)  acc1: 29.1667 (28.7009)  acc5: 43.7500 (47.4572)  time: 0.2023  data: 0.1359  max mem: 3726
Test:  [340/531]  eta: 0:00:39  loss: 4.4617 (4.2801)  acc1: 13.5417 (28.3541)  acc5: 39.5833 (47.0827)  time: 0.1582  data: 0.1009  max mem: 3726
Test:  [350/531]  eta: 0:00:37  loss: 4.7928 (4.2862)  acc1: 14.5833 (28.2111)  acc5: 33.3333 (46.9136)  time: 0.1810  data: 0.1354  max mem: 3726
Test:  [360/531]  eta: 0:00:35  loss: 4.7244 (4.2984)  acc1: 17.7083 (27.9086)  acc5: 33.3333 (46.6730)  time: 0.2333  data: 0.1682  max mem: 3726
Test:  [370/531]  eta: 0:00:33  loss: 4.7244 (4.3059)  acc1: 17.7083 (27.8302)  acc5: 37.5000 (46.5297)  time: 0.2364  data: 0.1556  max mem: 3726
Test:  [380/531]  eta: 0:00:31  loss: 4.6456 (4.3028)  acc1: 18.7500 (27.8188)  acc5: 38.5417 (46.4922)  time: 0.2228  data: 0.1558  max mem: 3726
Test:  [390/531]  eta: 0:00:28  loss: 4.8185 (4.3227)  acc1: 18.7500 (27.5922)  acc5: 36.4583 (46.1077)  time: 0.1781  data: 0.1233  max mem: 3726
Test:  [400/531]  eta: 0:00:26  loss: 4.9134 (4.3322)  acc1: 19.7917 (27.4808)  acc5: 29.1667 (45.9113)  time: 0.1875  data: 0.1354  max mem: 3726
Test:  [410/531]  eta: 0:00:24  loss: 4.0837 (4.3298)  acc1: 27.0833 (27.5649)  acc5: 41.6667 (45.9094)  time: 0.1893  data: 0.1384  max mem: 3726
Test:  [420/531]  eta: 0:00:22  loss: 4.3919 (4.3331)  acc1: 23.9583 (27.4743)  acc5: 41.6667 (45.8185)  time: 0.1527  data: 0.1040  max mem: 3726
Test:  [430/531]  eta: 0:00:20  loss: 4.3919 (4.3445)  acc1: 20.8333 (27.3323)  acc5: 41.6667 (45.5868)  time: 0.1681  data: 0.1122  max mem: 3726
Test:  [440/531]  eta: 0:00:18  loss: 4.1842 (4.3315)  acc1: 23.9583 (27.4353)  acc5: 44.7917 (45.7200)  time: 0.2627  data: 0.2045  max mem: 3726
Test:  [450/531]  eta: 0:00:16  loss: 3.8439 (4.3252)  acc1: 29.1667 (27.4159)  acc5: 47.9167 (45.7964)  time: 0.3420  data: 0.2639  max mem: 3726
Test:  [460/531]  eta: 0:00:15  loss: 4.4180 (4.3359)  acc1: 19.7917 (27.2551)  acc5: 37.5000 (45.4673)  time: 0.3366  data: 0.2362  max mem: 3726
Test:  [470/531]  eta: 0:00:12  loss: 4.2891 (4.3257)  acc1: 22.9167 (27.4306)  acc5: 40.6250 (45.5480)  time: 0.2309  data: 0.1515  max mem: 3726
Test:  [480/531]  eta: 0:00:10  loss: 4.2636 (4.3248)  acc1: 22.9167 (27.3129)  acc5: 44.7917 (45.5236)  time: 0.1508  data: 0.0974  max mem: 3726
Test:  [490/531]  eta: 0:00:08  loss: 4.3541 (4.3383)  acc1: 18.7500 (27.2297)  acc5: 36.4583 (45.3433)  time: 0.2450  data: 0.1976  max mem: 3726
Test:  [500/531]  eta: 0:00:06  loss: 5.9141 (4.3907)  acc1: 12.5000 (26.8733)  acc5: 23.9583 (44.7376)  time: 0.2856  data: 0.2379  max mem: 3726
Test:  [510/531]  eta: 0:00:04  loss: 6.3027 (4.4285)  acc1: 6.2500 (26.5350)  acc5: 11.4583 (44.1862)  time: 0.2316  data: 0.1846  max mem: 3726
Test:  [520/531]  eta: 0:00:02  loss: 6.0098 (4.4582)  acc1: 8.3333 (26.2496)  acc5: 18.7500 (43.7840)  time: 0.2120  data: 0.1653  max mem: 3726
Test:  [530/531]  eta: 0:00:00  loss: 6.1781 (4.4968)  acc1: 7.2917 (25.8916)  acc5: 18.7500 (43.2687)  time: 0.2366  data: 0.1831  max mem: 3726
Test: Total time: 0:01:53 (0.2134 s / it)
* Acc@1 25.892 Acc@5 43.269 loss 4.497
Accuracy of the network on 50889 test images: 25.89165%
Not using distributed mode
Namespace(Decom=False, aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=64, bn=False, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/sliu/project_space/datasets/sketch', data_set='IMNET', data_type='imagenet-s', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.1, enable_wandb=False, epochs=300, eval=True, eval_data_path=None, finetune='', fix=False, growth='random', head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, kernel_size=[7, 7, 7, 7, 100], layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=-1, log_dir=None, lr=0.004, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='resnet50', model_ema=False, model_ema_decay=0.9999, model_ema_eval=False, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_workers=10, only_L=False, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', parallel=True, pin_mem=True, project='SLaK', prune='magnitude', prune_rate=0.3, recount=1, redistribution='none', remode='pixel', reprob=0.25, resplit=False, resume='/gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_SwinTeacher_STRN50_NKD/checkpoint-best.pth', save_ckpt=True, save_ckpt_freq=10, save_ckpt_num=3, seed=0, smoothing=0.1, sparse=False, sparse_init='snip', sparsity=0.4, start_epoch=0, train_interpolation='bicubic', update_freq=1, update_frequency=100, use_amp=False, verbose=False, wandb_ckpt=False, warmup_epochs=20, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, width_factor=1.0, world_size=1)
Transform = 
Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /home/sliu/project_space/datasets/sketch
Number of the class = 1000
Mixup is activated!
Model = ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=True)
  (fc): Linear(in_features=2048, out_features=1000, bias=True)
)
number of params: 25557032
LR = 0.00400000
Batch size = 64
Update frequent = 1
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "conv1.weight",
      "layer1.0.conv1.weight",
      "layer1.0.conv2.weight",
      "layer1.0.conv3.weight",
      "layer1.0.downsample.0.weight",
      "layer1.1.conv1.weight",
      "layer1.1.conv2.weight",
      "layer1.1.conv3.weight",
      "layer1.2.conv1.weight",
      "layer1.2.conv2.weight",
      "layer1.2.conv3.weight",
      "layer2.0.conv1.weight",
      "layer2.0.conv2.weight",
      "layer2.0.conv3.weight",
      "layer2.0.downsample.0.weight",
      "layer2.1.conv1.weight",
      "layer2.1.conv2.weight",
      "layer2.1.conv3.weight",
      "layer2.2.conv1.weight",
      "layer2.2.conv2.weight",
      "layer2.2.conv3.weight",
      "layer2.3.conv1.weight",
      "layer2.3.conv2.weight",
      "layer2.3.conv3.weight",
      "layer3.0.conv1.weight",
      "layer3.0.conv2.weight",
      "layer3.0.conv3.weight",
      "layer3.0.downsample.0.weight",
      "layer3.1.conv1.weight",
      "layer3.1.conv2.weight",
      "layer3.1.conv3.weight",
      "layer3.2.conv1.weight",
      "layer3.2.conv2.weight",
      "layer3.2.conv3.weight",
      "layer3.3.conv1.weight",
      "layer3.3.conv2.weight",
      "layer3.3.conv3.weight",
      "layer3.4.conv1.weight",
      "layer3.4.conv2.weight",
      "layer3.4.conv3.weight",
      "layer3.5.conv1.weight",
      "layer3.5.conv2.weight",
      "layer3.5.conv3.weight",
      "layer4.0.conv1.weight",
      "layer4.0.conv2.weight",
      "layer4.0.conv3.weight",
      "layer4.0.downsample.0.weight",
      "layer4.1.conv1.weight",
      "layer4.1.conv2.weight",
      "layer4.1.conv3.weight",
      "layer4.2.conv1.weight",
      "layer4.2.conv2.weight",
      "layer4.2.conv3.weight",
      "fc.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "bn1.weight",
      "bn1.bias",
      "layer1.0.bn1.weight",
      "layer1.0.bn1.bias",
      "layer1.0.bn2.weight",
      "layer1.0.bn2.bias",
      "layer1.0.bn3.weight",
      "layer1.0.bn3.bias",
      "layer1.0.downsample.1.weight",
      "layer1.0.downsample.1.bias",
      "layer1.1.bn1.weight",
      "layer1.1.bn1.bias",
      "layer1.1.bn2.weight",
      "layer1.1.bn2.bias",
      "layer1.1.bn3.weight",
      "layer1.1.bn3.bias",
      "layer1.2.bn1.weight",
      "layer1.2.bn1.bias",
      "layer1.2.bn2.weight",
      "layer1.2.bn2.bias",
      "layer1.2.bn3.weight",
      "layer1.2.bn3.bias",
      "layer2.0.bn1.weight",
      "layer2.0.bn1.bias",
      "layer2.0.bn2.weight",
      "layer2.0.bn2.bias",
      "layer2.0.bn3.weight",
      "layer2.0.bn3.bias",
      "layer2.0.downsample.1.weight",
      "layer2.0.downsample.1.bias",
      "layer2.1.bn1.weight",
      "layer2.1.bn1.bias",
      "layer2.1.bn2.weight",
      "layer2.1.bn2.bias",
      "layer2.1.bn3.weight",
      "layer2.1.bn3.bias",
      "layer2.2.bn1.weight",
      "layer2.2.bn1.bias",
      "layer2.2.bn2.weight",
      "layer2.2.bn2.bias",
      "layer2.2.bn3.weight",
      "layer2.2.bn3.bias",
      "layer2.3.bn1.weight",
      "layer2.3.bn1.bias",
      "layer2.3.bn2.weight",
      "layer2.3.bn2.bias",
      "layer2.3.bn3.weight",
      "layer2.3.bn3.bias",
      "layer3.0.bn1.weight",
      "layer3.0.bn1.bias",
      "layer3.0.bn2.weight",
      "layer3.0.bn2.bias",
      "layer3.0.bn3.weight",
      "layer3.0.bn3.bias",
      "layer3.0.downsample.1.weight",
      "layer3.0.downsample.1.bias",
      "layer3.1.bn1.weight",
      "layer3.1.bn1.bias",
      "layer3.1.bn2.weight",
      "layer3.1.bn2.bias",
      "layer3.1.bn3.weight",
      "layer3.1.bn3.bias",
      "layer3.2.bn1.weight",
      "layer3.2.bn1.bias",
      "layer3.2.bn2.weight",
      "layer3.2.bn2.bias",
      "layer3.2.bn3.weight",
      "layer3.2.bn3.bias",
      "layer3.3.bn1.weight",
      "layer3.3.bn1.bias",
      "layer3.3.bn2.weight",
      "layer3.3.bn2.bias",
      "layer3.3.bn3.weight",
      "layer3.3.bn3.bias",
      "layer3.4.bn1.weight",
      "layer3.4.bn1.bias",
      "layer3.4.bn2.weight",
      "layer3.4.bn2.bias",
      "layer3.4.bn3.weight",
      "layer3.4.bn3.bias",
      "layer3.5.bn1.weight",
      "layer3.5.bn1.bias",
      "layer3.5.bn2.weight",
      "layer3.5.bn2.bias",
      "layer3.5.bn3.weight",
      "layer3.5.bn3.bias",
      "layer4.0.bn1.weight",
      "layer4.0.bn1.bias",
      "layer4.0.bn2.weight",
      "layer4.0.bn2.bias",
      "layer4.0.bn3.weight",
      "layer4.0.bn3.bias",
      "layer4.0.downsample.1.weight",
      "layer4.0.downsample.1.bias",
      "layer4.1.bn1.weight",
      "layer4.1.bn1.bias",
      "layer4.1.bn2.weight",
      "layer4.1.bn2.bias",
      "layer4.1.bn3.weight",
      "layer4.1.bn3.bias",
      "layer4.2.bn1.weight",
      "layer4.2.bn1.bias",
      "layer4.2.bn2.weight",
      "layer4.2.bn2.bias",
      "layer4.2.bn3.weight",
      "layer4.2.bn3.bias",
      "fc.bias"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
criterion = SoftTargetCrossEntropy()
dict_keys(['model', 'optimizer', 'epoch', 'scaler', 'args', 'model_ema'])
Resume checkpoint /gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_SwinTeacher_STRN50_NKD/checkpoint-best.pth
With optim & sched!
conv1.weight density is 1.0
bn1.weight density is 1.0
bn1.bias density is 1.0
layer1.0.conv1.weight density is 1.0
layer1.0.bn1.weight density is 1.0
layer1.0.bn1.bias density is 1.0
layer1.0.conv2.weight density is 1.0
layer1.0.bn2.weight density is 1.0
layer1.0.bn2.bias density is 1.0
layer1.0.conv3.weight density is 1.0
layer1.0.bn3.weight density is 1.0
layer1.0.bn3.bias density is 1.0
layer1.0.downsample.0.weight density is 1.0
layer1.0.downsample.1.weight density is 1.0
layer1.0.downsample.1.bias density is 1.0
layer1.1.conv1.weight density is 1.0
layer1.1.bn1.weight density is 1.0
layer1.1.bn1.bias density is 1.0
layer1.1.conv2.weight density is 1.0
layer1.1.bn2.weight density is 1.0
layer1.1.bn2.bias density is 1.0
layer1.1.conv3.weight density is 1.0
layer1.1.bn3.weight density is 1.0
layer1.1.bn3.bias density is 1.0
layer1.2.conv1.weight density is 1.0
layer1.2.bn1.weight density is 1.0
layer1.2.bn1.bias density is 1.0
layer1.2.conv2.weight density is 1.0
layer1.2.bn2.weight density is 1.0
layer1.2.bn2.bias density is 1.0
layer1.2.conv3.weight density is 1.0
layer1.2.bn3.weight density is 1.0
layer1.2.bn3.bias density is 1.0
layer2.0.conv1.weight density is 1.0
layer2.0.bn1.weight density is 1.0
layer2.0.bn1.bias density is 1.0
layer2.0.conv2.weight density is 1.0
layer2.0.bn2.weight density is 1.0
layer2.0.bn2.bias density is 1.0
layer2.0.conv3.weight density is 1.0
layer2.0.bn3.weight density is 1.0
layer2.0.bn3.bias density is 1.0
layer2.0.downsample.0.weight density is 1.0
layer2.0.downsample.1.weight density is 1.0
layer2.0.downsample.1.bias density is 1.0
layer2.1.conv1.weight density is 1.0
layer2.1.bn1.weight density is 1.0
layer2.1.bn1.bias density is 1.0
layer2.1.conv2.weight density is 1.0
layer2.1.bn2.weight density is 1.0
layer2.1.bn2.bias density is 1.0
layer2.1.conv3.weight density is 1.0
layer2.1.bn3.weight density is 1.0
layer2.1.bn3.bias density is 1.0
layer2.2.conv1.weight density is 1.0
layer2.2.bn1.weight density is 1.0
layer2.2.bn1.bias density is 1.0
layer2.2.conv2.weight density is 1.0
layer2.2.bn2.weight density is 1.0
layer2.2.bn2.bias density is 1.0
layer2.2.conv3.weight density is 1.0
layer2.2.bn3.weight density is 1.0
layer2.2.bn3.bias density is 1.0
layer2.3.conv1.weight density is 1.0
layer2.3.bn1.weight density is 1.0
layer2.3.bn1.bias density is 1.0
layer2.3.conv2.weight density is 1.0
layer2.3.bn2.weight density is 1.0
layer2.3.bn2.bias density is 1.0
layer2.3.conv3.weight density is 1.0
layer2.3.bn3.weight density is 1.0
layer2.3.bn3.bias density is 1.0
layer3.0.conv1.weight density is 1.0
layer3.0.bn1.weight density is 1.0
layer3.0.bn1.bias density is 1.0
layer3.0.conv2.weight density is 1.0
layer3.0.bn2.weight density is 1.0
layer3.0.bn2.bias density is 1.0
layer3.0.conv3.weight density is 1.0
layer3.0.bn3.weight density is 1.0
layer3.0.bn3.bias density is 1.0
layer3.0.downsample.0.weight density is 1.0
layer3.0.downsample.1.weight density is 1.0
layer3.0.downsample.1.bias density is 1.0
layer3.1.conv1.weight density is 1.0
layer3.1.bn1.weight density is 1.0
layer3.1.bn1.bias density is 1.0
layer3.1.conv2.weight density is 1.0
layer3.1.bn2.weight density is 1.0
layer3.1.bn2.bias density is 1.0
layer3.1.conv3.weight density is 1.0
layer3.1.bn3.weight density is 1.0
layer3.1.bn3.bias density is 1.0
layer3.2.conv1.weight density is 1.0
layer3.2.bn1.weight density is 1.0
layer3.2.bn1.bias density is 1.0
layer3.2.conv2.weight density is 1.0
layer3.2.bn2.weight density is 1.0
layer3.2.bn2.bias density is 1.0
layer3.2.conv3.weight density is 1.0
layer3.2.bn3.weight density is 1.0
layer3.2.bn3.bias density is 1.0
layer3.3.conv1.weight density is 1.0
layer3.3.bn1.weight density is 1.0
layer3.3.bn1.bias density is 1.0
layer3.3.conv2.weight density is 1.0
layer3.3.bn2.weight density is 1.0
layer3.3.bn2.bias density is 1.0
layer3.3.conv3.weight density is 1.0
layer3.3.bn3.weight density is 1.0
layer3.3.bn3.bias density is 1.0
layer3.4.conv1.weight density is 1.0
layer3.4.bn1.weight density is 1.0
layer3.4.bn1.bias density is 1.0
layer3.4.conv2.weight density is 1.0
layer3.4.bn2.weight density is 1.0
layer3.4.bn2.bias density is 1.0
layer3.4.conv3.weight density is 1.0
layer3.4.bn3.weight density is 1.0
layer3.4.bn3.bias density is 1.0
layer3.5.conv1.weight density is 1.0
layer3.5.bn1.weight density is 1.0
layer3.5.bn1.bias density is 1.0
layer3.5.conv2.weight density is 1.0
layer3.5.bn2.weight density is 1.0
layer3.5.bn2.bias density is 1.0
layer3.5.conv3.weight density is 1.0
layer3.5.bn3.weight density is 1.0
layer3.5.bn3.bias density is 1.0
layer4.0.conv1.weight density is 1.0
layer4.0.bn1.weight density is 1.0
layer4.0.bn1.bias density is 1.0
layer4.0.conv2.weight density is 1.0
layer4.0.bn2.weight density is 1.0
layer4.0.bn2.bias density is 1.0
layer4.0.conv3.weight density is 1.0
layer4.0.bn3.weight density is 1.0
layer4.0.bn3.bias density is 1.0
layer4.0.downsample.0.weight density is 1.0
layer4.0.downsample.1.weight density is 1.0
layer4.0.downsample.1.bias density is 1.0
layer4.1.conv1.weight density is 1.0
layer4.1.bn1.weight density is 1.0
layer4.1.bn1.bias density is 1.0
layer4.1.conv2.weight density is 1.0
layer4.1.bn2.weight density is 1.0
layer4.1.bn2.bias density is 1.0
layer4.1.conv3.weight density is 1.0
layer4.1.bn3.weight density is 1.0
layer4.1.bn3.bias density is 1.0
layer4.2.conv1.weight density is 1.0
layer4.2.bn1.weight density is 1.0
layer4.2.bn1.bias density is 1.0
layer4.2.conv2.weight density is 1.0
layer4.2.bn2.weight density is 1.0
layer4.2.bn2.bias density is 1.0
layer4.2.conv3.weight density is 1.0
layer4.2.bn3.weight density is 1.0
layer4.2.bn3.bias density is 1.0
fc.weight density is 1.0
fc.bias density is 1.0
Test:  [  0/531]  eta: 0:44:17  loss: 5.7823 (5.7823)  acc1: 3.1250 (3.1250)  acc5: 28.1250 (28.1250)  time: 5.0047  data: 2.7201  max mem: 3726
Test:  [ 10/531]  eta: 0:04:26  loss: 4.8621 (4.5613)  acc1: 20.8333 (26.0417)  acc5: 36.4583 (43.1818)  time: 0.5124  data: 0.2474  max mem: 3726
Test:  [ 20/531]  eta: 0:02:32  loss: 4.3576 (4.4406)  acc1: 20.8333 (24.9504)  acc5: 40.6250 (44.6429)  time: 0.0631  data: 0.0001  max mem: 3726
Test:  [ 30/531]  eta: 0:01:57  loss: 4.5194 (4.7925)  acc1: 18.7500 (22.0430)  acc5: 36.4583 (40.6586)  time: 0.0803  data: 0.0186  max mem: 3726
Test:  [ 40/531]  eta: 0:01:49  loss: 5.0311 (4.8252)  acc1: 13.5417 (21.8496)  acc5: 34.3750 (40.6504)  time: 0.1456  data: 0.0800  max mem: 3726
Test:  [ 50/531]  eta: 0:01:38  loss: 4.5425 (4.7013)  acc1: 25.0000 (23.5907)  acc5: 44.7917 (42.1773)  time: 0.1620  data: 0.1031  max mem: 3726
Test:  [ 60/531]  eta: 0:01:31  loss: 4.4979 (4.7025)  acc1: 27.0833 (23.6851)  acc5: 41.6667 (41.7179)  time: 0.1348  data: 0.0856  max mem: 3726
Test:  [ 70/531]  eta: 0:01:38  loss: 4.5069 (4.7169)  acc1: 22.9167 (23.3421)  acc5: 39.5833 (41.5640)  time: 0.2359  data: 0.1815  max mem: 3726
Test:  [ 80/531]  eta: 0:01:37  loss: 4.5745 (4.7015)  acc1: 20.8333 (23.7140)  acc5: 39.5833 (41.3966)  time: 0.2810  data: 0.1941  max mem: 3726
Test:  [ 90/531]  eta: 0:01:30  loss: 3.8047 (4.5951)  acc1: 29.1667 (24.4391)  acc5: 51.0417 (43.0975)  time: 0.1733  data: 0.0565  max mem: 3726
Test:  [100/531]  eta: 0:01:23  loss: 3.3768 (4.4987)  acc1: 35.4167 (25.4641)  acc5: 62.5000 (44.5751)  time: 0.1060  data: 0.0001  max mem: 3726
Test:  [110/531]  eta: 0:01:17  loss: 3.3768 (4.3944)  acc1: 35.4167 (26.4358)  acc5: 62.5000 (46.3495)  time: 0.0932  data: 0.0018  max mem: 3726
Test:  [120/531]  eta: 0:01:12  loss: 3.3127 (4.3285)  acc1: 31.2500 (26.9542)  acc5: 63.5417 (47.4346)  time: 0.0906  data: 0.0018  max mem: 3726
Test:  [130/531]  eta: 0:01:08  loss: 3.5582 (4.2907)  acc1: 32.2917 (27.2821)  acc5: 59.3750 (48.1314)  time: 0.0978  data: 0.0088  max mem: 3726
Test:  [140/531]  eta: 0:01:07  loss: 3.3853 (4.2363)  acc1: 33.3333 (27.9625)  acc5: 59.3750 (48.9731)  time: 0.1404  data: 0.0511  max mem: 3726
Test:  [150/531]  eta: 0:01:06  loss: 3.3804 (4.2388)  acc1: 26.0417 (27.8353)  acc5: 56.2500 (49.1170)  time: 0.1878  data: 0.1194  max mem: 3726
Test:  [160/531]  eta: 0:01:05  loss: 4.2574 (4.2368)  acc1: 29.1667 (28.1962)  acc5: 53.1250 (49.2365)  time: 0.2127  data: 0.1672  max mem: 3726
Test:  [170/531]  eta: 0:01:03  loss: 4.3085 (4.2511)  acc1: 29.1667 (28.2347)  acc5: 48.9583 (49.1411)  time: 0.2008  data: 0.1526  max mem: 3726
Test:  [180/531]  eta: 0:01:03  loss: 4.4817 (4.2330)  acc1: 28.1250 (28.7005)  acc5: 41.6667 (49.3554)  time: 0.2036  data: 0.1509  max mem: 3726
Test:  [190/531]  eta: 0:01:02  loss: 3.9911 (4.2345)  acc1: 31.2500 (28.8176)  acc5: 46.8750 (49.4164)  time: 0.2357  data: 0.1648  max mem: 3726
Test:  [200/531]  eta: 0:01:00  loss: 4.0851 (4.2413)  acc1: 31.2500 (28.7728)  acc5: 46.8750 (49.1656)  time: 0.2052  data: 0.1244  max mem: 3726
Test:  [210/531]  eta: 0:01:00  loss: 4.7306 (4.2906)  acc1: 17.7083 (28.3422)  acc5: 37.5000 (48.4202)  time: 0.2414  data: 0.1810  max mem: 3726
Test:  [220/531]  eta: 0:00:59  loss: 4.5902 (4.2867)  acc1: 26.0417 (28.5021)  acc5: 38.5417 (48.2655)  time: 0.2883  data: 0.2109  max mem: 3726
Test:  [230/531]  eta: 0:00:57  loss: 4.0507 (4.2889)  acc1: 28.1250 (28.4632)  acc5: 46.8750 (48.0069)  time: 0.2343  data: 0.1359  max mem: 3726
Test:  [240/531]  eta: 0:00:57  loss: 4.1384 (4.2958)  acc1: 27.0833 (28.3584)  acc5: 46.8750 (47.8173)  time: 0.2590  data: 0.1684  max mem: 3726
Test:  [250/531]  eta: 0:00:55  loss: 4.3893 (4.3239)  acc1: 18.7500 (27.9631)  acc5: 39.5833 (47.3025)  time: 0.2733  data: 0.1824  max mem: 3726
Test:  [260/531]  eta: 0:00:53  loss: 4.8207 (4.3286)  acc1: 21.8750 (27.7658)  acc5: 34.3750 (47.0346)  time: 0.2035  data: 0.1346  max mem: 3726
Test:  [270/531]  eta: 0:00:51  loss: 4.0070 (4.3131)  acc1: 26.0417 (27.9674)  acc5: 44.7917 (47.1825)  time: 0.1660  data: 0.1204  max mem: 3726
Test:  [280/531]  eta: 0:00:49  loss: 3.6120 (4.2953)  acc1: 32.2917 (28.2288)  acc5: 53.1250 (47.3495)  time: 0.2074  data: 0.1635  max mem: 3726
Test:  [290/531]  eta: 0:00:47  loss: 3.9945 (4.2999)  acc1: 31.2500 (28.2109)  acc5: 48.9583 (47.2294)  time: 0.2003  data: 0.1553  max mem: 3726
Test:  [300/531]  eta: 0:00:45  loss: 3.9945 (4.2912)  acc1: 31.2500 (28.3742)  acc5: 44.7917 (47.3041)  time: 0.1646  data: 0.1185  max mem: 3726
Test:  [310/531]  eta: 0:00:43  loss: 4.0007 (4.2886)  acc1: 32.2917 (28.5336)  acc5: 46.8750 (47.3841)  time: 0.2113  data: 0.1642  max mem: 3726
Test:  [320/531]  eta: 0:00:41  loss: 4.1402 (4.2821)  acc1: 32.2917 (28.6410)  acc5: 46.8750 (47.3715)  time: 0.2185  data: 0.1696  max mem: 3726
Test:  [330/531]  eta: 0:00:39  loss: 4.2658 (4.2750)  acc1: 29.1667 (28.5908)  acc5: 44.7917 (47.3408)  time: 0.2161  data: 0.1488  max mem: 3726
Test:  [340/531]  eta: 0:00:38  loss: 4.5994 (4.2912)  acc1: 19.7917 (28.2380)  acc5: 36.4583 (47.0155)  time: 0.2489  data: 0.1600  max mem: 3726
Test:  [350/531]  eta: 0:00:36  loss: 4.6891 (4.2974)  acc1: 17.7083 (28.1606)  acc5: 36.4583 (46.8335)  time: 0.2441  data: 0.1455  max mem: 3726
Test:  [360/531]  eta: 0:00:34  loss: 4.6891 (4.3078)  acc1: 17.7083 (27.9057)  acc5: 35.4167 (46.6528)  time: 0.2071  data: 0.1060  max mem: 3726
Test:  [370/531]  eta: 0:00:32  loss: 4.7233 (4.3134)  acc1: 16.6667 (27.8218)  acc5: 37.5000 (46.4707)  time: 0.2123  data: 0.1415  max mem: 3726
Test:  [380/531]  eta: 0:00:30  loss: 4.4306 (4.3101)  acc1: 26.0417 (27.8133)  acc5: 40.6250 (46.4758)  time: 0.2358  data: 0.1919  max mem: 3726
Test:  [390/531]  eta: 0:00:28  loss: 4.7111 (4.3309)  acc1: 25.0000 (27.6028)  acc5: 40.6250 (46.0838)  time: 0.2063  data: 0.1627  max mem: 3726
Test:  [400/531]  eta: 0:00:26  loss: 4.8492 (4.3387)  acc1: 19.7917 (27.4938)  acc5: 33.3333 (45.9035)  time: 0.1904  data: 0.1472  max mem: 3726
Test:  [410/531]  eta: 0:00:24  loss: 4.2956 (4.3371)  acc1: 29.1667 (27.6105)  acc5: 43.7500 (45.9220)  time: 0.1721  data: 0.1265  max mem: 3726
Test:  [420/531]  eta: 0:00:22  loss: 4.5068 (4.3435)  acc1: 25.0000 (27.5238)  acc5: 41.6667 (45.8086)  time: 0.1485  data: 0.0968  max mem: 3726
Test:  [430/531]  eta: 0:00:20  loss: 4.5068 (4.3535)  acc1: 21.8750 (27.3879)  acc5: 40.6250 (45.6037)  time: 0.1686  data: 0.1098  max mem: 3726
Test:  [440/531]  eta: 0:00:18  loss: 4.0248 (4.3438)  acc1: 25.0000 (27.5250)  acc5: 43.7500 (45.7554)  time: 0.2114  data: 0.1521  max mem: 3726
Test:  [450/531]  eta: 0:00:16  loss: 3.8986 (4.3398)  acc1: 32.2917 (27.4945)  acc5: 47.9167 (45.8449)  time: 0.2655  data: 0.2149  max mem: 3726
Test:  [460/531]  eta: 0:00:14  loss: 4.5431 (4.3520)  acc1: 19.7917 (27.3454)  acc5: 37.5000 (45.5102)  time: 0.2290  data: 0.1851  max mem: 3726
Test:  [470/531]  eta: 0:00:12  loss: 4.3469 (4.3407)  acc1: 23.9583 (27.5367)  acc5: 38.5417 (45.6100)  time: 0.1444  data: 0.1019  max mem: 3726
Test:  [480/531]  eta: 0:00:10  loss: 4.2383 (4.3404)  acc1: 23.9583 (27.4298)  acc5: 45.8333 (45.6103)  time: 0.1495  data: 0.1074  max mem: 3726
Test:  [490/531]  eta: 0:00:08  loss: 4.6280 (4.3516)  acc1: 19.7917 (27.3464)  acc5: 40.6250 (45.4260)  time: 0.2447  data: 0.1994  max mem: 3726
Test:  [500/531]  eta: 0:00:06  loss: 5.4366 (4.3964)  acc1: 10.4167 (27.0147)  acc5: 26.0417 (44.8312)  time: 0.2626  data: 0.2132  max mem: 3726
Test:  [510/531]  eta: 0:00:04  loss: 6.3806 (4.4336)  acc1: 5.2083 (26.6818)  acc5: 12.5000 (44.2984)  time: 0.2233  data: 0.1758  max mem: 3726
Test:  [520/531]  eta: 0:00:02  loss: 6.0068 (4.4588)  acc1: 6.2500 (26.4135)  acc5: 19.7917 (43.9499)  time: 0.2385  data: 0.1795  max mem: 3726
Test:  [530/531]  eta: 0:00:00  loss: 5.7883 (4.4912)  acc1: 6.2500 (26.0705)  acc5: 20.8333 (43.4514)  time: 0.2277  data: 0.1458  max mem: 3726
Test: Total time: 0:01:48 (0.2036 s / it)
* Acc@1 26.070 Acc@5 43.451 loss 4.491
Accuracy of the network on 50889 test images: 26.07047%
Not using distributed mode
Namespace(Decom=False, aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=64, bn=False, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/sliu/project_space/datasets/sketch', data_set='IMNET', data_type='imagenet-s', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.1, enable_wandb=False, epochs=300, eval=True, eval_data_path=None, finetune='', fix=False, growth='random', head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, kernel_size=[7, 7, 7, 7, 100], layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=-1, log_dir=None, lr=0.004, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='resnet50', model_ema=False, model_ema_decay=0.9999, model_ema_eval=False, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_workers=10, only_L=False, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', parallel=True, pin_mem=True, project='SLaK', prune='magnitude', prune_rate=0.3, recount=1, redistribution='none', remode='pixel', reprob=0.25, resplit=False, resume='/gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_Teacherscswin_STRN50_NKD/checkpoint-best.pth', save_ckpt=True, save_ckpt_freq=10, save_ckpt_num=3, seed=0, smoothing=0.1, sparse=False, sparse_init='snip', sparsity=0.4, start_epoch=0, train_interpolation='bicubic', update_freq=1, update_frequency=100, use_amp=False, verbose=False, wandb_ckpt=False, warmup_epochs=20, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, width_factor=1.0, world_size=1)
Transform = 
Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /home/sliu/project_space/datasets/sketch
Number of the class = 1000
Mixup is activated!
Model = ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=True)
  (fc): Linear(in_features=2048, out_features=1000, bias=True)
)
number of params: 25557032
LR = 0.00400000
Batch size = 64
Update frequent = 1
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "conv1.weight",
      "layer1.0.conv1.weight",
      "layer1.0.conv2.weight",
      "layer1.0.conv3.weight",
      "layer1.0.downsample.0.weight",
      "layer1.1.conv1.weight",
      "layer1.1.conv2.weight",
      "layer1.1.conv3.weight",
      "layer1.2.conv1.weight",
      "layer1.2.conv2.weight",
      "layer1.2.conv3.weight",
      "layer2.0.conv1.weight",
      "layer2.0.conv2.weight",
      "layer2.0.conv3.weight",
      "layer2.0.downsample.0.weight",
      "layer2.1.conv1.weight",
      "layer2.1.conv2.weight",
      "layer2.1.conv3.weight",
      "layer2.2.conv1.weight",
      "layer2.2.conv2.weight",
      "layer2.2.conv3.weight",
      "layer2.3.conv1.weight",
      "layer2.3.conv2.weight",
      "layer2.3.conv3.weight",
      "layer3.0.conv1.weight",
      "layer3.0.conv2.weight",
      "layer3.0.conv3.weight",
      "layer3.0.downsample.0.weight",
      "layer3.1.conv1.weight",
      "layer3.1.conv2.weight",
      "layer3.1.conv3.weight",
      "layer3.2.conv1.weight",
      "layer3.2.conv2.weight",
      "layer3.2.conv3.weight",
      "layer3.3.conv1.weight",
      "layer3.3.conv2.weight",
      "layer3.3.conv3.weight",
      "layer3.4.conv1.weight",
      "layer3.4.conv2.weight",
      "layer3.4.conv3.weight",
      "layer3.5.conv1.weight",
      "layer3.5.conv2.weight",
      "layer3.5.conv3.weight",
      "layer4.0.conv1.weight",
      "layer4.0.conv2.weight",
      "layer4.0.conv3.weight",
      "layer4.0.downsample.0.weight",
      "layer4.1.conv1.weight",
      "layer4.1.conv2.weight",
      "layer4.1.conv3.weight",
      "layer4.2.conv1.weight",
      "layer4.2.conv2.weight",
      "layer4.2.conv3.weight",
      "fc.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "bn1.weight",
      "bn1.bias",
      "layer1.0.bn1.weight",
      "layer1.0.bn1.bias",
      "layer1.0.bn2.weight",
      "layer1.0.bn2.bias",
      "layer1.0.bn3.weight",
      "layer1.0.bn3.bias",
      "layer1.0.downsample.1.weight",
      "layer1.0.downsample.1.bias",
      "layer1.1.bn1.weight",
      "layer1.1.bn1.bias",
      "layer1.1.bn2.weight",
      "layer1.1.bn2.bias",
      "layer1.1.bn3.weight",
      "layer1.1.bn3.bias",
      "layer1.2.bn1.weight",
      "layer1.2.bn1.bias",
      "layer1.2.bn2.weight",
      "layer1.2.bn2.bias",
      "layer1.2.bn3.weight",
      "layer1.2.bn3.bias",
      "layer2.0.bn1.weight",
      "layer2.0.bn1.bias",
      "layer2.0.bn2.weight",
      "layer2.0.bn2.bias",
      "layer2.0.bn3.weight",
      "layer2.0.bn3.bias",
      "layer2.0.downsample.1.weight",
      "layer2.0.downsample.1.bias",
      "layer2.1.bn1.weight",
      "layer2.1.bn1.bias",
      "layer2.1.bn2.weight",
      "layer2.1.bn2.bias",
      "layer2.1.bn3.weight",
      "layer2.1.bn3.bias",
      "layer2.2.bn1.weight",
      "layer2.2.bn1.bias",
      "layer2.2.bn2.weight",
      "layer2.2.bn2.bias",
      "layer2.2.bn3.weight",
      "layer2.2.bn3.bias",
      "layer2.3.bn1.weight",
      "layer2.3.bn1.bias",
      "layer2.3.bn2.weight",
      "layer2.3.bn2.bias",
      "layer2.3.bn3.weight",
      "layer2.3.bn3.bias",
      "layer3.0.bn1.weight",
      "layer3.0.bn1.bias",
      "layer3.0.bn2.weight",
      "layer3.0.bn2.bias",
      "layer3.0.bn3.weight",
      "layer3.0.bn3.bias",
      "layer3.0.downsample.1.weight",
      "layer3.0.downsample.1.bias",
      "layer3.1.bn1.weight",
      "layer3.1.bn1.bias",
      "layer3.1.bn2.weight",
      "layer3.1.bn2.bias",
      "layer3.1.bn3.weight",
      "layer3.1.bn3.bias",
      "layer3.2.bn1.weight",
      "layer3.2.bn1.bias",
      "layer3.2.bn2.weight",
      "layer3.2.bn2.bias",
      "layer3.2.bn3.weight",
      "layer3.2.bn3.bias",
      "layer3.3.bn1.weight",
      "layer3.3.bn1.bias",
      "layer3.3.bn2.weight",
      "layer3.3.bn2.bias",
      "layer3.3.bn3.weight",
      "layer3.3.bn3.bias",
      "layer3.4.bn1.weight",
      "layer3.4.bn1.bias",
      "layer3.4.bn2.weight",
      "layer3.4.bn2.bias",
      "layer3.4.bn3.weight",
      "layer3.4.bn3.bias",
      "layer3.5.bn1.weight",
      "layer3.5.bn1.bias",
      "layer3.5.bn2.weight",
      "layer3.5.bn2.bias",
      "layer3.5.bn3.weight",
      "layer3.5.bn3.bias",
      "layer4.0.bn1.weight",
      "layer4.0.bn1.bias",
      "layer4.0.bn2.weight",
      "layer4.0.bn2.bias",
      "layer4.0.bn3.weight",
      "layer4.0.bn3.bias",
      "layer4.0.downsample.1.weight",
      "layer4.0.downsample.1.bias",
      "layer4.1.bn1.weight",
      "layer4.1.bn1.bias",
      "layer4.1.bn2.weight",
      "layer4.1.bn2.bias",
      "layer4.1.bn3.weight",
      "layer4.1.bn3.bias",
      "layer4.2.bn1.weight",
      "layer4.2.bn1.bias",
      "layer4.2.bn2.weight",
      "layer4.2.bn2.bias",
      "layer4.2.bn3.weight",
      "layer4.2.bn3.bias",
      "fc.bias"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
criterion = SoftTargetCrossEntropy()
dict_keys(['model', 'optimizer', 'epoch', 'scaler', 'args', 'model_ema'])
Resume checkpoint /gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_Teacherscswin_STRN50_NKD/checkpoint-best.pth
With optim & sched!
conv1.weight density is 1.0
bn1.weight density is 1.0
bn1.bias density is 1.0
layer1.0.conv1.weight density is 1.0
layer1.0.bn1.weight density is 1.0
layer1.0.bn1.bias density is 1.0
layer1.0.conv2.weight density is 1.0
layer1.0.bn2.weight density is 1.0
layer1.0.bn2.bias density is 1.0
layer1.0.conv3.weight density is 1.0
layer1.0.bn3.weight density is 1.0
layer1.0.bn3.bias density is 1.0
layer1.0.downsample.0.weight density is 1.0
layer1.0.downsample.1.weight density is 1.0
layer1.0.downsample.1.bias density is 1.0
layer1.1.conv1.weight density is 1.0
layer1.1.bn1.weight density is 1.0
layer1.1.bn1.bias density is 1.0
layer1.1.conv2.weight density is 1.0
layer1.1.bn2.weight density is 1.0
layer1.1.bn2.bias density is 1.0
layer1.1.conv3.weight density is 1.0
layer1.1.bn3.weight density is 1.0
layer1.1.bn3.bias density is 1.0
layer1.2.conv1.weight density is 1.0
layer1.2.bn1.weight density is 1.0
layer1.2.bn1.bias density is 1.0
layer1.2.conv2.weight density is 1.0
layer1.2.bn2.weight density is 1.0
layer1.2.bn2.bias density is 1.0
layer1.2.conv3.weight density is 1.0
layer1.2.bn3.weight density is 1.0
layer1.2.bn3.bias density is 1.0
layer2.0.conv1.weight density is 1.0
layer2.0.bn1.weight density is 1.0
layer2.0.bn1.bias density is 1.0
layer2.0.conv2.weight density is 1.0
layer2.0.bn2.weight density is 1.0
layer2.0.bn2.bias density is 1.0
layer2.0.conv3.weight density is 1.0
layer2.0.bn3.weight density is 1.0
layer2.0.bn3.bias density is 1.0
layer2.0.downsample.0.weight density is 1.0
layer2.0.downsample.1.weight density is 1.0
layer2.0.downsample.1.bias density is 1.0
layer2.1.conv1.weight density is 1.0
layer2.1.bn1.weight density is 1.0
layer2.1.bn1.bias density is 1.0
layer2.1.conv2.weight density is 1.0
layer2.1.bn2.weight density is 1.0
layer2.1.bn2.bias density is 1.0
layer2.1.conv3.weight density is 1.0
layer2.1.bn3.weight density is 1.0
layer2.1.bn3.bias density is 1.0
layer2.2.conv1.weight density is 1.0
layer2.2.bn1.weight density is 1.0
layer2.2.bn1.bias density is 1.0
layer2.2.conv2.weight density is 1.0
layer2.2.bn2.weight density is 1.0
layer2.2.bn2.bias density is 1.0
layer2.2.conv3.weight density is 1.0
layer2.2.bn3.weight density is 1.0
layer2.2.bn3.bias density is 1.0
layer2.3.conv1.weight density is 1.0
layer2.3.bn1.weight density is 1.0
layer2.3.bn1.bias density is 1.0
layer2.3.conv2.weight density is 1.0
layer2.3.bn2.weight density is 1.0
layer2.3.bn2.bias density is 1.0
layer2.3.conv3.weight density is 1.0
layer2.3.bn3.weight density is 1.0
layer2.3.bn3.bias density is 1.0
layer3.0.conv1.weight density is 1.0
layer3.0.bn1.weight density is 1.0
layer3.0.bn1.bias density is 1.0
layer3.0.conv2.weight density is 1.0
layer3.0.bn2.weight density is 1.0
layer3.0.bn2.bias density is 1.0
layer3.0.conv3.weight density is 1.0
layer3.0.bn3.weight density is 1.0
layer3.0.bn3.bias density is 1.0
layer3.0.downsample.0.weight density is 1.0
layer3.0.downsample.1.weight density is 1.0
layer3.0.downsample.1.bias density is 1.0
layer3.1.conv1.weight density is 1.0
layer3.1.bn1.weight density is 1.0
layer3.1.bn1.bias density is 1.0
layer3.1.conv2.weight density is 1.0
layer3.1.bn2.weight density is 1.0
layer3.1.bn2.bias density is 1.0
layer3.1.conv3.weight density is 1.0
layer3.1.bn3.weight density is 1.0
layer3.1.bn3.bias density is 1.0
layer3.2.conv1.weight density is 1.0
layer3.2.bn1.weight density is 1.0
layer3.2.bn1.bias density is 1.0
layer3.2.conv2.weight density is 1.0
layer3.2.bn2.weight density is 1.0
layer3.2.bn2.bias density is 1.0
layer3.2.conv3.weight density is 1.0
layer3.2.bn3.weight density is 1.0
layer3.2.bn3.bias density is 1.0
layer3.3.conv1.weight density is 1.0
layer3.3.bn1.weight density is 1.0
layer3.3.bn1.bias density is 1.0
layer3.3.conv2.weight density is 1.0
layer3.3.bn2.weight density is 1.0
layer3.3.bn2.bias density is 1.0
layer3.3.conv3.weight density is 1.0
layer3.3.bn3.weight density is 1.0
layer3.3.bn3.bias density is 1.0
layer3.4.conv1.weight density is 1.0
layer3.4.bn1.weight density is 1.0
layer3.4.bn1.bias density is 1.0
layer3.4.conv2.weight density is 1.0
layer3.4.bn2.weight density is 1.0
layer3.4.bn2.bias density is 1.0
layer3.4.conv3.weight density is 1.0
layer3.4.bn3.weight density is 1.0
layer3.4.bn3.bias density is 1.0
layer3.5.conv1.weight density is 1.0
layer3.5.bn1.weight density is 1.0
layer3.5.bn1.bias density is 1.0
layer3.5.conv2.weight density is 1.0
layer3.5.bn2.weight density is 1.0
layer3.5.bn2.bias density is 1.0
layer3.5.conv3.weight density is 1.0
layer3.5.bn3.weight density is 1.0
layer3.5.bn3.bias density is 1.0
layer4.0.conv1.weight density is 1.0
layer4.0.bn1.weight density is 1.0
layer4.0.bn1.bias density is 1.0
layer4.0.conv2.weight density is 1.0
layer4.0.bn2.weight density is 1.0
layer4.0.bn2.bias density is 1.0
layer4.0.conv3.weight density is 1.0
layer4.0.bn3.weight density is 1.0
layer4.0.bn3.bias density is 1.0
layer4.0.downsample.0.weight density is 1.0
layer4.0.downsample.1.weight density is 1.0
layer4.0.downsample.1.bias density is 1.0
layer4.1.conv1.weight density is 1.0
layer4.1.bn1.weight density is 1.0
layer4.1.bn1.bias density is 1.0
layer4.1.conv2.weight density is 1.0
layer4.1.bn2.weight density is 1.0
layer4.1.bn2.bias density is 1.0
layer4.1.conv3.weight density is 1.0
layer4.1.bn3.weight density is 1.0
layer4.1.bn3.bias density is 1.0
layer4.2.conv1.weight density is 1.0
layer4.2.bn1.weight density is 1.0
layer4.2.bn1.bias density is 1.0
layer4.2.conv2.weight density is 1.0
layer4.2.bn2.weight density is 1.0
layer4.2.bn2.bias density is 1.0
layer4.2.conv3.weight density is 1.0
layer4.2.bn3.weight density is 1.0
layer4.2.bn3.bias density is 1.0
fc.weight density is 1.0
fc.bias density is 1.0
Test:  [  0/531]  eta: 0:40:30  loss: 5.7778 (5.7778)  acc1: 8.3333 (8.3333)  acc5: 30.2083 (30.2083)  time: 4.5769  data: 2.2044  max mem: 3726
Test:  [ 10/531]  eta: 0:04:09  loss: 4.5714 (4.3924)  acc1: 25.0000 (28.6932)  acc5: 44.7917 (46.0227)  time: 0.4787  data: 0.2005  max mem: 3726
Test:  [ 20/531]  eta: 0:02:18  loss: 4.4514 (4.3320)  acc1: 25.0000 (27.4306)  acc5: 44.7917 (47.1726)  time: 0.0555  data: 0.0001  max mem: 3726
Test:  [ 30/531]  eta: 0:01:48  loss: 4.6043 (4.6962)  acc1: 21.8750 (23.8911)  acc5: 36.4583 (42.2715)  time: 0.0710  data: 0.0290  max mem: 3726
Test:  [ 40/531]  eta: 0:01:39  loss: 4.8768 (4.7338)  acc1: 15.6250 (23.2215)  acc5: 33.3333 (41.5904)  time: 0.1305  data: 0.0871  max mem: 3726
Test:  [ 50/531]  eta: 0:01:32  loss: 4.4874 (4.6194)  acc1: 28.1250 (24.5507)  acc5: 41.6667 (43.0351)  time: 0.1584  data: 0.1142  max mem: 3726
Test:  [ 60/531]  eta: 0:01:26  loss: 4.4378 (4.6279)  acc1: 27.0833 (24.7439)  acc5: 43.7500 (42.7937)  time: 0.1423  data: 0.0989  max mem: 3726
Test:  [ 70/531]  eta: 0:01:30  loss: 4.5135 (4.6337)  acc1: 25.0000 (24.3251)  acc5: 41.6667 (42.6937)  time: 0.2021  data: 0.1579  max mem: 3726
Test:  [ 80/531]  eta: 0:01:27  loss: 4.4789 (4.6227)  acc1: 23.9583 (24.6914)  acc5: 42.7083 (42.6569)  time: 0.2241  data: 0.1779  max mem: 3726
Test:  [ 90/531]  eta: 0:01:20  loss: 4.0330 (4.5183)  acc1: 27.0833 (25.4922)  acc5: 51.0417 (44.3338)  time: 0.1327  data: 0.0864  max mem: 3726
Test:  [100/531]  eta: 0:01:14  loss: 3.2030 (4.4192)  acc1: 34.3750 (26.3304)  acc5: 62.5000 (45.9262)  time: 0.0899  data: 0.0459  max mem: 3726
Test:  [110/531]  eta: 0:01:11  loss: 3.2582 (4.3199)  acc1: 36.4583 (27.2241)  acc5: 63.5417 (47.7571)  time: 0.1196  data: 0.0699  max mem: 3726
Test:  [120/531]  eta: 0:01:07  loss: 3.0982 (4.2446)  acc1: 36.4583 (27.7893)  acc5: 67.7083 (49.0702)  time: 0.1259  data: 0.0749  max mem: 3726
Test:  [130/531]  eta: 0:01:07  loss: 3.5430 (4.2121)  acc1: 32.2917 (28.0455)  acc5: 59.3750 (49.7296)  time: 0.1587  data: 0.0990  max mem: 3726
Test:  [140/531]  eta: 0:01:05  loss: 3.3547 (4.1503)  acc1: 32.2917 (28.9450)  acc5: 63.5417 (50.7018)  time: 0.1845  data: 0.1123  max mem: 3726
Test:  [150/531]  eta: 0:01:05  loss: 3.3547 (4.1609)  acc1: 26.0417 (28.7804)  acc5: 56.2500 (50.5864)  time: 0.1871  data: 0.1138  max mem: 3726
Test:  [160/531]  eta: 0:01:04  loss: 3.8813 (4.1622)  acc1: 29.1667 (29.0567)  acc5: 53.1250 (50.6470)  time: 0.2166  data: 0.1489  max mem: 3726
Test:  [170/531]  eta: 0:01:02  loss: 3.9943 (4.1716)  acc1: 31.2500 (29.1301)  acc5: 47.9167 (50.4691)  time: 0.1773  data: 0.1173  max mem: 3726
Test:  [180/531]  eta: 0:01:00  loss: 4.3235 (4.1632)  acc1: 27.0833 (29.5235)  acc5: 43.7500 (50.4777)  time: 0.1517  data: 0.0995  max mem: 3726
Test:  [190/531]  eta: 0:00:59  loss: 3.8333 (4.1631)  acc1: 28.1250 (29.6357)  acc5: 45.8333 (50.4963)  time: 0.1870  data: 0.1420  max mem: 3726
Test:  [200/531]  eta: 0:00:57  loss: 4.2778 (4.1745)  acc1: 28.1250 (29.5605)  acc5: 45.8333 (50.2436)  time: 0.1961  data: 0.1430  max mem: 3726
Test:  [210/531]  eta: 0:00:56  loss: 4.5868 (4.2242)  acc1: 20.8333 (29.1519)  acc5: 38.5417 (49.5211)  time: 0.1967  data: 0.1435  max mem: 3726
Test:  [220/531]  eta: 0:00:55  loss: 4.4611 (4.2283)  acc1: 27.0833 (29.2185)  acc5: 39.5833 (49.2694)  time: 0.2181  data: 0.1736  max mem: 3726
Test:  [230/531]  eta: 0:00:53  loss: 4.1523 (4.2268)  acc1: 28.1250 (29.1486)  acc5: 48.9583 (49.1613)  time: 0.1897  data: 0.1477  max mem: 3726
Test:  [240/531]  eta: 0:00:51  loss: 4.1774 (4.2357)  acc1: 27.0833 (29.0284)  acc5: 47.9167 (48.9281)  time: 0.1836  data: 0.1416  max mem: 3726
Test:  [250/531]  eta: 0:00:50  loss: 4.5285 (4.2622)  acc1: 18.7500 (28.6106)  acc5: 41.6667 (48.4354)  time: 0.2316  data: 0.1870  max mem: 3726
Test:  [260/531]  eta: 0:00:49  loss: 4.3426 (4.2666)  acc1: 20.8333 (28.4203)  acc5: 41.6667 (48.2320)  time: 0.2250  data: 0.1781  max mem: 3726
Test:  [270/531]  eta: 0:00:47  loss: 3.9200 (4.2532)  acc1: 26.0417 (28.6862)  acc5: 53.1250 (48.3779)  time: 0.1897  data: 0.1442  max mem: 3726
Test:  [280/531]  eta: 0:00:45  loss: 3.4803 (4.2303)  acc1: 37.5000 (29.0443)  acc5: 57.2917 (48.6136)  time: 0.1882  data: 0.1441  max mem: 3726
Test:  [290/531]  eta: 0:00:43  loss: 3.6880 (4.2318)  acc1: 31.2500 (29.0557)  acc5: 51.0417 (48.5682)  time: 0.1760  data: 0.1279  max mem: 3726
Test:  [300/531]  eta: 0:00:41  loss: 4.0258 (4.2236)  acc1: 29.1667 (29.1667)  acc5: 46.8750 (48.6226)  time: 0.1616  data: 0.0923  max mem: 3726
Test:  [310/531]  eta: 0:00:40  loss: 4.0258 (4.2210)  acc1: 33.3333 (29.3140)  acc5: 46.8750 (48.6200)  time: 0.1784  data: 0.1105  max mem: 3726
Test:  [320/531]  eta: 0:00:38  loss: 3.9759 (4.2126)  acc1: 33.3333 (29.5074)  acc5: 46.8750 (48.6468)  time: 0.2014  data: 0.1458  max mem: 3726
Test:  [330/531]  eta: 0:00:36  loss: 3.9548 (4.2066)  acc1: 28.1250 (29.4310)  acc5: 46.8750 (48.6247)  time: 0.1818  data: 0.1148  max mem: 3726
Test:  [340/531]  eta: 0:00:34  loss: 4.3728 (4.2233)  acc1: 19.7917 (29.1270)  acc5: 37.5000 (48.3138)  time: 0.1648  data: 0.0934  max mem: 3726
Test:  [350/531]  eta: 0:00:32  loss: 4.5503 (4.2336)  acc1: 16.6667 (29.0034)  acc5: 35.4167 (48.0710)  time: 0.1754  data: 0.1118  max mem: 3726
Test:  [360/531]  eta: 0:00:31  loss: 4.5975 (4.2468)  acc1: 17.7083 (28.7252)  acc5: 34.3750 (47.7810)  time: 0.2067  data: 0.1369  max mem: 3726
Test:  [370/531]  eta: 0:00:29  loss: 4.5937 (4.2537)  acc1: 16.6667 (28.6107)  acc5: 37.5000 (47.6106)  time: 0.2199  data: 0.1572  max mem: 3726
Test:  [380/531]  eta: 0:00:27  loss: 4.5369 (4.2477)  acc1: 23.9583 (28.6773)  acc5: 45.8333 (47.6843)  time: 0.2066  data: 0.1633  max mem: 3726
Test:  [390/531]  eta: 0:00:25  loss: 4.6891 (4.2688)  acc1: 20.8333 (28.4687)  acc5: 40.6250 (47.2613)  time: 0.1661  data: 0.1223  max mem: 3726
Test:  [400/531]  eta: 0:00:23  loss: 4.8897 (4.2750)  acc1: 19.7917 (28.4081)  acc5: 32.2917 (47.1192)  time: 0.1629  data: 0.1200  max mem: 3726
Test:  [410/531]  eta: 0:00:21  loss: 3.9680 (4.2688)  acc1: 34.3750 (28.5508)  acc5: 43.7500 (47.1741)  time: 0.1663  data: 0.1234  max mem: 3726
Test:  [420/531]  eta: 0:00:20  loss: 4.1769 (4.2713)  acc1: 27.0833 (28.4788)  acc5: 40.6250 (47.0655)  time: 0.1439  data: 0.1006  max mem: 3726
Test:  [430/531]  eta: 0:00:18  loss: 4.4623 (4.2799)  acc1: 21.8750 (28.3594)  acc5: 40.6250 (46.9064)  time: 0.1755  data: 0.1327  max mem: 3726
Test:  [440/531]  eta: 0:00:16  loss: 4.0043 (4.2708)  acc1: 27.0833 (28.4793)  acc5: 50.0000 (47.0191)  time: 0.2193  data: 0.1763  max mem: 3726
Test:  [450/531]  eta: 0:00:14  loss: 3.8481 (4.2642)  acc1: 29.1667 (28.4969)  acc5: 50.0000 (47.1383)  time: 0.2489  data: 0.2052  max mem: 3726
Test:  [460/531]  eta: 0:00:13  loss: 4.5701 (4.2761)  acc1: 22.9167 (28.3238)  acc5: 40.6250 (46.8456)  time: 0.2272  data: 0.1832  max mem: 3726
Test:  [470/531]  eta: 0:00:11  loss: 4.4763 (4.2660)  acc1: 25.0000 (28.5408)  acc5: 41.6667 (46.9723)  time: 0.1684  data: 0.1251  max mem: 3726
Test:  [480/531]  eta: 0:00:09  loss: 3.9550 (4.2626)  acc1: 27.0833 (28.4758)  acc5: 45.8333 (47.0201)  time: 0.1455  data: 0.1030  max mem: 3726
Test:  [490/531]  eta: 0:00:07  loss: 4.3486 (4.2712)  acc1: 20.8333 (28.3881)  acc5: 40.6250 (46.8389)  time: 0.2274  data: 0.1840  max mem: 3726
Test:  [500/531]  eta: 0:00:05  loss: 5.5360 (4.3175)  acc1: 10.4167 (28.0106)  acc5: 26.0417 (46.2263)  time: 0.2531  data: 0.2084  max mem: 3726
Test:  [510/531]  eta: 0:00:03  loss: 6.6040 (4.3578)  acc1: 4.1667 (27.6378)  acc5: 11.4583 (45.6560)  time: 0.2052  data: 0.1620  max mem: 3726
Test:  [520/531]  eta: 0:00:02  loss: 5.9831 (4.3806)  acc1: 8.3333 (27.3732)  acc5: 21.8750 (45.2955)  time: 0.2069  data: 0.1649  max mem: 3726
Test:  [530/531]  eta: 0:00:00  loss: 5.7592 (4.4159)  acc1: 8.3333 (27.0019)  acc5: 14.5833 (44.7582)  time: 0.2154  data: 0.1703  max mem: 3726
Test: Total time: 0:01:39 (0.1875 s / it)
* Acc@1 27.002 Acc@5 44.758 loss 4.416
Accuracy of the network on 50889 test images: 27.00191%
Not using distributed mode
Namespace(Decom=False, aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=64, bn=False, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/sliu/project_space/datasets/sketch', data_set='IMNET', data_type='imagenet-s', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.1, enable_wandb=False, epochs=300, eval=True, eval_data_path=None, finetune='', fix=False, growth='random', head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, kernel_size=[7, 7, 7, 7, 100], layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=-1, log_dir=None, lr=0.004, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='resnet50', model_ema=False, model_ema_decay=0.9999, model_ema_eval=False, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_workers=10, only_L=False, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', parallel=True, pin_mem=True, project='SLaK', prune='magnitude', prune_rate=0.3, recount=1, redistribution='none', remode='pixel', reprob=0.25, resplit=False, resume='/gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_ConvNeXtTeacher_STRN50_NKD/checkpoint-best.pth', save_ckpt=True, save_ckpt_freq=10, save_ckpt_num=3, seed=0, smoothing=0.1, sparse=False, sparse_init='snip', sparsity=0.4, start_epoch=0, train_interpolation='bicubic', update_freq=1, update_frequency=100, use_amp=False, verbose=False, wandb_ckpt=False, warmup_epochs=20, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, width_factor=1.0, world_size=1)
Transform = 
Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /home/sliu/project_space/datasets/sketch
Number of the class = 1000
Mixup is activated!
Model = ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=True)
  (fc): Linear(in_features=2048, out_features=1000, bias=True)
)
number of params: 25557032
LR = 0.00400000
Batch size = 64
Update frequent = 1
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "conv1.weight",
      "layer1.0.conv1.weight",
      "layer1.0.conv2.weight",
      "layer1.0.conv3.weight",
      "layer1.0.downsample.0.weight",
      "layer1.1.conv1.weight",
      "layer1.1.conv2.weight",
      "layer1.1.conv3.weight",
      "layer1.2.conv1.weight",
      "layer1.2.conv2.weight",
      "layer1.2.conv3.weight",
      "layer2.0.conv1.weight",
      "layer2.0.conv2.weight",
      "layer2.0.conv3.weight",
      "layer2.0.downsample.0.weight",
      "layer2.1.conv1.weight",
      "layer2.1.conv2.weight",
      "layer2.1.conv3.weight",
      "layer2.2.conv1.weight",
      "layer2.2.conv2.weight",
      "layer2.2.conv3.weight",
      "layer2.3.conv1.weight",
      "layer2.3.conv2.weight",
      "layer2.3.conv3.weight",
      "layer3.0.conv1.weight",
      "layer3.0.conv2.weight",
      "layer3.0.conv3.weight",
      "layer3.0.downsample.0.weight",
      "layer3.1.conv1.weight",
      "layer3.1.conv2.weight",
      "layer3.1.conv3.weight",
      "layer3.2.conv1.weight",
      "layer3.2.conv2.weight",
      "layer3.2.conv3.weight",
      "layer3.3.conv1.weight",
      "layer3.3.conv2.weight",
      "layer3.3.conv3.weight",
      "layer3.4.conv1.weight",
      "layer3.4.conv2.weight",
      "layer3.4.conv3.weight",
      "layer3.5.conv1.weight",
      "layer3.5.conv2.weight",
      "layer3.5.conv3.weight",
      "layer4.0.conv1.weight",
      "layer4.0.conv2.weight",
      "layer4.0.conv3.weight",
      "layer4.0.downsample.0.weight",
      "layer4.1.conv1.weight",
      "layer4.1.conv2.weight",
      "layer4.1.conv3.weight",
      "layer4.2.conv1.weight",
      "layer4.2.conv2.weight",
      "layer4.2.conv3.weight",
      "fc.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "bn1.weight",
      "bn1.bias",
      "layer1.0.bn1.weight",
      "layer1.0.bn1.bias",
      "layer1.0.bn2.weight",
      "layer1.0.bn2.bias",
      "layer1.0.bn3.weight",
      "layer1.0.bn3.bias",
      "layer1.0.downsample.1.weight",
      "layer1.0.downsample.1.bias",
      "layer1.1.bn1.weight",
      "layer1.1.bn1.bias",
      "layer1.1.bn2.weight",
      "layer1.1.bn2.bias",
      "layer1.1.bn3.weight",
      "layer1.1.bn3.bias",
      "layer1.2.bn1.weight",
      "layer1.2.bn1.bias",
      "layer1.2.bn2.weight",
      "layer1.2.bn2.bias",
      "layer1.2.bn3.weight",
      "layer1.2.bn3.bias",
      "layer2.0.bn1.weight",
      "layer2.0.bn1.bias",
      "layer2.0.bn2.weight",
      "layer2.0.bn2.bias",
      "layer2.0.bn3.weight",
      "layer2.0.bn3.bias",
      "layer2.0.downsample.1.weight",
      "layer2.0.downsample.1.bias",
      "layer2.1.bn1.weight",
      "layer2.1.bn1.bias",
      "layer2.1.bn2.weight",
      "layer2.1.bn2.bias",
      "layer2.1.bn3.weight",
      "layer2.1.bn3.bias",
      "layer2.2.bn1.weight",
      "layer2.2.bn1.bias",
      "layer2.2.bn2.weight",
      "layer2.2.bn2.bias",
      "layer2.2.bn3.weight",
      "layer2.2.bn3.bias",
      "layer2.3.bn1.weight",
      "layer2.3.bn1.bias",
      "layer2.3.bn2.weight",
      "layer2.3.bn2.bias",
      "layer2.3.bn3.weight",
      "layer2.3.bn3.bias",
      "layer3.0.bn1.weight",
      "layer3.0.bn1.bias",
      "layer3.0.bn2.weight",
      "layer3.0.bn2.bias",
      "layer3.0.bn3.weight",
      "layer3.0.bn3.bias",
      "layer3.0.downsample.1.weight",
      "layer3.0.downsample.1.bias",
      "layer3.1.bn1.weight",
      "layer3.1.bn1.bias",
      "layer3.1.bn2.weight",
      "layer3.1.bn2.bias",
      "layer3.1.bn3.weight",
      "layer3.1.bn3.bias",
      "layer3.2.bn1.weight",
      "layer3.2.bn1.bias",
      "layer3.2.bn2.weight",
      "layer3.2.bn2.bias",
      "layer3.2.bn3.weight",
      "layer3.2.bn3.bias",
      "layer3.3.bn1.weight",
      "layer3.3.bn1.bias",
      "layer3.3.bn2.weight",
      "layer3.3.bn2.bias",
      "layer3.3.bn3.weight",
      "layer3.3.bn3.bias",
      "layer3.4.bn1.weight",
      "layer3.4.bn1.bias",
      "layer3.4.bn2.weight",
      "layer3.4.bn2.bias",
      "layer3.4.bn3.weight",
      "layer3.4.bn3.bias",
      "layer3.5.bn1.weight",
      "layer3.5.bn1.bias",
      "layer3.5.bn2.weight",
      "layer3.5.bn2.bias",
      "layer3.5.bn3.weight",
      "layer3.5.bn3.bias",
      "layer4.0.bn1.weight",
      "layer4.0.bn1.bias",
      "layer4.0.bn2.weight",
      "layer4.0.bn2.bias",
      "layer4.0.bn3.weight",
      "layer4.0.bn3.bias",
      "layer4.0.downsample.1.weight",
      "layer4.0.downsample.1.bias",
      "layer4.1.bn1.weight",
      "layer4.1.bn1.bias",
      "layer4.1.bn2.weight",
      "layer4.1.bn2.bias",
      "layer4.1.bn3.weight",
      "layer4.1.bn3.bias",
      "layer4.2.bn1.weight",
      "layer4.2.bn1.bias",
      "layer4.2.bn2.weight",
      "layer4.2.bn2.bias",
      "layer4.2.bn3.weight",
      "layer4.2.bn3.bias",
      "fc.bias"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
criterion = SoftTargetCrossEntropy()
dict_keys(['model', 'optimizer', 'epoch', 'scaler', 'args', 'model_ema'])
Resume checkpoint /gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_ConvNeXtTeacher_STRN50_NKD/checkpoint-best.pth
With optim & sched!
conv1.weight density is 1.0
bn1.weight density is 1.0
bn1.bias density is 1.0
layer1.0.conv1.weight density is 1.0
layer1.0.bn1.weight density is 1.0
layer1.0.bn1.bias density is 1.0
layer1.0.conv2.weight density is 1.0
layer1.0.bn2.weight density is 1.0
layer1.0.bn2.bias density is 1.0
layer1.0.conv3.weight density is 1.0
layer1.0.bn3.weight density is 1.0
layer1.0.bn3.bias density is 1.0
layer1.0.downsample.0.weight density is 1.0
layer1.0.downsample.1.weight density is 1.0
layer1.0.downsample.1.bias density is 1.0
layer1.1.conv1.weight density is 1.0
layer1.1.bn1.weight density is 1.0
layer1.1.bn1.bias density is 1.0
layer1.1.conv2.weight density is 1.0
layer1.1.bn2.weight density is 1.0
layer1.1.bn2.bias density is 1.0
layer1.1.conv3.weight density is 1.0
layer1.1.bn3.weight density is 1.0
layer1.1.bn3.bias density is 1.0
layer1.2.conv1.weight density is 1.0
layer1.2.bn1.weight density is 1.0
layer1.2.bn1.bias density is 1.0
layer1.2.conv2.weight density is 1.0
layer1.2.bn2.weight density is 1.0
layer1.2.bn2.bias density is 1.0
layer1.2.conv3.weight density is 1.0
layer1.2.bn3.weight density is 1.0
layer1.2.bn3.bias density is 1.0
layer2.0.conv1.weight density is 1.0
layer2.0.bn1.weight density is 1.0
layer2.0.bn1.bias density is 1.0
layer2.0.conv2.weight density is 1.0
layer2.0.bn2.weight density is 1.0
layer2.0.bn2.bias density is 1.0
layer2.0.conv3.weight density is 1.0
layer2.0.bn3.weight density is 1.0
layer2.0.bn3.bias density is 1.0
layer2.0.downsample.0.weight density is 1.0
layer2.0.downsample.1.weight density is 1.0
layer2.0.downsample.1.bias density is 1.0
layer2.1.conv1.weight density is 1.0
layer2.1.bn1.weight density is 1.0
layer2.1.bn1.bias density is 1.0
layer2.1.conv2.weight density is 1.0
layer2.1.bn2.weight density is 1.0
layer2.1.bn2.bias density is 1.0
layer2.1.conv3.weight density is 1.0
layer2.1.bn3.weight density is 1.0
layer2.1.bn3.bias density is 1.0
layer2.2.conv1.weight density is 1.0
layer2.2.bn1.weight density is 1.0
layer2.2.bn1.bias density is 1.0
layer2.2.conv2.weight density is 1.0
layer2.2.bn2.weight density is 1.0
layer2.2.bn2.bias density is 1.0
layer2.2.conv3.weight density is 1.0
layer2.2.bn3.weight density is 1.0
layer2.2.bn3.bias density is 1.0
layer2.3.conv1.weight density is 1.0
layer2.3.bn1.weight density is 1.0
layer2.3.bn1.bias density is 1.0
layer2.3.conv2.weight density is 1.0
layer2.3.bn2.weight density is 1.0
layer2.3.bn2.bias density is 1.0
layer2.3.conv3.weight density is 1.0
layer2.3.bn3.weight density is 1.0
layer2.3.bn3.bias density is 1.0
layer3.0.conv1.weight density is 1.0
layer3.0.bn1.weight density is 1.0
layer3.0.bn1.bias density is 1.0
layer3.0.conv2.weight density is 1.0
layer3.0.bn2.weight density is 1.0
layer3.0.bn2.bias density is 1.0
layer3.0.conv3.weight density is 1.0
layer3.0.bn3.weight density is 1.0
layer3.0.bn3.bias density is 1.0
layer3.0.downsample.0.weight density is 1.0
layer3.0.downsample.1.weight density is 1.0
layer3.0.downsample.1.bias density is 1.0
layer3.1.conv1.weight density is 1.0
layer3.1.bn1.weight density is 1.0
layer3.1.bn1.bias density is 1.0
layer3.1.conv2.weight density is 1.0
layer3.1.bn2.weight density is 1.0
layer3.1.bn2.bias density is 1.0
layer3.1.conv3.weight density is 1.0
layer3.1.bn3.weight density is 1.0
layer3.1.bn3.bias density is 1.0
layer3.2.conv1.weight density is 1.0
layer3.2.bn1.weight density is 1.0
layer3.2.bn1.bias density is 1.0
layer3.2.conv2.weight density is 1.0
layer3.2.bn2.weight density is 1.0
layer3.2.bn2.bias density is 1.0
layer3.2.conv3.weight density is 1.0
layer3.2.bn3.weight density is 1.0
layer3.2.bn3.bias density is 1.0
layer3.3.conv1.weight density is 1.0
layer3.3.bn1.weight density is 1.0
layer3.3.bn1.bias density is 1.0
layer3.3.conv2.weight density is 1.0
layer3.3.bn2.weight density is 1.0
layer3.3.bn2.bias density is 1.0
layer3.3.conv3.weight density is 1.0
layer3.3.bn3.weight density is 1.0
layer3.3.bn3.bias density is 1.0
layer3.4.conv1.weight density is 1.0
layer3.4.bn1.weight density is 1.0
layer3.4.bn1.bias density is 1.0
layer3.4.conv2.weight density is 1.0
layer3.4.bn2.weight density is 1.0
layer3.4.bn2.bias density is 1.0
layer3.4.conv3.weight density is 1.0
layer3.4.bn3.weight density is 1.0
layer3.4.bn3.bias density is 1.0
layer3.5.conv1.weight density is 1.0
layer3.5.bn1.weight density is 1.0
layer3.5.bn1.bias density is 1.0
layer3.5.conv2.weight density is 1.0
layer3.5.bn2.weight density is 1.0
layer3.5.bn2.bias density is 1.0
layer3.5.conv3.weight density is 1.0
layer3.5.bn3.weight density is 1.0
layer3.5.bn3.bias density is 1.0
layer4.0.conv1.weight density is 1.0
layer4.0.bn1.weight density is 1.0
layer4.0.bn1.bias density is 1.0
layer4.0.conv2.weight density is 1.0
layer4.0.bn2.weight density is 1.0
layer4.0.bn2.bias density is 1.0
layer4.0.conv3.weight density is 1.0
layer4.0.bn3.weight density is 1.0
layer4.0.bn3.bias density is 1.0
layer4.0.downsample.0.weight density is 1.0
layer4.0.downsample.1.weight density is 1.0
layer4.0.downsample.1.bias density is 1.0
layer4.1.conv1.weight density is 1.0
layer4.1.bn1.weight density is 1.0
layer4.1.bn1.bias density is 1.0
layer4.1.conv2.weight density is 1.0
layer4.1.bn2.weight density is 1.0
layer4.1.bn2.bias density is 1.0
layer4.1.conv3.weight density is 1.0
layer4.1.bn3.weight density is 1.0
layer4.1.bn3.bias density is 1.0
layer4.2.conv1.weight density is 1.0
layer4.2.bn1.weight density is 1.0
layer4.2.bn1.bias density is 1.0
layer4.2.conv2.weight density is 1.0
layer4.2.bn2.weight density is 1.0
layer4.2.bn2.bias density is 1.0
layer4.2.conv3.weight density is 1.0
layer4.2.bn3.weight density is 1.0
layer4.2.bn3.bias density is 1.0
fc.weight density is 1.0
fc.bias density is 1.0
Test:  [  0/531]  eta: 0:33:06  loss: 5.4356 (5.4356)  acc1: 10.4167 (10.4167)  acc5: 33.3333 (33.3333)  time: 3.7415  data: 2.3983  max mem: 3726
Test:  [ 10/531]  eta: 0:03:40  loss: 4.7762 (4.3940)  acc1: 25.0000 (29.2614)  acc5: 40.6250 (46.1174)  time: 0.4223  data: 0.2599  max mem: 3726
Test:  [ 20/531]  eta: 0:02:21  loss: 4.1201 (4.2783)  acc1: 25.0000 (27.2817)  acc5: 46.8750 (48.2639)  time: 0.1031  data: 0.0599  max mem: 3726
Test:  [ 30/531]  eta: 0:01:56  loss: 4.4639 (4.6775)  acc1: 23.9583 (23.9583)  acc5: 45.8333 (43.1452)  time: 0.1287  data: 0.0868  max mem: 3726
Test:  [ 40/531]  eta: 0:01:42  loss: 4.8438 (4.6971)  acc1: 17.7083 (24.0346)  acc5: 34.3750 (42.4797)  time: 0.1395  data: 0.0968  max mem: 3726
Test:  [ 50/531]  eta: 0:01:39  loss: 4.4145 (4.5826)  acc1: 28.1250 (25.5719)  acc5: 45.8333 (44.0972)  time: 0.1658  data: 0.1228  max mem: 3726
Test:  [ 60/531]  eta: 0:01:33  loss: 4.4496 (4.6068)  acc1: 29.1667 (25.8197)  acc5: 44.7917 (43.7158)  time: 0.1774  data: 0.1343  max mem: 3726
Test:  [ 70/531]  eta: 0:01:43  loss: 4.4945 (4.6163)  acc1: 23.9583 (25.5428)  acc5: 39.5833 (43.5886)  time: 0.2672  data: 0.2233  max mem: 3726
Test:  [ 80/531]  eta: 0:01:38  loss: 4.2526 (4.5974)  acc1: 23.9583 (25.8102)  acc5: 41.6667 (43.4414)  time: 0.2806  data: 0.2375  max mem: 3726
Test:  [ 90/531]  eta: 0:01:31  loss: 3.9287 (4.4990)  acc1: 28.1250 (26.6255)  acc5: 48.9583 (45.0893)  time: 0.1508  data: 0.1081  max mem: 3726
Test:  [100/531]  eta: 0:01:24  loss: 3.2777 (4.3957)  acc1: 37.5000 (27.7125)  acc5: 64.5833 (46.8441)  time: 0.1063  data: 0.0637  max mem: 3726
Test:  [110/531]  eta: 0:01:20  loss: 3.2020 (4.2923)  acc1: 37.5000 (28.4628)  acc5: 64.5833 (48.7425)  time: 0.1190  data: 0.0763  max mem: 3726
Test:  [120/531]  eta: 0:01:15  loss: 3.1864 (4.2248)  acc1: 35.4167 (28.9945)  acc5: 66.6667 (50.0172)  time: 0.1101  data: 0.0674  max mem: 3726
Test:  [130/531]  eta: 0:01:13  loss: 3.5323 (4.1855)  acc1: 31.2500 (29.2780)  acc5: 59.3750 (50.6043)  time: 0.1384  data: 0.0961  max mem: 3726
Test:  [140/531]  eta: 0:01:11  loss: 3.3951 (4.1212)  acc1: 31.2500 (30.0827)  acc5: 64.5833 (51.6992)  time: 0.1900  data: 0.1477  max mem: 3726
Test:  [150/531]  eta: 0:01:10  loss: 3.4720 (4.1262)  acc1: 28.1250 (29.9531)  acc5: 61.4583 (51.7108)  time: 0.1921  data: 0.1501  max mem: 3726
Test:  [160/531]  eta: 0:01:09  loss: 3.9943 (4.1261)  acc1: 28.1250 (30.3119)  acc5: 53.1250 (51.7210)  time: 0.2111  data: 0.1684  max mem: 3726
Test:  [170/531]  eta: 0:01:06  loss: 4.2177 (4.1419)  acc1: 28.1250 (30.2266)  acc5: 48.9583 (51.5168)  time: 0.1806  data: 0.1376  max mem: 3726
Test:  [180/531]  eta: 0:01:04  loss: 4.5871 (4.1316)  acc1: 27.0833 (30.5536)  acc5: 43.7500 (51.5999)  time: 0.1465  data: 0.1038  max mem: 3726
Test:  [190/531]  eta: 0:01:03  loss: 3.8741 (4.1269)  acc1: 30.2083 (30.7155)  acc5: 54.1667 (51.7507)  time: 0.1909  data: 0.1481  max mem: 3726
Test:  [200/531]  eta: 0:01:00  loss: 3.8741 (4.1310)  acc1: 30.2083 (30.7680)  acc5: 54.1667 (51.5392)  time: 0.1966  data: 0.1538  max mem: 3726
Test:  [210/531]  eta: 0:00:59  loss: 4.5806 (4.1753)  acc1: 20.8333 (30.2923)  acc5: 41.6667 (50.8442)  time: 0.1822  data: 0.1399  max mem: 3726
Test:  [220/531]  eta: 0:00:58  loss: 4.3904 (4.1772)  acc1: 28.1250 (30.4063)  acc5: 41.6667 (50.6127)  time: 0.2301  data: 0.1882  max mem: 3726
Test:  [230/531]  eta: 0:00:56  loss: 4.1726 (4.1787)  acc1: 30.2083 (30.3256)  acc5: 46.8750 (50.3833)  time: 0.2043  data: 0.1623  max mem: 3726
Test:  [240/531]  eta: 0:00:54  loss: 4.1726 (4.1928)  acc1: 29.1667 (30.1089)  acc5: 46.8750 (50.0778)  time: 0.1871  data: 0.1448  max mem: 3726
Test:  [250/531]  eta: 0:00:53  loss: 4.4040 (4.2237)  acc1: 17.7083 (29.7103)  acc5: 40.6250 (49.4480)  time: 0.2380  data: 0.1949  max mem: 3726
Test:  [260/531]  eta: 0:00:51  loss: 4.5354 (4.2279)  acc1: 18.7500 (29.4381)  acc5: 38.5417 (49.2217)  time: 0.2154  data: 0.1722  max mem: 3726
Test:  [270/531]  eta: 0:00:49  loss: 3.9021 (4.2160)  acc1: 25.0000 (29.6625)  acc5: 51.0417 (49.3542)  time: 0.1961  data: 0.1531  max mem: 3726
Test:  [280/531]  eta: 0:00:48  loss: 3.4838 (4.1898)  acc1: 36.4583 (30.0452)  acc5: 57.2917 (49.5811)  time: 0.2199  data: 0.1769  max mem: 3726
Test:  [290/531]  eta: 0:00:45  loss: 3.6365 (4.1887)  acc1: 31.2500 (29.9971)  acc5: 50.0000 (49.5132)  time: 0.1796  data: 0.1373  max mem: 3726
Test:  [300/531]  eta: 0:00:43  loss: 3.9236 (4.1821)  acc1: 29.1667 (30.1322)  acc5: 44.7917 (49.5120)  time: 0.1474  data: 0.1053  max mem: 3726
Test:  [310/531]  eta: 0:00:42  loss: 3.8836 (4.1766)  acc1: 31.2500 (30.3323)  acc5: 48.9583 (49.5512)  time: 0.1951  data: 0.1527  max mem: 3726
Test:  [320/531]  eta: 0:00:40  loss: 3.9692 (4.1690)  acc1: 35.4167 (30.4355)  acc5: 46.8750 (49.5327)  time: 0.2124  data: 0.1701  max mem: 3726
Test:  [330/531]  eta: 0:00:37  loss: 3.9692 (4.1622)  acc1: 31.2500 (30.3342)  acc5: 43.7500 (49.4713)  time: 0.1635  data: 0.1209  max mem: 3726
Test:  [340/531]  eta: 0:00:36  loss: 4.2636 (4.1792)  acc1: 15.6250 (29.9914)  acc5: 39.5833 (49.1202)  time: 0.1599  data: 0.1169  max mem: 3726
Test:  [350/531]  eta: 0:00:34  loss: 4.7887 (4.1895)  acc1: 14.5833 (29.8552)  acc5: 38.5417 (48.8901)  time: 0.1862  data: 0.1439  max mem: 3726
Test:  [360/531]  eta: 0:00:32  loss: 4.5802 (4.2002)  acc1: 15.6250 (29.5620)  acc5: 38.5417 (48.6756)  time: 0.1672  data: 0.1249  max mem: 3726
Test:  [370/531]  eta: 0:00:30  loss: 4.5166 (4.2099)  acc1: 15.6250 (29.4615)  acc5: 41.6667 (48.5175)  time: 0.1715  data: 0.1288  max mem: 3726
Test:  [380/531]  eta: 0:00:28  loss: 4.3605 (4.2068)  acc1: 27.0833 (29.5057)  acc5: 43.7500 (48.5127)  time: 0.1777  data: 0.1353  max mem: 3726
Test:  [390/531]  eta: 0:00:26  loss: 4.4742 (4.2252)  acc1: 27.0833 (29.2839)  acc5: 42.7083 (48.1484)  time: 0.1563  data: 0.1136  max mem: 3726
Test:  [400/531]  eta: 0:00:24  loss: 4.8830 (4.2347)  acc1: 18.7500 (29.1952)  acc5: 34.3750 (47.9634)  time: 0.1903  data: 0.1473  max mem: 3726
Test:  [410/531]  eta: 0:00:22  loss: 3.8186 (4.2298)  acc1: 34.3750 (29.3289)  acc5: 43.7500 (48.0180)  time: 0.2073  data: 0.1646  max mem: 3726
Test:  [420/531]  eta: 0:00:20  loss: 4.4930 (4.2359)  acc1: 25.0000 (29.2310)  acc5: 42.7083 (47.9092)  time: 0.1877  data: 0.1447  max mem: 3726
Test:  [430/531]  eta: 0:00:18  loss: 4.4930 (4.2469)  acc1: 22.9167 (29.1207)  acc5: 41.6667 (47.7330)  time: 0.1801  data: 0.1374  max mem: 3726
Test:  [440/531]  eta: 0:00:17  loss: 3.9168 (4.2365)  acc1: 27.0833 (29.2989)  acc5: 46.8750 (47.8434)  time: 0.1940  data: 0.1520  max mem: 3726
Test:  [450/531]  eta: 0:00:15  loss: 3.9011 (4.2278)  acc1: 37.5000 (29.3399)  acc5: 51.0417 (47.9582)  time: 0.2152  data: 0.1729  max mem: 3726
Test:  [460/531]  eta: 0:00:13  loss: 4.2230 (4.2401)  acc1: 20.8333 (29.1418)  acc5: 42.7083 (47.6794)  time: 0.1983  data: 0.1557  max mem: 3726
Test:  [470/531]  eta: 0:00:11  loss: 4.2686 (4.2297)  acc1: 22.9167 (29.3547)  acc5: 38.5417 (47.7818)  time: 0.1826  data: 0.1399  max mem: 3726
Test:  [480/531]  eta: 0:00:09  loss: 4.1648 (4.2278)  acc1: 26.0417 (29.2576)  acc5: 50.0000 (47.8170)  time: 0.1755  data: 0.1332  max mem: 3726
Test:  [490/531]  eta: 0:00:07  loss: 4.5041 (4.2409)  acc1: 20.8333 (29.1476)  acc5: 37.5000 (47.6366)  time: 0.1851  data: 0.1432  max mem: 3726
Test:  [500/531]  eta: 0:00:05  loss: 5.4416 (4.2855)  acc1: 12.5000 (28.7924)  acc5: 29.1667 (47.0185)  time: 0.2094  data: 0.1661  max mem: 3726
Test:  [510/531]  eta: 0:00:03  loss: 6.5143 (4.3260)  acc1: 6.2500 (28.4124)  acc5: 10.4167 (46.4428)  time: 0.2192  data: 0.1753  max mem: 3726
Test:  [520/531]  eta: 0:00:02  loss: 6.1415 (4.3522)  acc1: 8.3333 (28.1070)  acc5: 18.7500 (46.0553)  time: 0.2056  data: 0.1622  max mem: 3726
Test:  [530/531]  eta: 0:00:00  loss: 6.1162 (4.3876)  acc1: 6.2500 (27.7388)  acc5: 19.7917 (45.5226)  time: 0.2021  data: 0.1565  max mem: 3726
Test: Total time: 0:01:40 (0.1897 s / it)
* Acc@1 27.739 Acc@5 45.523 loss 4.388
Accuracy of the network on 50889 test images: 27.73880%
Not using distributed mode
Namespace(Decom=False, aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=64, bn=False, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/sliu/project_space/datasets/sketch', data_set='IMNET', data_type='imagenet-s', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.1, enable_wandb=False, epochs=300, eval=True, eval_data_path=None, finetune='', fix=False, growth='random', head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, kernel_size=[7, 7, 7, 7, 100], layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=-1, log_dir=None, lr=0.004, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='resnet50', model_ema=False, model_ema_decay=0.9999, model_ema_eval=False, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_workers=10, only_L=False, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', parallel=True, pin_mem=True, project='SLaK', prune='magnitude', prune_rate=0.3, recount=1, redistribution='none', remode='pixel', reprob=0.25, resplit=False, resume='/gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_SLakTTeachers_STRN50_NKD/checkpoint-best.pth', save_ckpt=True, save_ckpt_freq=10, save_ckpt_num=3, seed=0, smoothing=0.1, sparse=False, sparse_init='snip', sparsity=0.4, start_epoch=0, train_interpolation='bicubic', update_freq=1, update_frequency=100, use_amp=False, verbose=False, wandb_ckpt=False, warmup_epochs=20, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, width_factor=1.0, world_size=1)
Transform = 
Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /home/sliu/project_space/datasets/sketch
Number of the class = 1000
Mixup is activated!
Model = ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=True)
  (fc): Linear(in_features=2048, out_features=1000, bias=True)
)
number of params: 25557032
LR = 0.00400000
Batch size = 64
Update frequent = 1
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "conv1.weight",
      "layer1.0.conv1.weight",
      "layer1.0.conv2.weight",
      "layer1.0.conv3.weight",
      "layer1.0.downsample.0.weight",
      "layer1.1.conv1.weight",
      "layer1.1.conv2.weight",
      "layer1.1.conv3.weight",
      "layer1.2.conv1.weight",
      "layer1.2.conv2.weight",
      "layer1.2.conv3.weight",
      "layer2.0.conv1.weight",
      "layer2.0.conv2.weight",
      "layer2.0.conv3.weight",
      "layer2.0.downsample.0.weight",
      "layer2.1.conv1.weight",
      "layer2.1.conv2.weight",
      "layer2.1.conv3.weight",
      "layer2.2.conv1.weight",
      "layer2.2.conv2.weight",
      "layer2.2.conv3.weight",
      "layer2.3.conv1.weight",
      "layer2.3.conv2.weight",
      "layer2.3.conv3.weight",
      "layer3.0.conv1.weight",
      "layer3.0.conv2.weight",
      "layer3.0.conv3.weight",
      "layer3.0.downsample.0.weight",
      "layer3.1.conv1.weight",
      "layer3.1.conv2.weight",
      "layer3.1.conv3.weight",
      "layer3.2.conv1.weight",
      "layer3.2.conv2.weight",
      "layer3.2.conv3.weight",
      "layer3.3.conv1.weight",
      "layer3.3.conv2.weight",
      "layer3.3.conv3.weight",
      "layer3.4.conv1.weight",
      "layer3.4.conv2.weight",
      "layer3.4.conv3.weight",
      "layer3.5.conv1.weight",
      "layer3.5.conv2.weight",
      "layer3.5.conv3.weight",
      "layer4.0.conv1.weight",
      "layer4.0.conv2.weight",
      "layer4.0.conv3.weight",
      "layer4.0.downsample.0.weight",
      "layer4.1.conv1.weight",
      "layer4.1.conv2.weight",
      "layer4.1.conv3.weight",
      "layer4.2.conv1.weight",
      "layer4.2.conv2.weight",
      "layer4.2.conv3.weight",
      "fc.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "bn1.weight",
      "bn1.bias",
      "layer1.0.bn1.weight",
      "layer1.0.bn1.bias",
      "layer1.0.bn2.weight",
      "layer1.0.bn2.bias",
      "layer1.0.bn3.weight",
      "layer1.0.bn3.bias",
      "layer1.0.downsample.1.weight",
      "layer1.0.downsample.1.bias",
      "layer1.1.bn1.weight",
      "layer1.1.bn1.bias",
      "layer1.1.bn2.weight",
      "layer1.1.bn2.bias",
      "layer1.1.bn3.weight",
      "layer1.1.bn3.bias",
      "layer1.2.bn1.weight",
      "layer1.2.bn1.bias",
      "layer1.2.bn2.weight",
      "layer1.2.bn2.bias",
      "layer1.2.bn3.weight",
      "layer1.2.bn3.bias",
      "layer2.0.bn1.weight",
      "layer2.0.bn1.bias",
      "layer2.0.bn2.weight",
      "layer2.0.bn2.bias",
      "layer2.0.bn3.weight",
      "layer2.0.bn3.bias",
      "layer2.0.downsample.1.weight",
      "layer2.0.downsample.1.bias",
      "layer2.1.bn1.weight",
      "layer2.1.bn1.bias",
      "layer2.1.bn2.weight",
      "layer2.1.bn2.bias",
      "layer2.1.bn3.weight",
      "layer2.1.bn3.bias",
      "layer2.2.bn1.weight",
      "layer2.2.bn1.bias",
      "layer2.2.bn2.weight",
      "layer2.2.bn2.bias",
      "layer2.2.bn3.weight",
      "layer2.2.bn3.bias",
      "layer2.3.bn1.weight",
      "layer2.3.bn1.bias",
      "layer2.3.bn2.weight",
      "layer2.3.bn2.bias",
      "layer2.3.bn3.weight",
      "layer2.3.bn3.bias",
      "layer3.0.bn1.weight",
      "layer3.0.bn1.bias",
      "layer3.0.bn2.weight",
      "layer3.0.bn2.bias",
      "layer3.0.bn3.weight",
      "layer3.0.bn3.bias",
      "layer3.0.downsample.1.weight",
      "layer3.0.downsample.1.bias",
      "layer3.1.bn1.weight",
      "layer3.1.bn1.bias",
      "layer3.1.bn2.weight",
      "layer3.1.bn2.bias",
      "layer3.1.bn3.weight",
      "layer3.1.bn3.bias",
      "layer3.2.bn1.weight",
      "layer3.2.bn1.bias",
      "layer3.2.bn2.weight",
      "layer3.2.bn2.bias",
      "layer3.2.bn3.weight",
      "layer3.2.bn3.bias",
      "layer3.3.bn1.weight",
      "layer3.3.bn1.bias",
      "layer3.3.bn2.weight",
      "layer3.3.bn2.bias",
      "layer3.3.bn3.weight",
      "layer3.3.bn3.bias",
      "layer3.4.bn1.weight",
      "layer3.4.bn1.bias",
      "layer3.4.bn2.weight",
      "layer3.4.bn2.bias",
      "layer3.4.bn3.weight",
      "layer3.4.bn3.bias",
      "layer3.5.bn1.weight",
      "layer3.5.bn1.bias",
      "layer3.5.bn2.weight",
      "layer3.5.bn2.bias",
      "layer3.5.bn3.weight",
      "layer3.5.bn3.bias",
      "layer4.0.bn1.weight",
      "layer4.0.bn1.bias",
      "layer4.0.bn2.weight",
      "layer4.0.bn2.bias",
      "layer4.0.bn3.weight",
      "layer4.0.bn3.bias",
      "layer4.0.downsample.1.weight",
      "layer4.0.downsample.1.bias",
      "layer4.1.bn1.weight",
      "layer4.1.bn1.bias",
      "layer4.1.bn2.weight",
      "layer4.1.bn2.bias",
      "layer4.1.bn3.weight",
      "layer4.1.bn3.bias",
      "layer4.2.bn1.weight",
      "layer4.2.bn1.bias",
      "layer4.2.bn2.weight",
      "layer4.2.bn2.bias",
      "layer4.2.bn3.weight",
      "layer4.2.bn3.bias",
      "fc.bias"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
criterion = SoftTargetCrossEntropy()
dict_keys(['model', 'optimizer', 'epoch', 'scaler', 'args', 'model_ema'])
Resume checkpoint /gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_SLakTTeachers_STRN50_NKD/checkpoint-best.pth
With optim & sched!
conv1.weight density is 1.0
bn1.weight density is 1.0
bn1.bias density is 1.0
layer1.0.conv1.weight density is 1.0
layer1.0.bn1.weight density is 1.0
layer1.0.bn1.bias density is 1.0
layer1.0.conv2.weight density is 1.0
layer1.0.bn2.weight density is 1.0
layer1.0.bn2.bias density is 1.0
layer1.0.conv3.weight density is 1.0
layer1.0.bn3.weight density is 1.0
layer1.0.bn3.bias density is 1.0
layer1.0.downsample.0.weight density is 1.0
layer1.0.downsample.1.weight density is 1.0
layer1.0.downsample.1.bias density is 1.0
layer1.1.conv1.weight density is 1.0
layer1.1.bn1.weight density is 1.0
layer1.1.bn1.bias density is 1.0
layer1.1.conv2.weight density is 1.0
layer1.1.bn2.weight density is 1.0
layer1.1.bn2.bias density is 1.0
layer1.1.conv3.weight density is 1.0
layer1.1.bn3.weight density is 1.0
layer1.1.bn3.bias density is 1.0
layer1.2.conv1.weight density is 1.0
layer1.2.bn1.weight density is 1.0
layer1.2.bn1.bias density is 1.0
layer1.2.conv2.weight density is 1.0
layer1.2.bn2.weight density is 1.0
layer1.2.bn2.bias density is 1.0
layer1.2.conv3.weight density is 1.0
layer1.2.bn3.weight density is 1.0
layer1.2.bn3.bias density is 1.0
layer2.0.conv1.weight density is 1.0
layer2.0.bn1.weight density is 1.0
layer2.0.bn1.bias density is 1.0
layer2.0.conv2.weight density is 1.0
layer2.0.bn2.weight density is 1.0
layer2.0.bn2.bias density is 1.0
layer2.0.conv3.weight density is 1.0
layer2.0.bn3.weight density is 1.0
layer2.0.bn3.bias density is 1.0
layer2.0.downsample.0.weight density is 1.0
layer2.0.downsample.1.weight density is 1.0
layer2.0.downsample.1.bias density is 1.0
layer2.1.conv1.weight density is 1.0
layer2.1.bn1.weight density is 1.0
layer2.1.bn1.bias density is 1.0
layer2.1.conv2.weight density is 1.0
layer2.1.bn2.weight density is 1.0
layer2.1.bn2.bias density is 1.0
layer2.1.conv3.weight density is 1.0
layer2.1.bn3.weight density is 1.0
layer2.1.bn3.bias density is 1.0
layer2.2.conv1.weight density is 1.0
layer2.2.bn1.weight density is 1.0
layer2.2.bn1.bias density is 1.0
layer2.2.conv2.weight density is 1.0
layer2.2.bn2.weight density is 1.0
layer2.2.bn2.bias density is 1.0
layer2.2.conv3.weight density is 1.0
layer2.2.bn3.weight density is 1.0
layer2.2.bn3.bias density is 1.0
layer2.3.conv1.weight density is 1.0
layer2.3.bn1.weight density is 1.0
layer2.3.bn1.bias density is 1.0
layer2.3.conv2.weight density is 1.0
layer2.3.bn2.weight density is 1.0
layer2.3.bn2.bias density is 1.0
layer2.3.conv3.weight density is 1.0
layer2.3.bn3.weight density is 1.0
layer2.3.bn3.bias density is 1.0
layer3.0.conv1.weight density is 1.0
layer3.0.bn1.weight density is 1.0
layer3.0.bn1.bias density is 1.0
layer3.0.conv2.weight density is 1.0
layer3.0.bn2.weight density is 1.0
layer3.0.bn2.bias density is 1.0
layer3.0.conv3.weight density is 1.0
layer3.0.bn3.weight density is 1.0
layer3.0.bn3.bias density is 1.0
layer3.0.downsample.0.weight density is 1.0
layer3.0.downsample.1.weight density is 1.0
layer3.0.downsample.1.bias density is 1.0
layer3.1.conv1.weight density is 1.0
layer3.1.bn1.weight density is 1.0
layer3.1.bn1.bias density is 1.0
layer3.1.conv2.weight density is 1.0
layer3.1.bn2.weight density is 1.0
layer3.1.bn2.bias density is 1.0
layer3.1.conv3.weight density is 1.0
layer3.1.bn3.weight density is 1.0
layer3.1.bn3.bias density is 1.0
layer3.2.conv1.weight density is 1.0
layer3.2.bn1.weight density is 1.0
layer3.2.bn1.bias density is 1.0
layer3.2.conv2.weight density is 1.0
layer3.2.bn2.weight density is 1.0
layer3.2.bn2.bias density is 1.0
layer3.2.conv3.weight density is 1.0
layer3.2.bn3.weight density is 1.0
layer3.2.bn3.bias density is 1.0
layer3.3.conv1.weight density is 1.0
layer3.3.bn1.weight density is 1.0
layer3.3.bn1.bias density is 1.0
layer3.3.conv2.weight density is 1.0
layer3.3.bn2.weight density is 1.0
layer3.3.bn2.bias density is 1.0
layer3.3.conv3.weight density is 1.0
layer3.3.bn3.weight density is 1.0
layer3.3.bn3.bias density is 1.0
layer3.4.conv1.weight density is 1.0
layer3.4.bn1.weight density is 1.0
layer3.4.bn1.bias density is 1.0
layer3.4.conv2.weight density is 1.0
layer3.4.bn2.weight density is 1.0
layer3.4.bn2.bias density is 1.0
layer3.4.conv3.weight density is 1.0
layer3.4.bn3.weight density is 1.0
layer3.4.bn3.bias density is 1.0
layer3.5.conv1.weight density is 1.0
layer3.5.bn1.weight density is 1.0
layer3.5.bn1.bias density is 1.0
layer3.5.conv2.weight density is 1.0
layer3.5.bn2.weight density is 1.0
layer3.5.bn2.bias density is 1.0
layer3.5.conv3.weight density is 1.0
layer3.5.bn3.weight density is 1.0
layer3.5.bn3.bias density is 1.0
layer4.0.conv1.weight density is 1.0
layer4.0.bn1.weight density is 1.0
layer4.0.bn1.bias density is 1.0
layer4.0.conv2.weight density is 1.0
layer4.0.bn2.weight density is 1.0
layer4.0.bn2.bias density is 1.0
layer4.0.conv3.weight density is 1.0
layer4.0.bn3.weight density is 1.0
layer4.0.bn3.bias density is 1.0
layer4.0.downsample.0.weight density is 1.0
layer4.0.downsample.1.weight density is 1.0
layer4.0.downsample.1.bias density is 1.0
layer4.1.conv1.weight density is 1.0
layer4.1.bn1.weight density is 1.0
layer4.1.bn1.bias density is 1.0
layer4.1.conv2.weight density is 1.0
layer4.1.bn2.weight density is 1.0
layer4.1.bn2.bias density is 1.0
layer4.1.conv3.weight density is 1.0
layer4.1.bn3.weight density is 1.0
layer4.1.bn3.bias density is 1.0
layer4.2.conv1.weight density is 1.0
layer4.2.bn1.weight density is 1.0
layer4.2.bn1.bias density is 1.0
layer4.2.conv2.weight density is 1.0
layer4.2.bn2.weight density is 1.0
layer4.2.bn2.bias density is 1.0
layer4.2.conv3.weight density is 1.0
layer4.2.bn3.weight density is 1.0
layer4.2.bn3.bias density is 1.0
fc.weight density is 1.0
fc.bias density is 1.0
Test:  [  0/531]  eta: 0:35:45  loss: 5.2172 (5.2172)  acc1: 16.6667 (16.6667)  acc5: 36.4583 (36.4583)  time: 4.0405  data: 2.6606  max mem: 3726
Test:  [ 10/531]  eta: 0:03:57  loss: 4.5204 (4.1859)  acc1: 28.1250 (32.2917)  acc5: 48.9583 (49.8106)  time: 0.4563  data: 0.2910  max mem: 3726
Test:  [ 20/531]  eta: 0:02:28  loss: 4.0941 (4.1676)  acc1: 28.1250 (29.5635)  acc5: 52.0833 (50.4464)  time: 0.1029  data: 0.0597  max mem: 3726
Test:  [ 30/531]  eta: 0:02:04  loss: 4.3666 (4.5850)  acc1: 22.9167 (25.3360)  acc5: 40.6250 (44.4556)  time: 0.1347  data: 0.0923  max mem: 3726
Test:  [ 40/531]  eta: 0:01:49  loss: 4.7176 (4.5488)  acc1: 19.7917 (25.2033)  acc5: 37.5000 (45.0203)  time: 0.1513  data: 0.1090  max mem: 3726
Test:  [ 50/531]  eta: 0:01:45  loss: 4.3299 (4.4640)  acc1: 28.1250 (26.5727)  acc5: 46.8750 (46.2214)  time: 0.1755  data: 0.1331  max mem: 3726
Test:  [ 60/531]  eta: 0:01:41  loss: 4.4454 (4.4873)  acc1: 29.1667 (26.6052)  acc5: 45.8333 (45.5601)  time: 0.2006  data: 0.1582  max mem: 3726
Test:  [ 70/531]  eta: 0:01:51  loss: 4.3573 (4.4661)  acc1: 25.0000 (26.4818)  acc5: 41.6667 (45.7453)  time: 0.2972  data: 0.2537  max mem: 3726
Test:  [ 80/531]  eta: 0:01:48  loss: 4.2738 (4.4552)  acc1: 22.9167 (26.7233)  acc5: 44.7917 (45.6533)  time: 0.3160  data: 0.2723  max mem: 3726
Test:  [ 90/531]  eta: 0:01:40  loss: 3.7502 (4.3664)  acc1: 26.0417 (27.5183)  acc5: 53.1250 (47.1383)  time: 0.1827  data: 0.1394  max mem: 3726
Test:  [100/531]  eta: 0:01:34  loss: 3.3576 (4.2755)  acc1: 37.5000 (28.4138)  acc5: 64.5833 (48.7418)  time: 0.1317  data: 0.0885  max mem: 3726
Test:  [110/531]  eta: 0:01:30  loss: 3.2990 (4.1891)  acc1: 36.4583 (29.1479)  acc5: 66.6667 (50.4035)  time: 0.1482  data: 0.1046  max mem: 3726
Test:  [120/531]  eta: 0:01:23  loss: 3.2701 (4.1357)  acc1: 33.3333 (29.5455)  acc5: 66.6667 (51.4118)  time: 0.1316  data: 0.0884  max mem: 3726
Test:  [130/531]  eta: 0:01:21  loss: 3.3427 (4.1010)  acc1: 33.3333 (29.9062)  acc5: 61.4583 (51.9879)  time: 0.1407  data: 0.0984  max mem: 3726
Test:  [140/531]  eta: 0:01:18  loss: 3.3163 (4.0439)  acc1: 37.5000 (30.6664)  acc5: 64.5833 (52.8886)  time: 0.1768  data: 0.1341  max mem: 3726
Test:  [150/531]  eta: 0:01:16  loss: 3.3287 (4.0556)  acc1: 29.1667 (30.5671)  acc5: 57.2917 (52.8422)  time: 0.1829  data: 0.1405  max mem: 3726
Test:  [160/531]  eta: 0:01:15  loss: 3.6831 (4.0505)  acc1: 29.1667 (30.8295)  acc5: 53.1250 (52.8856)  time: 0.2226  data: 0.1796  max mem: 3726
Test:  [170/531]  eta: 0:01:11  loss: 4.0335 (4.0589)  acc1: 29.1667 (30.8601)  acc5: 51.0417 (52.7412)  time: 0.1895  data: 0.1462  max mem: 3726
Test:  [180/531]  eta: 0:01:09  loss: 4.4000 (4.0521)  acc1: 29.1667 (31.1982)  acc5: 46.8750 (52.7970)  time: 0.1488  data: 0.1064  max mem: 3726
Test:  [190/531]  eta: 0:01:07  loss: 3.8217 (4.0514)  acc1: 35.4167 (31.3754)  acc5: 47.9167 (52.7978)  time: 0.1883  data: 0.1463  max mem: 3726
Test:  [200/531]  eta: 0:01:05  loss: 3.8406 (4.0583)  acc1: 29.1667 (31.2966)  acc5: 47.9167 (52.5964)  time: 0.1932  data: 0.1505  max mem: 3726
Test:  [210/531]  eta: 0:01:03  loss: 4.3707 (4.1014)  acc1: 21.8750 (30.8501)  acc5: 43.7500 (51.9451)  time: 0.1888  data: 0.1446  max mem: 3726
Test:  [220/531]  eta: 0:01:02  loss: 4.3085 (4.1005)  acc1: 28.1250 (30.9813)  acc5: 43.7500 (51.7675)  time: 0.2255  data: 0.1817  max mem: 3726
Test:  [230/531]  eta: 0:00:59  loss: 4.0310 (4.1028)  acc1: 30.2083 (30.8847)  acc5: 50.0000 (51.5828)  time: 0.1971  data: 0.1547  max mem: 3726
Test:  [240/531]  eta: 0:00:57  loss: 4.0310 (4.1134)  acc1: 30.2083 (30.7529)  acc5: 50.0000 (51.2837)  time: 0.1613  data: 0.1189  max mem: 3726
Test:  [250/531]  eta: 0:00:55  loss: 4.1889 (4.1404)  acc1: 21.8750 (30.4034)  acc5: 42.7083 (50.7595)  time: 0.2130  data: 0.1706  max mem: 3726
Test:  [260/531]  eta: 0:00:53  loss: 4.4232 (4.1450)  acc1: 23.9583 (30.2203)  acc5: 41.6667 (50.5747)  time: 0.2180  data: 0.1756  max mem: 3726
Test:  [270/531]  eta: 0:00:51  loss: 3.9030 (4.1316)  acc1: 28.1250 (30.4313)  acc5: 54.1667 (50.7803)  time: 0.1826  data: 0.1402  max mem: 3726
Test:  [280/531]  eta: 0:00:49  loss: 3.4594 (4.1102)  acc1: 39.5833 (30.8015)  acc5: 58.3333 (51.0194)  time: 0.1981  data: 0.1551  max mem: 3726
Test:  [290/531]  eta: 0:00:47  loss: 3.6506 (4.1187)  acc1: 32.2917 (30.7310)  acc5: 48.9583 (50.8627)  time: 0.1795  data: 0.1361  max mem: 3726
Test:  [300/531]  eta: 0:00:44  loss: 4.0359 (4.1166)  acc1: 29.1667 (30.8209)  acc5: 46.8750 (50.8063)  time: 0.1489  data: 0.1061  max mem: 3726
Test:  [310/531]  eta: 0:00:43  loss: 3.9042 (4.1129)  acc1: 33.3333 (31.0423)  acc5: 46.8750 (50.8139)  time: 0.1800  data: 0.1372  max mem: 3726
Test:  [320/531]  eta: 0:00:41  loss: 3.8597 (4.1073)  acc1: 35.4167 (31.1591)  acc5: 48.9583 (50.7496)  time: 0.1994  data: 0.1566  max mem: 3726
Test:  [330/531]  eta: 0:00:38  loss: 3.7259 (4.0974)  acc1: 31.2500 (31.1556)  acc5: 47.9167 (50.7899)  time: 0.1612  data: 0.1188  max mem: 3726
Test:  [340/531]  eta: 0:00:36  loss: 4.0781 (4.1132)  acc1: 21.8750 (30.7979)  acc5: 45.8333 (50.4613)  time: 0.1464  data: 0.1040  max mem: 3726
Test:  [350/531]  eta: 0:00:34  loss: 4.5046 (4.1200)  acc1: 16.6667 (30.6950)  acc5: 38.5417 (50.2849)  time: 0.1791  data: 0.1364  max mem: 3726
Test:  [360/531]  eta: 0:00:32  loss: 4.5347 (4.1299)  acc1: 21.8750 (30.4796)  acc5: 41.6667 (50.1558)  time: 0.1733  data: 0.1301  max mem: 3726
Test:  [370/531]  eta: 0:00:30  loss: 4.5777 (4.1396)  acc1: 20.8333 (30.3768)  acc5: 42.7083 (49.9579)  time: 0.1978  data: 0.1543  max mem: 3726
Test:  [380/531]  eta: 0:00:29  loss: 4.0755 (4.1372)  acc1: 29.1667 (30.4325)  acc5: 45.8333 (50.0027)  time: 0.2172  data: 0.1742  max mem: 3726
Test:  [390/531]  eta: 0:00:26  loss: 4.5453 (4.1597)  acc1: 25.0000 (30.1870)  acc5: 45.8333 (49.5631)  time: 0.1705  data: 0.1273  max mem: 3726
Test:  [400/531]  eta: 0:00:25  loss: 4.7099 (4.1673)  acc1: 21.8750 (30.1200)  acc5: 35.4167 (49.3506)  time: 0.1696  data: 0.1261  max mem: 3726
Test:  [410/531]  eta: 0:00:23  loss: 3.8435 (4.1655)  acc1: 33.3333 (30.2235)  acc5: 43.7500 (49.3461)  time: 0.1765  data: 0.1333  max mem: 3726
Test:  [420/531]  eta: 0:00:21  loss: 4.1937 (4.1684)  acc1: 29.1667 (30.1935)  acc5: 44.7917 (49.2355)  time: 0.1599  data: 0.1175  max mem: 3726
Test:  [430/531]  eta: 0:00:19  loss: 4.2333 (4.1782)  acc1: 22.9167 (30.0247)  acc5: 44.7917 (49.0429)  time: 0.1623  data: 0.1199  max mem: 3726
Test:  [440/531]  eta: 0:00:17  loss: 4.0130 (4.1688)  acc1: 27.0833 (30.1422)  acc5: 48.9583 (49.1544)  time: 0.2145  data: 0.1711  max mem: 3726
Test:  [450/531]  eta: 0:00:15  loss: 3.7314 (4.1603)  acc1: 34.3750 (30.1806)  acc5: 52.0833 (49.3048)  time: 0.2446  data: 0.1998  max mem: 3726
Test:  [460/531]  eta: 0:00:13  loss: 4.3917 (4.1733)  acc1: 26.0417 (30.0321)  acc5: 44.7917 (49.0261)  time: 0.2063  data: 0.1617  max mem: 3726
Test:  [470/531]  eta: 0:00:11  loss: 4.1971 (4.1648)  acc1: 30.2083 (30.2128)  acc5: 43.7500 (49.1131)  time: 0.1686  data: 0.1254  max mem: 3726
Test:  [480/531]  eta: 0:00:09  loss: 4.0470 (4.1648)  acc1: 27.0833 (30.0957)  acc5: 51.0417 (49.1186)  time: 0.1495  data: 0.1067  max mem: 3726
Test:  [490/531]  eta: 0:00:07  loss: 4.2943 (4.1795)  acc1: 20.8333 (29.9771)  acc5: 41.6667 (48.8883)  time: 0.2194  data: 0.1762  max mem: 3726
Test:  [500/531]  eta: 0:00:05  loss: 5.3561 (4.2241)  acc1: 14.5833 (29.6054)  acc5: 27.0833 (48.2805)  time: 0.2559  data: 0.2132  max mem: 3726
Test:  [510/531]  eta: 0:00:04  loss: 6.5234 (4.2639)  acc1: 7.2917 (29.2421)  acc5: 13.5417 (47.6884)  time: 0.2137  data: 0.1713  max mem: 3726
Test:  [520/531]  eta: 0:00:02  loss: 5.6827 (4.2931)  acc1: 7.2917 (28.9088)  acc5: 17.7083 (47.2369)  time: 0.2142  data: 0.1711  max mem: 3726
Test:  [530/531]  eta: 0:00:00  loss: 5.9169 (4.3278)  acc1: 7.2917 (28.5405)  acc5: 17.7083 (46.7036)  time: 0.2279  data: 0.1817  max mem: 3726
Test: Total time: 0:01:43 (0.1943 s / it)
* Acc@1 28.541 Acc@5 46.704 loss 4.328
Accuracy of the network on 50889 test images: 28.54055%
Not using distributed mode
Namespace(Decom=False, aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=64, bn=True, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/sliu/project_space/datasets/sketch', data_set='IMNET', data_type='imagenet-s', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.1, enable_wandb=False, epochs=300, eval=True, eval_data_path=None, finetune='', fix=False, growth='random', head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, kernel_size=[7, 7, 7, 7, 100], layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=-1, log_dir=None, lr=0.004, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='SLaK_tiny', model_ema=False, model_ema_decay=0.9999, model_ema_eval=False, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_workers=10, only_L=False, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', parallel=True, pin_mem=True, project='SLaK', prune='magnitude', prune_rate=0.3, recount=1, redistribution='none', remode='pixel', reprob=0.25, resplit=False, resume='/gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T3_bnTrue_Teacherswin_STConvNext_NKD/checkpoint-best.pth', save_ckpt=True, save_ckpt_freq=10, save_ckpt_num=3, seed=0, smoothing=0.1, sparse=False, sparse_init='snip', sparsity=0.4, start_epoch=0, train_interpolation='bicubic', update_freq=1, update_frequency=100, use_amp=False, verbose=False, wandb_ckpt=False, warmup_epochs=20, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, width_factor=1.0, world_size=1)
Transform = 
Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /home/sliu/project_space/datasets/sketch
Number of the class = 1000
Mixup is activated!
Model = SLaK(
  (downsample_layers): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (1): LayerNorm()
    )
    (1): Sequential(
      (0): LayerNorm()
      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))
    )
    (2): Sequential(
      (0): LayerNorm()
      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))
    )
    (3): Sequential(
      (0): LayerNorm()
      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (stages): ModuleList(
    (0): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): Identity()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): DropPath()
      )
    )
    (1): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
    )
    (2): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (3): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (4): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (5): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (6): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (7): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (8): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
    )
    (3): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
number of params: 28595752
LR = 0.00400000
Batch size = 64
Update frequent = 1
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "downsample_layers.0.0.weight",
      "downsample_layers.1.1.weight",
      "downsample_layers.2.1.weight",
      "downsample_layers.3.1.weight",
      "stages.0.0.large_kernel.lkb_origin.conv.weight",
      "stages.0.0.pwconv1.weight",
      "stages.0.0.pwconv2.weight",
      "stages.0.1.large_kernel.lkb_origin.conv.weight",
      "stages.0.1.pwconv1.weight",
      "stages.0.1.pwconv2.weight",
      "stages.0.2.large_kernel.lkb_origin.conv.weight",
      "stages.0.2.pwconv1.weight",
      "stages.0.2.pwconv2.weight",
      "stages.1.0.large_kernel.lkb_origin.conv.weight",
      "stages.1.0.pwconv1.weight",
      "stages.1.0.pwconv2.weight",
      "stages.1.1.large_kernel.lkb_origin.conv.weight",
      "stages.1.1.pwconv1.weight",
      "stages.1.1.pwconv2.weight",
      "stages.1.2.large_kernel.lkb_origin.conv.weight",
      "stages.1.2.pwconv1.weight",
      "stages.1.2.pwconv2.weight",
      "stages.2.0.large_kernel.lkb_origin.conv.weight",
      "stages.2.0.pwconv1.weight",
      "stages.2.0.pwconv2.weight",
      "stages.2.1.large_kernel.lkb_origin.conv.weight",
      "stages.2.1.pwconv1.weight",
      "stages.2.1.pwconv2.weight",
      "stages.2.2.large_kernel.lkb_origin.conv.weight",
      "stages.2.2.pwconv1.weight",
      "stages.2.2.pwconv2.weight",
      "stages.2.3.large_kernel.lkb_origin.conv.weight",
      "stages.2.3.pwconv1.weight",
      "stages.2.3.pwconv2.weight",
      "stages.2.4.large_kernel.lkb_origin.conv.weight",
      "stages.2.4.pwconv1.weight",
      "stages.2.4.pwconv2.weight",
      "stages.2.5.large_kernel.lkb_origin.conv.weight",
      "stages.2.5.pwconv1.weight",
      "stages.2.5.pwconv2.weight",
      "stages.2.6.large_kernel.lkb_origin.conv.weight",
      "stages.2.6.pwconv1.weight",
      "stages.2.6.pwconv2.weight",
      "stages.2.7.large_kernel.lkb_origin.conv.weight",
      "stages.2.7.pwconv1.weight",
      "stages.2.7.pwconv2.weight",
      "stages.2.8.large_kernel.lkb_origin.conv.weight",
      "stages.2.8.pwconv1.weight",
      "stages.2.8.pwconv2.weight",
      "stages.3.0.large_kernel.lkb_origin.conv.weight",
      "stages.3.0.pwconv1.weight",
      "stages.3.0.pwconv2.weight",
      "stages.3.1.large_kernel.lkb_origin.conv.weight",
      "stages.3.1.pwconv1.weight",
      "stages.3.1.pwconv2.weight",
      "stages.3.2.large_kernel.lkb_origin.conv.weight",
      "stages.3.2.pwconv1.weight",
      "stages.3.2.pwconv2.weight",
      "head.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "downsample_layers.0.0.bias",
      "downsample_layers.0.1.weight",
      "downsample_layers.0.1.bias",
      "downsample_layers.1.0.weight",
      "downsample_layers.1.0.bias",
      "downsample_layers.1.1.bias",
      "downsample_layers.2.0.weight",
      "downsample_layers.2.0.bias",
      "downsample_layers.2.1.bias",
      "downsample_layers.3.0.weight",
      "downsample_layers.3.0.bias",
      "downsample_layers.3.1.bias",
      "stages.0.0.gamma",
      "stages.0.0.large_kernel.lkb_origin.bn.weight",
      "stages.0.0.large_kernel.lkb_origin.bn.bias",
      "stages.0.0.norm.weight",
      "stages.0.0.norm.bias",
      "stages.0.0.pwconv1.bias",
      "stages.0.0.pwconv2.bias",
      "stages.0.1.gamma",
      "stages.0.1.large_kernel.lkb_origin.bn.weight",
      "stages.0.1.large_kernel.lkb_origin.bn.bias",
      "stages.0.1.norm.weight",
      "stages.0.1.norm.bias",
      "stages.0.1.pwconv1.bias",
      "stages.0.1.pwconv2.bias",
      "stages.0.2.gamma",
      "stages.0.2.large_kernel.lkb_origin.bn.weight",
      "stages.0.2.large_kernel.lkb_origin.bn.bias",
      "stages.0.2.norm.weight",
      "stages.0.2.norm.bias",
      "stages.0.2.pwconv1.bias",
      "stages.0.2.pwconv2.bias",
      "stages.1.0.gamma",
      "stages.1.0.large_kernel.lkb_origin.bn.weight",
      "stages.1.0.large_kernel.lkb_origin.bn.bias",
      "stages.1.0.norm.weight",
      "stages.1.0.norm.bias",
      "stages.1.0.pwconv1.bias",
      "stages.1.0.pwconv2.bias",
      "stages.1.1.gamma",
      "stages.1.1.large_kernel.lkb_origin.bn.weight",
      "stages.1.1.large_kernel.lkb_origin.bn.bias",
      "stages.1.1.norm.weight",
      "stages.1.1.norm.bias",
      "stages.1.1.pwconv1.bias",
      "stages.1.1.pwconv2.bias",
      "stages.1.2.gamma",
      "stages.1.2.large_kernel.lkb_origin.bn.weight",
      "stages.1.2.large_kernel.lkb_origin.bn.bias",
      "stages.1.2.norm.weight",
      "stages.1.2.norm.bias",
      "stages.1.2.pwconv1.bias",
      "stages.1.2.pwconv2.bias",
      "stages.2.0.gamma",
      "stages.2.0.large_kernel.lkb_origin.bn.weight",
      "stages.2.0.large_kernel.lkb_origin.bn.bias",
      "stages.2.0.norm.weight",
      "stages.2.0.norm.bias",
      "stages.2.0.pwconv1.bias",
      "stages.2.0.pwconv2.bias",
      "stages.2.1.gamma",
      "stages.2.1.large_kernel.lkb_origin.bn.weight",
      "stages.2.1.large_kernel.lkb_origin.bn.bias",
      "stages.2.1.norm.weight",
      "stages.2.1.norm.bias",
      "stages.2.1.pwconv1.bias",
      "stages.2.1.pwconv2.bias",
      "stages.2.2.gamma",
      "stages.2.2.large_kernel.lkb_origin.bn.weight",
      "stages.2.2.large_kernel.lkb_origin.bn.bias",
      "stages.2.2.norm.weight",
      "stages.2.2.norm.bias",
      "stages.2.2.pwconv1.bias",
      "stages.2.2.pwconv2.bias",
      "stages.2.3.gamma",
      "stages.2.3.large_kernel.lkb_origin.bn.weight",
      "stages.2.3.large_kernel.lkb_origin.bn.bias",
      "stages.2.3.norm.weight",
      "stages.2.3.norm.bias",
      "stages.2.3.pwconv1.bias",
      "stages.2.3.pwconv2.bias",
      "stages.2.4.gamma",
      "stages.2.4.large_kernel.lkb_origin.bn.weight",
      "stages.2.4.large_kernel.lkb_origin.bn.bias",
      "stages.2.4.norm.weight",
      "stages.2.4.norm.bias",
      "stages.2.4.pwconv1.bias",
      "stages.2.4.pwconv2.bias",
      "stages.2.5.gamma",
      "stages.2.5.large_kernel.lkb_origin.bn.weight",
      "stages.2.5.large_kernel.lkb_origin.bn.bias",
      "stages.2.5.norm.weight",
      "stages.2.5.norm.bias",
      "stages.2.5.pwconv1.bias",
      "stages.2.5.pwconv2.bias",
      "stages.2.6.gamma",
      "stages.2.6.large_kernel.lkb_origin.bn.weight",
      "stages.2.6.large_kernel.lkb_origin.bn.bias",
      "stages.2.6.norm.weight",
      "stages.2.6.norm.bias",
      "stages.2.6.pwconv1.bias",
      "stages.2.6.pwconv2.bias",
      "stages.2.7.gamma",
      "stages.2.7.large_kernel.lkb_origin.bn.weight",
      "stages.2.7.large_kernel.lkb_origin.bn.bias",
      "stages.2.7.norm.weight",
      "stages.2.7.norm.bias",
      "stages.2.7.pwconv1.bias",
      "stages.2.7.pwconv2.bias",
      "stages.2.8.gamma",
      "stages.2.8.large_kernel.lkb_origin.bn.weight",
      "stages.2.8.large_kernel.lkb_origin.bn.bias",
      "stages.2.8.norm.weight",
      "stages.2.8.norm.bias",
      "stages.2.8.pwconv1.bias",
      "stages.2.8.pwconv2.bias",
      "stages.3.0.gamma",
      "stages.3.0.large_kernel.lkb_origin.bn.weight",
      "stages.3.0.large_kernel.lkb_origin.bn.bias",
      "stages.3.0.norm.weight",
      "stages.3.0.norm.bias",
      "stages.3.0.pwconv1.bias",
      "stages.3.0.pwconv2.bias",
      "stages.3.1.gamma",
      "stages.3.1.large_kernel.lkb_origin.bn.weight",
      "stages.3.1.large_kernel.lkb_origin.bn.bias",
      "stages.3.1.norm.weight",
      "stages.3.1.norm.bias",
      "stages.3.1.pwconv1.bias",
      "stages.3.1.pwconv2.bias",
      "stages.3.2.gamma",
      "stages.3.2.large_kernel.lkb_origin.bn.weight",
      "stages.3.2.large_kernel.lkb_origin.bn.bias",
      "stages.3.2.norm.weight",
      "stages.3.2.norm.bias",
      "stages.3.2.pwconv1.bias",
      "stages.3.2.pwconv2.bias",
      "norm.weight",
      "norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
criterion = SoftTargetCrossEntropy()
dict_keys(['model', 'optimizer', 'epoch', 'scaler', 'args', 'model_ema'])
Resume checkpoint /gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T3_bnTrue_Teacherswin_STConvNext_NKD/checkpoint-best.pth
With optim & sched!
downsample_layers.0.0.weight density is 1.0
downsample_layers.0.0.bias density is 1.0
downsample_layers.0.1.weight density is 1.0
downsample_layers.0.1.bias density is 1.0
downsample_layers.1.0.weight density is 1.0
downsample_layers.1.0.bias density is 1.0
downsample_layers.1.1.weight density is 1.0
downsample_layers.1.1.bias density is 1.0
downsample_layers.2.0.weight density is 1.0
downsample_layers.2.0.bias density is 1.0
downsample_layers.2.1.weight density is 1.0
downsample_layers.2.1.bias density is 1.0
downsample_layers.3.0.weight density is 1.0
downsample_layers.3.0.bias density is 1.0
downsample_layers.3.1.weight density is 1.0
downsample_layers.3.1.bias density is 1.0
stages.0.0.gamma density is 1.0
stages.0.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.0.norm.weight density is 1.0
stages.0.0.norm.bias density is 1.0
stages.0.0.pwconv1.weight density is 1.0
stages.0.0.pwconv1.bias density is 1.0
stages.0.0.pwconv2.weight density is 1.0
stages.0.0.pwconv2.bias density is 1.0
stages.0.1.gamma density is 1.0
stages.0.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.1.norm.weight density is 1.0
stages.0.1.norm.bias density is 1.0
stages.0.1.pwconv1.weight density is 1.0
stages.0.1.pwconv1.bias density is 1.0
stages.0.1.pwconv2.weight density is 1.0
stages.0.1.pwconv2.bias density is 1.0
stages.0.2.gamma density is 1.0
stages.0.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.2.norm.weight density is 1.0
stages.0.2.norm.bias density is 1.0
stages.0.2.pwconv1.weight density is 1.0
stages.0.2.pwconv1.bias density is 1.0
stages.0.2.pwconv2.weight density is 1.0
stages.0.2.pwconv2.bias density is 1.0
stages.1.0.gamma density is 1.0
stages.1.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.0.norm.weight density is 1.0
stages.1.0.norm.bias density is 1.0
stages.1.0.pwconv1.weight density is 1.0
stages.1.0.pwconv1.bias density is 1.0
stages.1.0.pwconv2.weight density is 1.0
stages.1.0.pwconv2.bias density is 1.0
stages.1.1.gamma density is 1.0
stages.1.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.1.norm.weight density is 1.0
stages.1.1.norm.bias density is 1.0
stages.1.1.pwconv1.weight density is 1.0
stages.1.1.pwconv1.bias density is 1.0
stages.1.1.pwconv2.weight density is 1.0
stages.1.1.pwconv2.bias density is 1.0
stages.1.2.gamma density is 1.0
stages.1.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.2.norm.weight density is 1.0
stages.1.2.norm.bias density is 1.0
stages.1.2.pwconv1.weight density is 1.0
stages.1.2.pwconv1.bias density is 1.0
stages.1.2.pwconv2.weight density is 1.0
stages.1.2.pwconv2.bias density is 1.0
stages.2.0.gamma density is 1.0
stages.2.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.0.norm.weight density is 1.0
stages.2.0.norm.bias density is 1.0
stages.2.0.pwconv1.weight density is 1.0
stages.2.0.pwconv1.bias density is 1.0
stages.2.0.pwconv2.weight density is 1.0
stages.2.0.pwconv2.bias density is 1.0
stages.2.1.gamma density is 1.0
stages.2.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.1.norm.weight density is 1.0
stages.2.1.norm.bias density is 1.0
stages.2.1.pwconv1.weight density is 1.0
stages.2.1.pwconv1.bias density is 1.0
stages.2.1.pwconv2.weight density is 1.0
stages.2.1.pwconv2.bias density is 1.0
stages.2.2.gamma density is 1.0
stages.2.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.2.norm.weight density is 1.0
stages.2.2.norm.bias density is 1.0
stages.2.2.pwconv1.weight density is 1.0
stages.2.2.pwconv1.bias density is 1.0
stages.2.2.pwconv2.weight density is 1.0
stages.2.2.pwconv2.bias density is 1.0
stages.2.3.gamma density is 1.0
stages.2.3.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.3.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.3.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.3.norm.weight density is 1.0
stages.2.3.norm.bias density is 1.0
stages.2.3.pwconv1.weight density is 1.0
stages.2.3.pwconv1.bias density is 1.0
stages.2.3.pwconv2.weight density is 1.0
stages.2.3.pwconv2.bias density is 1.0
stages.2.4.gamma density is 1.0
stages.2.4.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.4.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.4.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.4.norm.weight density is 1.0
stages.2.4.norm.bias density is 1.0
stages.2.4.pwconv1.weight density is 1.0
stages.2.4.pwconv1.bias density is 1.0
stages.2.4.pwconv2.weight density is 1.0
stages.2.4.pwconv2.bias density is 1.0
stages.2.5.gamma density is 1.0
stages.2.5.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.5.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.5.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.5.norm.weight density is 1.0
stages.2.5.norm.bias density is 1.0
stages.2.5.pwconv1.weight density is 1.0
stages.2.5.pwconv1.bias density is 1.0
stages.2.5.pwconv2.weight density is 1.0
stages.2.5.pwconv2.bias density is 1.0
stages.2.6.gamma density is 1.0
stages.2.6.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.6.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.6.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.6.norm.weight density is 1.0
stages.2.6.norm.bias density is 1.0
stages.2.6.pwconv1.weight density is 1.0
stages.2.6.pwconv1.bias density is 1.0
stages.2.6.pwconv2.weight density is 1.0
stages.2.6.pwconv2.bias density is 1.0
stages.2.7.gamma density is 1.0
stages.2.7.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.7.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.7.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.7.norm.weight density is 1.0
stages.2.7.norm.bias density is 1.0
stages.2.7.pwconv1.weight density is 1.0
stages.2.7.pwconv1.bias density is 1.0
stages.2.7.pwconv2.weight density is 1.0
stages.2.7.pwconv2.bias density is 1.0
stages.2.8.gamma density is 1.0
stages.2.8.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.8.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.8.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.8.norm.weight density is 1.0
stages.2.8.norm.bias density is 1.0
stages.2.8.pwconv1.weight density is 1.0
stages.2.8.pwconv1.bias density is 1.0
stages.2.8.pwconv2.weight density is 1.0
stages.2.8.pwconv2.bias density is 1.0
stages.3.0.gamma density is 1.0
stages.3.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.0.norm.weight density is 1.0
stages.3.0.norm.bias density is 1.0
stages.3.0.pwconv1.weight density is 1.0
stages.3.0.pwconv1.bias density is 1.0
stages.3.0.pwconv2.weight density is 1.0
stages.3.0.pwconv2.bias density is 1.0
stages.3.1.gamma density is 1.0
stages.3.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.1.norm.weight density is 1.0
stages.3.1.norm.bias density is 1.0
stages.3.1.pwconv1.weight density is 1.0
stages.3.1.pwconv1.bias density is 1.0
stages.3.1.pwconv2.weight density is 1.0
stages.3.1.pwconv2.bias density is 1.0
stages.3.2.gamma density is 1.0
stages.3.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.2.norm.weight density is 1.0
stages.3.2.norm.bias density is 1.0
stages.3.2.pwconv1.weight density is 1.0
stages.3.2.pwconv1.bias density is 1.0
stages.3.2.pwconv2.weight density is 1.0
stages.3.2.pwconv2.bias density is 1.0
norm.weight density is 1.0
norm.bias density is 1.0
head.weight density is 1.0
head.bias density is 1.0
Test:  [  0/531]  eta: 0:32:35  loss: 4.5675 (4.5675)  acc1: 23.9583 (23.9583)  acc5: 45.8333 (45.8333)  time: 3.6834  data: 2.7236  max mem: 2139
Test:  [ 10/531]  eta: 0:04:02  loss: 4.4251 (3.9588)  acc1: 23.9583 (34.1856)  acc5: 46.8750 (51.7045)  time: 0.4646  data: 0.3170  max mem: 2139
Test:  [ 20/531]  eta: 0:02:30  loss: 3.7363 (3.9516)  acc1: 28.1250 (31.0516)  acc5: 54.1667 (53.0754)  time: 0.1257  data: 0.0611  max mem: 2139
Test:  [ 30/531]  eta: 0:02:07  loss: 4.0979 (4.4536)  acc1: 25.0000 (26.6129)  acc5: 42.7083 (47.2446)  time: 0.1390  data: 0.0763  max mem: 2139
Test:  [ 40/531]  eta: 0:01:54  loss: 4.5780 (4.4525)  acc1: 17.7083 (26.3211)  acc5: 41.6667 (47.3323)  time: 0.1671  data: 0.1033  max mem: 2139
Test:  [ 50/531]  eta: 0:01:50  loss: 4.1502 (4.3281)  acc1: 31.2500 (28.2884)  acc5: 46.8750 (48.8358)  time: 0.1936  data: 0.1288  max mem: 2139
Test:  [ 60/531]  eta: 0:01:45  loss: 3.9448 (4.3103)  acc1: 32.2917 (28.7056)  acc5: 48.9583 (48.6168)  time: 0.2080  data: 0.1436  max mem: 2139
Test:  [ 70/531]  eta: 0:01:55  loss: 3.8088 (4.2492)  acc1: 31.2500 (29.0346)  acc5: 50.0000 (49.3251)  time: 0.3036  data: 0.2391  max mem: 2139
Test:  [ 80/531]  eta: 0:01:52  loss: 4.1345 (4.2266)  acc1: 29.1667 (29.3338)  acc5: 50.0000 (49.4470)  time: 0.3255  data: 0.2613  max mem: 2139
Test:  [ 90/531]  eta: 0:01:43  loss: 3.3496 (4.1212)  acc1: 29.1667 (30.2541)  acc5: 53.1250 (50.9386)  time: 0.1810  data: 0.1172  max mem: 2139
Test:  [100/531]  eta: 0:01:36  loss: 3.1114 (4.0244)  acc1: 37.5000 (31.1262)  acc5: 65.6250 (52.2999)  time: 0.1234  data: 0.0600  max mem: 2139
Test:  [110/531]  eta: 0:01:32  loss: 3.1305 (3.9343)  acc1: 37.5000 (31.8412)  acc5: 69.7917 (53.9039)  time: 0.1447  data: 0.0804  max mem: 2139
Test:  [120/531]  eta: 0:01:26  loss: 2.8944 (3.8622)  acc1: 35.4167 (32.3261)  acc5: 71.8750 (55.1481)  time: 0.1337  data: 0.0690  max mem: 2139
Test:  [130/531]  eta: 0:01:23  loss: 2.9440 (3.8271)  acc1: 35.4167 (32.5700)  acc5: 65.6250 (55.5741)  time: 0.1387  data: 0.0749  max mem: 2139
Test:  [140/531]  eta: 0:01:19  loss: 2.8955 (3.7676)  acc1: 37.5000 (33.3407)  acc5: 65.6250 (56.4642)  time: 0.1700  data: 0.1062  max mem: 2139
Test:  [150/531]  eta: 0:01:17  loss: 2.8955 (3.7688)  acc1: 29.1667 (33.2781)  acc5: 62.5000 (56.4777)  time: 0.1814  data: 0.1176  max mem: 2139
Test:  [160/531]  eta: 0:01:16  loss: 3.2380 (3.7594)  acc1: 33.3333 (33.6568)  acc5: 59.3750 (56.5541)  time: 0.2185  data: 0.1525  max mem: 2139
Test:  [170/531]  eta: 0:01:12  loss: 3.4248 (3.7663)  acc1: 34.3750 (33.7049)  acc5: 56.2500 (56.4754)  time: 0.1846  data: 0.1190  max mem: 2139
Test:  [180/531]  eta: 0:01:09  loss: 3.8775 (3.7577)  acc1: 34.3750 (34.1045)  acc5: 53.1250 (56.4514)  time: 0.1479  data: 0.0841  max mem: 2139
Test:  [190/531]  eta: 0:01:08  loss: 3.0888 (3.7409)  acc1: 35.4167 (34.3968)  acc5: 64.5833 (56.6808)  time: 0.1891  data: 0.1253  max mem: 2139
Test:  [200/531]  eta: 0:01:05  loss: 3.1975 (3.7518)  acc1: 35.4167 (34.2973)  acc5: 53.1250 (56.2863)  time: 0.1980  data: 0.1350  max mem: 2139
Test:  [210/531]  eta: 0:01:04  loss: 4.4670 (3.7972)  acc1: 28.1250 (33.9356)  acc5: 44.7917 (55.6576)  time: 0.2065  data: 0.1434  max mem: 2139
Test:  [220/531]  eta: 0:01:02  loss: 4.1677 (3.8038)  acc1: 33.3333 (34.0969)  acc5: 45.8333 (55.4063)  time: 0.2257  data: 0.1622  max mem: 2139
Test:  [230/531]  eta: 0:01:00  loss: 3.6605 (3.8186)  acc1: 35.4167 (33.9105)  acc5: 56.2500 (55.1407)  time: 0.1883  data: 0.1248  max mem: 2139
Test:  [240/531]  eta: 0:00:57  loss: 3.7259 (3.8314)  acc1: 33.3333 (33.8304)  acc5: 53.1250 (54.9015)  time: 0.1498  data: 0.0863  max mem: 2139
Test:  [250/531]  eta: 0:00:56  loss: 4.0665 (3.8680)  acc1: 22.9167 (33.4246)  acc5: 47.9167 (54.3368)  time: 0.2068  data: 0.1434  max mem: 2139
Test:  [260/531]  eta: 0:00:53  loss: 4.2648 (3.8739)  acc1: 20.8333 (33.1577)  acc5: 42.7083 (54.1707)  time: 0.2201  data: 0.1567  max mem: 2139
Test:  [270/531]  eta: 0:00:52  loss: 3.5880 (3.8663)  acc1: 33.3333 (33.3641)  acc5: 53.1250 (54.1936)  time: 0.1915  data: 0.1277  max mem: 2139
Test:  [280/531]  eta: 0:00:50  loss: 3.3934 (3.8412)  acc1: 39.5833 (33.7411)  acc5: 57.2917 (54.4558)  time: 0.2063  data: 0.1420  max mem: 2139
Test:  [290/531]  eta: 0:00:47  loss: 3.4219 (3.8431)  acc1: 35.4167 (33.7056)  acc5: 55.2083 (54.4459)  time: 0.1720  data: 0.1081  max mem: 2139
Test:  [300/531]  eta: 0:00:45  loss: 3.5487 (3.8403)  acc1: 34.3750 (33.8040)  acc5: 51.0417 (54.4193)  time: 0.1600  data: 0.0961  max mem: 2139
Test:  [310/531]  eta: 0:00:43  loss: 3.6732 (3.8388)  acc1: 37.5000 (33.9530)  acc5: 52.0833 (54.4447)  time: 0.1918  data: 0.1279  max mem: 2139
Test:  [320/531]  eta: 0:00:41  loss: 3.6981 (3.8343)  acc1: 37.5000 (34.1089)  acc5: 48.9583 (54.4068)  time: 0.1881  data: 0.1246  max mem: 2139
Test:  [330/531]  eta: 0:00:39  loss: 3.8770 (3.8234)  acc1: 32.2917 (34.0823)  acc5: 47.9167 (54.4625)  time: 0.1647  data: 0.1001  max mem: 2139
Test:  [340/531]  eta: 0:00:37  loss: 3.8780 (3.8430)  acc1: 27.0833 (33.7885)  acc5: 48.9583 (54.1392)  time: 0.1658  data: 0.1012  max mem: 2139
Test:  [350/531]  eta: 0:00:35  loss: 4.1216 (3.8552)  acc1: 25.0000 (33.6212)  acc5: 44.7917 (53.9174)  time: 0.1700  data: 0.1066  max mem: 2139
Test:  [360/531]  eta: 0:00:32  loss: 4.4465 (3.8681)  acc1: 21.8750 (33.3449)  acc5: 44.7917 (53.7194)  time: 0.1535  data: 0.0900  max mem: 2139
Test:  [370/531]  eta: 0:00:31  loss: 4.3050 (3.8802)  acc1: 20.8333 (33.2126)  acc5: 44.7917 (53.4928)  time: 0.1904  data: 0.1273  max mem: 2139
Test:  [380/531]  eta: 0:00:29  loss: 3.9858 (3.8801)  acc1: 31.2500 (33.2486)  acc5: 45.8333 (53.4968)  time: 0.2210  data: 0.1575  max mem: 2139
Test:  [390/531]  eta: 0:00:27  loss: 4.4721 (3.9040)  acc1: 31.2500 (33.0110)  acc5: 45.8333 (53.0238)  time: 0.1745  data: 0.1099  max mem: 2139
Test:  [400/531]  eta: 0:00:25  loss: 4.5209 (3.9199)  acc1: 22.9167 (32.8761)  acc5: 37.5000 (52.7276)  time: 0.1771  data: 0.1125  max mem: 2139
Test:  [410/531]  eta: 0:00:23  loss: 4.0145 (3.9191)  acc1: 34.3750 (32.9886)  acc5: 47.9167 (52.7220)  time: 0.1778  data: 0.1143  max mem: 2139
Test:  [420/531]  eta: 0:00:21  loss: 4.1900 (3.9236)  acc1: 32.2917 (32.9572)  acc5: 47.9167 (52.6524)  time: 0.1631  data: 0.1004  max mem: 2139
Test:  [430/531]  eta: 0:00:19  loss: 4.1996 (3.9342)  acc1: 30.2083 (32.8838)  acc5: 46.8750 (52.4845)  time: 0.1666  data: 0.1039  max mem: 2139
Test:  [440/531]  eta: 0:00:17  loss: 3.6144 (3.9228)  acc1: 35.4167 (33.0050)  acc5: 53.1250 (52.6195)  time: 0.1837  data: 0.1202  max mem: 2139
Test:  [450/531]  eta: 0:00:15  loss: 3.4322 (3.9145)  acc1: 36.4583 (33.0446)  acc5: 56.2500 (52.7000)  time: 0.2359  data: 0.1706  max mem: 2139
Test:  [460/531]  eta: 0:00:13  loss: 3.9285 (3.9245)  acc1: 34.3750 (32.9560)  acc5: 45.8333 (52.4584)  time: 0.2127  data: 0.1474  max mem: 2139
Test:  [470/531]  eta: 0:00:11  loss: 3.9305 (3.9168)  acc1: 34.3750 (33.1100)  acc5: 45.8333 (52.5013)  time: 0.1476  data: 0.0841  max mem: 2139
Test:  [480/531]  eta: 0:00:09  loss: 3.6741 (3.9224)  acc1: 31.2500 (32.9284)  acc5: 50.0000 (52.4558)  time: 0.1647  data: 0.1020  max mem: 2139
Test:  [490/531]  eta: 0:00:07  loss: 4.1869 (3.9409)  acc1: 27.0833 (32.7817)  acc5: 44.7917 (52.1788)  time: 0.2176  data: 0.1530  max mem: 2139
Test:  [500/531]  eta: 0:00:05  loss: 5.5981 (3.9940)  acc1: 10.4167 (32.3520)  acc5: 27.0833 (51.4991)  time: 0.2347  data: 0.1693  max mem: 2139
Test:  [510/531]  eta: 0:00:04  loss: 6.6189 (4.0395)  acc1: 7.2917 (31.9512)  acc5: 14.5833 (50.8908)  time: 0.2263  data: 0.1624  max mem: 2139
Test:  [520/531]  eta: 0:00:02  loss: 5.9065 (4.0718)  acc1: 10.4167 (31.6339)  acc5: 23.9583 (50.4659)  time: 0.2116  data: 0.1476  max mem: 2139
Test:  [530/531]  eta: 0:00:00  loss: 5.7004 (4.1077)  acc1: 10.4167 (31.2346)  acc5: 23.9583 (49.9636)  time: 0.2059  data: 0.1436  max mem: 2139
Test: Total time: 0:01:43 (0.1940 s / it)
* Acc@1 31.235 Acc@5 49.964 loss 4.108
Accuracy of the network on 50889 test images: 31.23465%
Not using distributed mode
Namespace(Decom=False, aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=64, bn=True, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/sliu/project_space/datasets/sketch', data_set='IMNET', data_type='imagenet-s', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.1, enable_wandb=False, epochs=300, eval=True, eval_data_path=None, finetune='', fix=False, growth='random', head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, kernel_size=[7, 7, 7, 7, 100], layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=-1, log_dir=None, lr=0.004, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='SLaK_tiny', model_ema=False, model_ema_decay=0.9999, model_ema_eval=False, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_workers=10, only_L=False, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', parallel=True, pin_mem=True, project='SLaK', prune='magnitude', prune_rate=0.3, recount=1, redistribution='none', remode='pixel', reprob=0.25, resplit=False, resume='/gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T3_bnTrue_Teachercswin_STConvNext_NKD/checkpoint-best.pth', save_ckpt=True, save_ckpt_freq=10, save_ckpt_num=3, seed=0, smoothing=0.1, sparse=False, sparse_init='snip', sparsity=0.4, start_epoch=0, train_interpolation='bicubic', update_freq=1, update_frequency=100, use_amp=False, verbose=False, wandb_ckpt=False, warmup_epochs=20, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, width_factor=1.0, world_size=1)
Transform = 
Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /home/sliu/project_space/datasets/sketch
Number of the class = 1000
Mixup is activated!
Model = SLaK(
  (downsample_layers): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (1): LayerNorm()
    )
    (1): Sequential(
      (0): LayerNorm()
      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))
    )
    (2): Sequential(
      (0): LayerNorm()
      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))
    )
    (3): Sequential(
      (0): LayerNorm()
      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (stages): ModuleList(
    (0): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): Identity()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): DropPath()
      )
    )
    (1): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
    )
    (2): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (3): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (4): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (5): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (6): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (7): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (8): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
    )
    (3): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
number of params: 28595752
LR = 0.00400000
Batch size = 64
Update frequent = 1
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "downsample_layers.0.0.weight",
      "downsample_layers.1.1.weight",
      "downsample_layers.2.1.weight",
      "downsample_layers.3.1.weight",
      "stages.0.0.large_kernel.lkb_origin.conv.weight",
      "stages.0.0.pwconv1.weight",
      "stages.0.0.pwconv2.weight",
      "stages.0.1.large_kernel.lkb_origin.conv.weight",
      "stages.0.1.pwconv1.weight",
      "stages.0.1.pwconv2.weight",
      "stages.0.2.large_kernel.lkb_origin.conv.weight",
      "stages.0.2.pwconv1.weight",
      "stages.0.2.pwconv2.weight",
      "stages.1.0.large_kernel.lkb_origin.conv.weight",
      "stages.1.0.pwconv1.weight",
      "stages.1.0.pwconv2.weight",
      "stages.1.1.large_kernel.lkb_origin.conv.weight",
      "stages.1.1.pwconv1.weight",
      "stages.1.1.pwconv2.weight",
      "stages.1.2.large_kernel.lkb_origin.conv.weight",
      "stages.1.2.pwconv1.weight",
      "stages.1.2.pwconv2.weight",
      "stages.2.0.large_kernel.lkb_origin.conv.weight",
      "stages.2.0.pwconv1.weight",
      "stages.2.0.pwconv2.weight",
      "stages.2.1.large_kernel.lkb_origin.conv.weight",
      "stages.2.1.pwconv1.weight",
      "stages.2.1.pwconv2.weight",
      "stages.2.2.large_kernel.lkb_origin.conv.weight",
      "stages.2.2.pwconv1.weight",
      "stages.2.2.pwconv2.weight",
      "stages.2.3.large_kernel.lkb_origin.conv.weight",
      "stages.2.3.pwconv1.weight",
      "stages.2.3.pwconv2.weight",
      "stages.2.4.large_kernel.lkb_origin.conv.weight",
      "stages.2.4.pwconv1.weight",
      "stages.2.4.pwconv2.weight",
      "stages.2.5.large_kernel.lkb_origin.conv.weight",
      "stages.2.5.pwconv1.weight",
      "stages.2.5.pwconv2.weight",
      "stages.2.6.large_kernel.lkb_origin.conv.weight",
      "stages.2.6.pwconv1.weight",
      "stages.2.6.pwconv2.weight",
      "stages.2.7.large_kernel.lkb_origin.conv.weight",
      "stages.2.7.pwconv1.weight",
      "stages.2.7.pwconv2.weight",
      "stages.2.8.large_kernel.lkb_origin.conv.weight",
      "stages.2.8.pwconv1.weight",
      "stages.2.8.pwconv2.weight",
      "stages.3.0.large_kernel.lkb_origin.conv.weight",
      "stages.3.0.pwconv1.weight",
      "stages.3.0.pwconv2.weight",
      "stages.3.1.large_kernel.lkb_origin.conv.weight",
      "stages.3.1.pwconv1.weight",
      "stages.3.1.pwconv2.weight",
      "stages.3.2.large_kernel.lkb_origin.conv.weight",
      "stages.3.2.pwconv1.weight",
      "stages.3.2.pwconv2.weight",
      "head.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "downsample_layers.0.0.bias",
      "downsample_layers.0.1.weight",
      "downsample_layers.0.1.bias",
      "downsample_layers.1.0.weight",
      "downsample_layers.1.0.bias",
      "downsample_layers.1.1.bias",
      "downsample_layers.2.0.weight",
      "downsample_layers.2.0.bias",
      "downsample_layers.2.1.bias",
      "downsample_layers.3.0.weight",
      "downsample_layers.3.0.bias",
      "downsample_layers.3.1.bias",
      "stages.0.0.gamma",
      "stages.0.0.large_kernel.lkb_origin.bn.weight",
      "stages.0.0.large_kernel.lkb_origin.bn.bias",
      "stages.0.0.norm.weight",
      "stages.0.0.norm.bias",
      "stages.0.0.pwconv1.bias",
      "stages.0.0.pwconv2.bias",
      "stages.0.1.gamma",
      "stages.0.1.large_kernel.lkb_origin.bn.weight",
      "stages.0.1.large_kernel.lkb_origin.bn.bias",
      "stages.0.1.norm.weight",
      "stages.0.1.norm.bias",
      "stages.0.1.pwconv1.bias",
      "stages.0.1.pwconv2.bias",
      "stages.0.2.gamma",
      "stages.0.2.large_kernel.lkb_origin.bn.weight",
      "stages.0.2.large_kernel.lkb_origin.bn.bias",
      "stages.0.2.norm.weight",
      "stages.0.2.norm.bias",
      "stages.0.2.pwconv1.bias",
      "stages.0.2.pwconv2.bias",
      "stages.1.0.gamma",
      "stages.1.0.large_kernel.lkb_origin.bn.weight",
      "stages.1.0.large_kernel.lkb_origin.bn.bias",
      "stages.1.0.norm.weight",
      "stages.1.0.norm.bias",
      "stages.1.0.pwconv1.bias",
      "stages.1.0.pwconv2.bias",
      "stages.1.1.gamma",
      "stages.1.1.large_kernel.lkb_origin.bn.weight",
      "stages.1.1.large_kernel.lkb_origin.bn.bias",
      "stages.1.1.norm.weight",
      "stages.1.1.norm.bias",
      "stages.1.1.pwconv1.bias",
      "stages.1.1.pwconv2.bias",
      "stages.1.2.gamma",
      "stages.1.2.large_kernel.lkb_origin.bn.weight",
      "stages.1.2.large_kernel.lkb_origin.bn.bias",
      "stages.1.2.norm.weight",
      "stages.1.2.norm.bias",
      "stages.1.2.pwconv1.bias",
      "stages.1.2.pwconv2.bias",
      "stages.2.0.gamma",
      "stages.2.0.large_kernel.lkb_origin.bn.weight",
      "stages.2.0.large_kernel.lkb_origin.bn.bias",
      "stages.2.0.norm.weight",
      "stages.2.0.norm.bias",
      "stages.2.0.pwconv1.bias",
      "stages.2.0.pwconv2.bias",
      "stages.2.1.gamma",
      "stages.2.1.large_kernel.lkb_origin.bn.weight",
      "stages.2.1.large_kernel.lkb_origin.bn.bias",
      "stages.2.1.norm.weight",
      "stages.2.1.norm.bias",
      "stages.2.1.pwconv1.bias",
      "stages.2.1.pwconv2.bias",
      "stages.2.2.gamma",
      "stages.2.2.large_kernel.lkb_origin.bn.weight",
      "stages.2.2.large_kernel.lkb_origin.bn.bias",
      "stages.2.2.norm.weight",
      "stages.2.2.norm.bias",
      "stages.2.2.pwconv1.bias",
      "stages.2.2.pwconv2.bias",
      "stages.2.3.gamma",
      "stages.2.3.large_kernel.lkb_origin.bn.weight",
      "stages.2.3.large_kernel.lkb_origin.bn.bias",
      "stages.2.3.norm.weight",
      "stages.2.3.norm.bias",
      "stages.2.3.pwconv1.bias",
      "stages.2.3.pwconv2.bias",
      "stages.2.4.gamma",
      "stages.2.4.large_kernel.lkb_origin.bn.weight",
      "stages.2.4.large_kernel.lkb_origin.bn.bias",
      "stages.2.4.norm.weight",
      "stages.2.4.norm.bias",
      "stages.2.4.pwconv1.bias",
      "stages.2.4.pwconv2.bias",
      "stages.2.5.gamma",
      "stages.2.5.large_kernel.lkb_origin.bn.weight",
      "stages.2.5.large_kernel.lkb_origin.bn.bias",
      "stages.2.5.norm.weight",
      "stages.2.5.norm.bias",
      "stages.2.5.pwconv1.bias",
      "stages.2.5.pwconv2.bias",
      "stages.2.6.gamma",
      "stages.2.6.large_kernel.lkb_origin.bn.weight",
      "stages.2.6.large_kernel.lkb_origin.bn.bias",
      "stages.2.6.norm.weight",
      "stages.2.6.norm.bias",
      "stages.2.6.pwconv1.bias",
      "stages.2.6.pwconv2.bias",
      "stages.2.7.gamma",
      "stages.2.7.large_kernel.lkb_origin.bn.weight",
      "stages.2.7.large_kernel.lkb_origin.bn.bias",
      "stages.2.7.norm.weight",
      "stages.2.7.norm.bias",
      "stages.2.7.pwconv1.bias",
      "stages.2.7.pwconv2.bias",
      "stages.2.8.gamma",
      "stages.2.8.large_kernel.lkb_origin.bn.weight",
      "stages.2.8.large_kernel.lkb_origin.bn.bias",
      "stages.2.8.norm.weight",
      "stages.2.8.norm.bias",
      "stages.2.8.pwconv1.bias",
      "stages.2.8.pwconv2.bias",
      "stages.3.0.gamma",
      "stages.3.0.large_kernel.lkb_origin.bn.weight",
      "stages.3.0.large_kernel.lkb_origin.bn.bias",
      "stages.3.0.norm.weight",
      "stages.3.0.norm.bias",
      "stages.3.0.pwconv1.bias",
      "stages.3.0.pwconv2.bias",
      "stages.3.1.gamma",
      "stages.3.1.large_kernel.lkb_origin.bn.weight",
      "stages.3.1.large_kernel.lkb_origin.bn.bias",
      "stages.3.1.norm.weight",
      "stages.3.1.norm.bias",
      "stages.3.1.pwconv1.bias",
      "stages.3.1.pwconv2.bias",
      "stages.3.2.gamma",
      "stages.3.2.large_kernel.lkb_origin.bn.weight",
      "stages.3.2.large_kernel.lkb_origin.bn.bias",
      "stages.3.2.norm.weight",
      "stages.3.2.norm.bias",
      "stages.3.2.pwconv1.bias",
      "stages.3.2.pwconv2.bias",
      "norm.weight",
      "norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
criterion = SoftTargetCrossEntropy()
dict_keys(['model', 'optimizer', 'epoch', 'scaler', 'args', 'model_ema'])
Resume checkpoint /gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T3_bnTrue_Teachercswin_STConvNext_NKD/checkpoint-best.pth
With optim & sched!
downsample_layers.0.0.weight density is 1.0
downsample_layers.0.0.bias density is 1.0
downsample_layers.0.1.weight density is 1.0
downsample_layers.0.1.bias density is 1.0
downsample_layers.1.0.weight density is 1.0
downsample_layers.1.0.bias density is 1.0
downsample_layers.1.1.weight density is 1.0
downsample_layers.1.1.bias density is 1.0
downsample_layers.2.0.weight density is 1.0
downsample_layers.2.0.bias density is 1.0
downsample_layers.2.1.weight density is 1.0
downsample_layers.2.1.bias density is 1.0
downsample_layers.3.0.weight density is 1.0
downsample_layers.3.0.bias density is 1.0
downsample_layers.3.1.weight density is 1.0
downsample_layers.3.1.bias density is 1.0
stages.0.0.gamma density is 1.0
stages.0.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.0.norm.weight density is 1.0
stages.0.0.norm.bias density is 1.0
stages.0.0.pwconv1.weight density is 1.0
stages.0.0.pwconv1.bias density is 1.0
stages.0.0.pwconv2.weight density is 1.0
stages.0.0.pwconv2.bias density is 1.0
stages.0.1.gamma density is 1.0
stages.0.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.1.norm.weight density is 1.0
stages.0.1.norm.bias density is 1.0
stages.0.1.pwconv1.weight density is 1.0
stages.0.1.pwconv1.bias density is 1.0
stages.0.1.pwconv2.weight density is 1.0
stages.0.1.pwconv2.bias density is 1.0
stages.0.2.gamma density is 1.0
stages.0.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.2.norm.weight density is 1.0
stages.0.2.norm.bias density is 1.0
stages.0.2.pwconv1.weight density is 1.0
stages.0.2.pwconv1.bias density is 1.0
stages.0.2.pwconv2.weight density is 1.0
stages.0.2.pwconv2.bias density is 1.0
stages.1.0.gamma density is 1.0
stages.1.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.0.norm.weight density is 1.0
stages.1.0.norm.bias density is 1.0
stages.1.0.pwconv1.weight density is 1.0
stages.1.0.pwconv1.bias density is 1.0
stages.1.0.pwconv2.weight density is 1.0
stages.1.0.pwconv2.bias density is 1.0
stages.1.1.gamma density is 1.0
stages.1.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.1.norm.weight density is 1.0
stages.1.1.norm.bias density is 1.0
stages.1.1.pwconv1.weight density is 1.0
stages.1.1.pwconv1.bias density is 1.0
stages.1.1.pwconv2.weight density is 1.0
stages.1.1.pwconv2.bias density is 1.0
stages.1.2.gamma density is 1.0
stages.1.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.2.norm.weight density is 1.0
stages.1.2.norm.bias density is 1.0
stages.1.2.pwconv1.weight density is 1.0
stages.1.2.pwconv1.bias density is 1.0
stages.1.2.pwconv2.weight density is 1.0
stages.1.2.pwconv2.bias density is 1.0
stages.2.0.gamma density is 1.0
stages.2.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.0.norm.weight density is 1.0
stages.2.0.norm.bias density is 1.0
stages.2.0.pwconv1.weight density is 1.0
stages.2.0.pwconv1.bias density is 1.0
stages.2.0.pwconv2.weight density is 1.0
stages.2.0.pwconv2.bias density is 1.0
stages.2.1.gamma density is 1.0
stages.2.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.1.norm.weight density is 1.0
stages.2.1.norm.bias density is 1.0
stages.2.1.pwconv1.weight density is 1.0
stages.2.1.pwconv1.bias density is 1.0
stages.2.1.pwconv2.weight density is 1.0
stages.2.1.pwconv2.bias density is 1.0
stages.2.2.gamma density is 1.0
stages.2.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.2.norm.weight density is 1.0
stages.2.2.norm.bias density is 1.0
stages.2.2.pwconv1.weight density is 1.0
stages.2.2.pwconv1.bias density is 1.0
stages.2.2.pwconv2.weight density is 1.0
stages.2.2.pwconv2.bias density is 1.0
stages.2.3.gamma density is 1.0
stages.2.3.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.3.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.3.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.3.norm.weight density is 1.0
stages.2.3.norm.bias density is 1.0
stages.2.3.pwconv1.weight density is 1.0
stages.2.3.pwconv1.bias density is 1.0
stages.2.3.pwconv2.weight density is 1.0
stages.2.3.pwconv2.bias density is 1.0
stages.2.4.gamma density is 1.0
stages.2.4.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.4.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.4.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.4.norm.weight density is 1.0
stages.2.4.norm.bias density is 1.0
stages.2.4.pwconv1.weight density is 1.0
stages.2.4.pwconv1.bias density is 1.0
stages.2.4.pwconv2.weight density is 1.0
stages.2.4.pwconv2.bias density is 1.0
stages.2.5.gamma density is 1.0
stages.2.5.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.5.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.5.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.5.norm.weight density is 1.0
stages.2.5.norm.bias density is 1.0
stages.2.5.pwconv1.weight density is 1.0
stages.2.5.pwconv1.bias density is 1.0
stages.2.5.pwconv2.weight density is 1.0
stages.2.5.pwconv2.bias density is 1.0
stages.2.6.gamma density is 1.0
stages.2.6.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.6.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.6.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.6.norm.weight density is 1.0
stages.2.6.norm.bias density is 1.0
stages.2.6.pwconv1.weight density is 1.0
stages.2.6.pwconv1.bias density is 1.0
stages.2.6.pwconv2.weight density is 1.0
stages.2.6.pwconv2.bias density is 1.0
stages.2.7.gamma density is 1.0
stages.2.7.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.7.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.7.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.7.norm.weight density is 1.0
stages.2.7.norm.bias density is 1.0
stages.2.7.pwconv1.weight density is 1.0
stages.2.7.pwconv1.bias density is 1.0
stages.2.7.pwconv2.weight density is 1.0
stages.2.7.pwconv2.bias density is 1.0
stages.2.8.gamma density is 1.0
stages.2.8.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.8.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.8.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.8.norm.weight density is 1.0
stages.2.8.norm.bias density is 1.0
stages.2.8.pwconv1.weight density is 1.0
stages.2.8.pwconv1.bias density is 1.0
stages.2.8.pwconv2.weight density is 1.0
stages.2.8.pwconv2.bias density is 1.0
stages.3.0.gamma density is 1.0
stages.3.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.0.norm.weight density is 1.0
stages.3.0.norm.bias density is 1.0
stages.3.0.pwconv1.weight density is 1.0
stages.3.0.pwconv1.bias density is 1.0
stages.3.0.pwconv2.weight density is 1.0
stages.3.0.pwconv2.bias density is 1.0
stages.3.1.gamma density is 1.0
stages.3.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.1.norm.weight density is 1.0
stages.3.1.norm.bias density is 1.0
stages.3.1.pwconv1.weight density is 1.0
stages.3.1.pwconv1.bias density is 1.0
stages.3.1.pwconv2.weight density is 1.0
stages.3.1.pwconv2.bias density is 1.0
stages.3.2.gamma density is 1.0
stages.3.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.2.norm.weight density is 1.0
stages.3.2.norm.bias density is 1.0
stages.3.2.pwconv1.weight density is 1.0
stages.3.2.pwconv1.bias density is 1.0
stages.3.2.pwconv2.weight density is 1.0
stages.3.2.pwconv2.bias density is 1.0
norm.weight density is 1.0
norm.bias density is 1.0
head.weight density is 1.0
head.bias density is 1.0
Test:  [  0/531]  eta: 0:33:24  loss: 4.9639 (4.9639)  acc1: 15.6250 (15.6250)  acc5: 42.7083 (42.7083)  time: 3.7757  data: 2.6920  max mem: 2139
Test:  [ 10/531]  eta: 0:04:09  loss: 4.3156 (3.9647)  acc1: 27.0833 (34.1856)  acc5: 47.9167 (51.7045)  time: 0.4787  data: 0.3197  max mem: 2139
Test:  [ 20/531]  eta: 0:02:38  loss: 3.8540 (3.8904)  acc1: 30.2083 (32.7877)  acc5: 50.0000 (54.0179)  time: 0.1360  data: 0.0707  max mem: 2139
Test:  [ 30/531]  eta: 0:02:12  loss: 4.0758 (4.3453)  acc1: 28.1250 (28.8306)  acc5: 48.9583 (49.6304)  time: 0.1478  data: 0.0844  max mem: 2139
Test:  [ 40/531]  eta: 0:01:56  loss: 4.3188 (4.3433)  acc1: 19.7917 (28.4045)  acc5: 36.4583 (49.4157)  time: 0.1599  data: 0.0968  max mem: 2139
Test:  [ 50/531]  eta: 0:01:49  loss: 3.9044 (4.2399)  acc1: 32.2917 (29.7386)  acc5: 51.0417 (50.7149)  time: 0.1679  data: 0.1045  max mem: 2139
Test:  [ 60/531]  eta: 0:01:43  loss: 4.0007 (4.2418)  acc1: 32.2917 (30.1230)  acc5: 50.0000 (50.1025)  time: 0.1844  data: 0.1207  max mem: 2139
Test:  [ 70/531]  eta: 0:01:54  loss: 4.0007 (4.1898)  acc1: 31.2500 (30.7365)  acc5: 50.0000 (50.7189)  time: 0.3046  data: 0.2385  max mem: 2139
Test:  [ 80/531]  eta: 0:01:51  loss: 4.0100 (4.1847)  acc1: 30.2083 (30.8256)  acc5: 55.2083 (50.7330)  time: 0.3283  data: 0.2629  max mem: 2139
Test:  [ 90/531]  eta: 0:01:43  loss: 3.4592 (4.0887)  acc1: 31.2500 (31.8681)  acc5: 55.2083 (52.1864)  time: 0.1800  data: 0.1166  max mem: 2139
Test:  [100/531]  eta: 0:01:36  loss: 3.1193 (3.9869)  acc1: 40.6250 (32.8486)  acc5: 67.7083 (53.8160)  time: 0.1294  data: 0.0657  max mem: 2139
Test:  [110/531]  eta: 0:01:31  loss: 3.0817 (3.8899)  acc1: 37.5000 (33.5210)  acc5: 69.7917 (55.5931)  time: 0.1453  data: 0.0815  max mem: 2139
Test:  [120/531]  eta: 0:01:25  loss: 3.0258 (3.8296)  acc1: 38.5417 (34.0909)  acc5: 73.9583 (56.6030)  time: 0.1338  data: 0.0697  max mem: 2139
Test:  [130/531]  eta: 0:01:22  loss: 3.1645 (3.7929)  acc1: 37.5000 (34.4227)  acc5: 65.6250 (57.1485)  time: 0.1440  data: 0.0803  max mem: 2139
Test:  [140/531]  eta: 0:01:19  loss: 2.9898 (3.7314)  acc1: 37.5000 (35.2024)  acc5: 69.7917 (58.0526)  time: 0.1730  data: 0.1093  max mem: 2139
Test:  [150/531]  eta: 0:01:17  loss: 3.0050 (3.7370)  acc1: 32.2917 (34.9131)  acc5: 64.5833 (58.0160)  time: 0.1901  data: 0.1257  max mem: 2139
Test:  [160/531]  eta: 0:01:16  loss: 3.4183 (3.7321)  acc1: 32.2917 (35.2355)  acc5: 60.4167 (58.0551)  time: 0.2270  data: 0.1614  max mem: 2139
Test:  [170/531]  eta: 0:01:12  loss: 3.4183 (3.7359)  acc1: 36.4583 (35.3314)  acc5: 59.3750 (58.0166)  time: 0.1781  data: 0.1135  max mem: 2139
Test:  [180/531]  eta: 0:01:09  loss: 3.8502 (3.7367)  acc1: 34.3750 (35.5375)  acc5: 48.9583 (57.8960)  time: 0.1420  data: 0.0783  max mem: 2139
Test:  [190/531]  eta: 0:01:08  loss: 3.3656 (3.7272)  acc1: 41.6667 (35.8421)  acc5: 60.4167 (58.0552)  time: 0.1928  data: 0.1288  max mem: 2139
Test:  [200/531]  eta: 0:01:05  loss: 3.4109 (3.7406)  acc1: 39.5833 (35.7691)  acc5: 53.1250 (57.7270)  time: 0.1921  data: 0.1284  max mem: 2139
Test:  [210/531]  eta: 0:01:03  loss: 4.1522 (3.7885)  acc1: 26.0417 (35.3377)  acc5: 45.8333 (57.0053)  time: 0.1778  data: 0.1140  max mem: 2139
Test:  [220/531]  eta: 0:01:02  loss: 4.1026 (3.7929)  acc1: 30.2083 (35.5392)  acc5: 45.8333 (56.7213)  time: 0.2268  data: 0.1631  max mem: 2139
Test:  [230/531]  eta: 0:00:59  loss: 3.6875 (3.7961)  acc1: 36.4583 (35.4212)  acc5: 52.0833 (56.6017)  time: 0.2036  data: 0.1398  max mem: 2139
Test:  [240/531]  eta: 0:00:57  loss: 3.6562 (3.8058)  acc1: 36.4583 (35.3518)  acc5: 55.2083 (56.3581)  time: 0.1408  data: 0.0766  max mem: 2139
Test:  [250/531]  eta: 0:00:55  loss: 3.8823 (3.8343)  acc1: 27.0833 (35.0058)  acc5: 52.0833 (55.8723)  time: 0.1967  data: 0.1326  max mem: 2139
Test:  [260/531]  eta: 0:00:53  loss: 3.8753 (3.8308)  acc1: 27.0833 (34.8499)  acc5: 50.0000 (55.8269)  time: 0.2057  data: 0.1423  max mem: 2139
Test:  [270/531]  eta: 0:00:51  loss: 3.3698 (3.8189)  acc1: 37.5000 (35.0669)  acc5: 58.3333 (55.9194)  time: 0.1910  data: 0.1272  max mem: 2139
Test:  [280/531]  eta: 0:00:49  loss: 3.3151 (3.7919)  acc1: 40.6250 (35.4945)  acc5: 60.4167 (56.1647)  time: 0.2252  data: 0.1614  max mem: 2139
Test:  [290/531]  eta: 0:00:47  loss: 3.3791 (3.7921)  acc1: 36.4583 (35.5169)  acc5: 56.2500 (56.1927)  time: 0.1764  data: 0.1130  max mem: 2139
Test:  [300/531]  eta: 0:00:45  loss: 3.4435 (3.7849)  acc1: 38.5417 (35.6070)  acc5: 58.3333 (56.2258)  time: 0.1500  data: 0.0865  max mem: 2139
Test:  [310/531]  eta: 0:00:43  loss: 3.3960 (3.7820)  acc1: 39.5833 (35.7382)  acc5: 58.3333 (56.1696)  time: 0.1905  data: 0.1267  max mem: 2139
Test:  [320/531]  eta: 0:00:41  loss: 3.7656 (3.7728)  acc1: 39.5833 (35.9326)  acc5: 54.1667 (56.1689)  time: 0.1909  data: 0.1271  max mem: 2139
Test:  [330/531]  eta: 0:00:38  loss: 3.2459 (3.7564)  acc1: 40.6250 (35.9737)  acc5: 57.2917 (56.3004)  time: 0.1427  data: 0.0785  max mem: 2139
Test:  [340/531]  eta: 0:00:36  loss: 3.6489 (3.7690)  acc1: 29.1667 (35.6794)  acc5: 55.2083 (56.0392)  time: 0.1496  data: 0.0848  max mem: 2139
Test:  [350/531]  eta: 0:00:34  loss: 3.9424 (3.7793)  acc1: 26.0417 (35.5443)  acc5: 50.0000 (55.8345)  time: 0.1971  data: 0.1326  max mem: 2139
Test:  [360/531]  eta: 0:00:32  loss: 4.2189 (3.7884)  acc1: 25.0000 (35.2551)  acc5: 48.9583 (55.7248)  time: 0.1780  data: 0.1134  max mem: 2139
Test:  [370/531]  eta: 0:00:30  loss: 4.2189 (3.8006)  acc1: 25.0000 (35.1162)  acc5: 48.9583 (55.4863)  time: 0.1802  data: 0.1160  max mem: 2139
Test:  [380/531]  eta: 0:00:29  loss: 3.6488 (3.7953)  acc1: 31.2500 (35.1897)  acc5: 51.0417 (55.5200)  time: 0.2094  data: 0.1459  max mem: 2139
Test:  [390/531]  eta: 0:00:27  loss: 3.6785 (3.8163)  acc1: 28.1250 (34.9558)  acc5: 50.0000 (55.1311)  time: 0.1755  data: 0.1118  max mem: 2139
Test:  [400/531]  eta: 0:00:25  loss: 4.2541 (3.8248)  acc1: 25.0000 (34.8789)  acc5: 44.7917 (54.9538)  time: 0.1684  data: 0.1046  max mem: 2139
Test:  [410/531]  eta: 0:00:23  loss: 3.5271 (3.8181)  acc1: 39.5833 (35.0416)  acc5: 55.2083 (55.0081)  time: 0.1870  data: 0.1239  max mem: 2139
Test:  [420/531]  eta: 0:00:21  loss: 3.5271 (3.8216)  acc1: 39.5833 (35.0158)  acc5: 51.0417 (54.9015)  time: 0.1771  data: 0.1139  max mem: 2139
Test:  [430/531]  eta: 0:00:19  loss: 3.9997 (3.8284)  acc1: 30.2083 (34.9164)  acc5: 50.0000 (54.7757)  time: 0.1645  data: 0.1013  max mem: 2139
Test:  [440/531]  eta: 0:00:17  loss: 3.6516 (3.8186)  acc1: 34.3750 (35.0435)  acc5: 56.2500 (54.9036)  time: 0.1878  data: 0.1250  max mem: 2139
Test:  [450/531]  eta: 0:00:15  loss: 3.3388 (3.8068)  acc1: 38.5417 (35.1557)  acc5: 58.3333 (55.0674)  time: 0.2377  data: 0.1734  max mem: 2139
Test:  [460/531]  eta: 0:00:13  loss: 3.6110 (3.8123)  acc1: 35.4167 (35.1161)  acc5: 54.1667 (54.9507)  time: 0.2126  data: 0.1480  max mem: 2139
Test:  [470/531]  eta: 0:00:11  loss: 3.6331 (3.8042)  acc1: 34.3750 (35.2862)  acc5: 51.0417 (55.0027)  time: 0.1462  data: 0.0825  max mem: 2139
Test:  [480/531]  eta: 0:00:09  loss: 3.6331 (3.8042)  acc1: 33.3333 (35.1416)  acc5: 53.1250 (54.9918)  time: 0.1587  data: 0.0950  max mem: 2139
Test:  [490/531]  eta: 0:00:07  loss: 4.1191 (3.8226)  acc1: 22.9167 (34.9584)  acc5: 47.9167 (54.7204)  time: 0.2279  data: 0.1626  max mem: 2139
Test:  [500/531]  eta: 0:00:05  loss: 5.2070 (3.8740)  acc1: 16.6667 (34.5185)  acc5: 31.2500 (54.0274)  time: 0.2407  data: 0.1750  max mem: 2139
Test:  [510/531]  eta: 0:00:04  loss: 6.6405 (3.9229)  acc1: 8.3333 (34.1202)  acc5: 13.5417 (53.3717)  time: 0.2123  data: 0.1481  max mem: 2139
Test:  [520/531]  eta: 0:00:02  loss: 5.9331 (3.9573)  acc1: 8.3333 (33.7572)  acc5: 20.8333 (52.8811)  time: 0.2100  data: 0.1463  max mem: 2139
Test:  [530/531]  eta: 0:00:00  loss: 5.6650 (3.9947)  acc1: 9.3750 (33.3373)  acc5: 28.1250 (52.3669)  time: 0.2080  data: 0.1469  max mem: 2139
Test: Total time: 0:01:42 (0.1935 s / it)
* Acc@1 33.337 Acc@5 52.367 loss 3.995
Accuracy of the network on 50889 test images: 33.33726%
Not using distributed mode
Namespace(Decom=False, aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=64, bn=True, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/sliu/project_space/datasets/sketch', data_set='IMNET', data_type='imagenet-s', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.1, enable_wandb=False, epochs=300, eval=True, eval_data_path=None, finetune='', fix=False, growth='random', head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, kernel_size=[7, 7, 7, 7, 100], layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=-1, log_dir=None, lr=0.004, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='SLaK_tiny', model_ema=False, model_ema_decay=0.9999, model_ema_eval=False, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_workers=10, only_L=False, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', parallel=True, pin_mem=True, project='SLaK', prune='magnitude', prune_rate=0.3, recount=1, redistribution='none', remode='pixel', reprob=0.25, resplit=False, resume='/gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_ConvNeXtTeacher_STconvnextv1_NKD/checkpoint-best.pth', save_ckpt=True, save_ckpt_freq=10, save_ckpt_num=3, seed=0, smoothing=0.1, sparse=False, sparse_init='snip', sparsity=0.4, start_epoch=0, train_interpolation='bicubic', update_freq=1, update_frequency=100, use_amp=False, verbose=False, wandb_ckpt=False, warmup_epochs=20, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, width_factor=1.0, world_size=1)
Transform = 
Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /home/sliu/project_space/datasets/sketch
Number of the class = 1000
Mixup is activated!
Model = SLaK(
  (downsample_layers): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (1): LayerNorm()
    )
    (1): Sequential(
      (0): LayerNorm()
      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))
    )
    (2): Sequential(
      (0): LayerNorm()
      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))
    )
    (3): Sequential(
      (0): LayerNorm()
      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (stages): ModuleList(
    (0): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): Identity()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): DropPath()
      )
    )
    (1): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
    )
    (2): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (3): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (4): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (5): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (6): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (7): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (8): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
    )
    (3): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
number of params: 28595752
LR = 0.00400000
Batch size = 64
Update frequent = 1
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "downsample_layers.0.0.weight",
      "downsample_layers.1.1.weight",
      "downsample_layers.2.1.weight",
      "downsample_layers.3.1.weight",
      "stages.0.0.large_kernel.lkb_origin.conv.weight",
      "stages.0.0.pwconv1.weight",
      "stages.0.0.pwconv2.weight",
      "stages.0.1.large_kernel.lkb_origin.conv.weight",
      "stages.0.1.pwconv1.weight",
      "stages.0.1.pwconv2.weight",
      "stages.0.2.large_kernel.lkb_origin.conv.weight",
      "stages.0.2.pwconv1.weight",
      "stages.0.2.pwconv2.weight",
      "stages.1.0.large_kernel.lkb_origin.conv.weight",
      "stages.1.0.pwconv1.weight",
      "stages.1.0.pwconv2.weight",
      "stages.1.1.large_kernel.lkb_origin.conv.weight",
      "stages.1.1.pwconv1.weight",
      "stages.1.1.pwconv2.weight",
      "stages.1.2.large_kernel.lkb_origin.conv.weight",
      "stages.1.2.pwconv1.weight",
      "stages.1.2.pwconv2.weight",
      "stages.2.0.large_kernel.lkb_origin.conv.weight",
      "stages.2.0.pwconv1.weight",
      "stages.2.0.pwconv2.weight",
      "stages.2.1.large_kernel.lkb_origin.conv.weight",
      "stages.2.1.pwconv1.weight",
      "stages.2.1.pwconv2.weight",
      "stages.2.2.large_kernel.lkb_origin.conv.weight",
      "stages.2.2.pwconv1.weight",
      "stages.2.2.pwconv2.weight",
      "stages.2.3.large_kernel.lkb_origin.conv.weight",
      "stages.2.3.pwconv1.weight",
      "stages.2.3.pwconv2.weight",
      "stages.2.4.large_kernel.lkb_origin.conv.weight",
      "stages.2.4.pwconv1.weight",
      "stages.2.4.pwconv2.weight",
      "stages.2.5.large_kernel.lkb_origin.conv.weight",
      "stages.2.5.pwconv1.weight",
      "stages.2.5.pwconv2.weight",
      "stages.2.6.large_kernel.lkb_origin.conv.weight",
      "stages.2.6.pwconv1.weight",
      "stages.2.6.pwconv2.weight",
      "stages.2.7.large_kernel.lkb_origin.conv.weight",
      "stages.2.7.pwconv1.weight",
      "stages.2.7.pwconv2.weight",
      "stages.2.8.large_kernel.lkb_origin.conv.weight",
      "stages.2.8.pwconv1.weight",
      "stages.2.8.pwconv2.weight",
      "stages.3.0.large_kernel.lkb_origin.conv.weight",
      "stages.3.0.pwconv1.weight",
      "stages.3.0.pwconv2.weight",
      "stages.3.1.large_kernel.lkb_origin.conv.weight",
      "stages.3.1.pwconv1.weight",
      "stages.3.1.pwconv2.weight",
      "stages.3.2.large_kernel.lkb_origin.conv.weight",
      "stages.3.2.pwconv1.weight",
      "stages.3.2.pwconv2.weight",
      "head.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "downsample_layers.0.0.bias",
      "downsample_layers.0.1.weight",
      "downsample_layers.0.1.bias",
      "downsample_layers.1.0.weight",
      "downsample_layers.1.0.bias",
      "downsample_layers.1.1.bias",
      "downsample_layers.2.0.weight",
      "downsample_layers.2.0.bias",
      "downsample_layers.2.1.bias",
      "downsample_layers.3.0.weight",
      "downsample_layers.3.0.bias",
      "downsample_layers.3.1.bias",
      "stages.0.0.gamma",
      "stages.0.0.large_kernel.lkb_origin.bn.weight",
      "stages.0.0.large_kernel.lkb_origin.bn.bias",
      "stages.0.0.norm.weight",
      "stages.0.0.norm.bias",
      "stages.0.0.pwconv1.bias",
      "stages.0.0.pwconv2.bias",
      "stages.0.1.gamma",
      "stages.0.1.large_kernel.lkb_origin.bn.weight",
      "stages.0.1.large_kernel.lkb_origin.bn.bias",
      "stages.0.1.norm.weight",
      "stages.0.1.norm.bias",
      "stages.0.1.pwconv1.bias",
      "stages.0.1.pwconv2.bias",
      "stages.0.2.gamma",
      "stages.0.2.large_kernel.lkb_origin.bn.weight",
      "stages.0.2.large_kernel.lkb_origin.bn.bias",
      "stages.0.2.norm.weight",
      "stages.0.2.norm.bias",
      "stages.0.2.pwconv1.bias",
      "stages.0.2.pwconv2.bias",
      "stages.1.0.gamma",
      "stages.1.0.large_kernel.lkb_origin.bn.weight",
      "stages.1.0.large_kernel.lkb_origin.bn.bias",
      "stages.1.0.norm.weight",
      "stages.1.0.norm.bias",
      "stages.1.0.pwconv1.bias",
      "stages.1.0.pwconv2.bias",
      "stages.1.1.gamma",
      "stages.1.1.large_kernel.lkb_origin.bn.weight",
      "stages.1.1.large_kernel.lkb_origin.bn.bias",
      "stages.1.1.norm.weight",
      "stages.1.1.norm.bias",
      "stages.1.1.pwconv1.bias",
      "stages.1.1.pwconv2.bias",
      "stages.1.2.gamma",
      "stages.1.2.large_kernel.lkb_origin.bn.weight",
      "stages.1.2.large_kernel.lkb_origin.bn.bias",
      "stages.1.2.norm.weight",
      "stages.1.2.norm.bias",
      "stages.1.2.pwconv1.bias",
      "stages.1.2.pwconv2.bias",
      "stages.2.0.gamma",
      "stages.2.0.large_kernel.lkb_origin.bn.weight",
      "stages.2.0.large_kernel.lkb_origin.bn.bias",
      "stages.2.0.norm.weight",
      "stages.2.0.norm.bias",
      "stages.2.0.pwconv1.bias",
      "stages.2.0.pwconv2.bias",
      "stages.2.1.gamma",
      "stages.2.1.large_kernel.lkb_origin.bn.weight",
      "stages.2.1.large_kernel.lkb_origin.bn.bias",
      "stages.2.1.norm.weight",
      "stages.2.1.norm.bias",
      "stages.2.1.pwconv1.bias",
      "stages.2.1.pwconv2.bias",
      "stages.2.2.gamma",
      "stages.2.2.large_kernel.lkb_origin.bn.weight",
      "stages.2.2.large_kernel.lkb_origin.bn.bias",
      "stages.2.2.norm.weight",
      "stages.2.2.norm.bias",
      "stages.2.2.pwconv1.bias",
      "stages.2.2.pwconv2.bias",
      "stages.2.3.gamma",
      "stages.2.3.large_kernel.lkb_origin.bn.weight",
      "stages.2.3.large_kernel.lkb_origin.bn.bias",
      "stages.2.3.norm.weight",
      "stages.2.3.norm.bias",
      "stages.2.3.pwconv1.bias",
      "stages.2.3.pwconv2.bias",
      "stages.2.4.gamma",
      "stages.2.4.large_kernel.lkb_origin.bn.weight",
      "stages.2.4.large_kernel.lkb_origin.bn.bias",
      "stages.2.4.norm.weight",
      "stages.2.4.norm.bias",
      "stages.2.4.pwconv1.bias",
      "stages.2.4.pwconv2.bias",
      "stages.2.5.gamma",
      "stages.2.5.large_kernel.lkb_origin.bn.weight",
      "stages.2.5.large_kernel.lkb_origin.bn.bias",
      "stages.2.5.norm.weight",
      "stages.2.5.norm.bias",
      "stages.2.5.pwconv1.bias",
      "stages.2.5.pwconv2.bias",
      "stages.2.6.gamma",
      "stages.2.6.large_kernel.lkb_origin.bn.weight",
      "stages.2.6.large_kernel.lkb_origin.bn.bias",
      "stages.2.6.norm.weight",
      "stages.2.6.norm.bias",
      "stages.2.6.pwconv1.bias",
      "stages.2.6.pwconv2.bias",
      "stages.2.7.gamma",
      "stages.2.7.large_kernel.lkb_origin.bn.weight",
      "stages.2.7.large_kernel.lkb_origin.bn.bias",
      "stages.2.7.norm.weight",
      "stages.2.7.norm.bias",
      "stages.2.7.pwconv1.bias",
      "stages.2.7.pwconv2.bias",
      "stages.2.8.gamma",
      "stages.2.8.large_kernel.lkb_origin.bn.weight",
      "stages.2.8.large_kernel.lkb_origin.bn.bias",
      "stages.2.8.norm.weight",
      "stages.2.8.norm.bias",
      "stages.2.8.pwconv1.bias",
      "stages.2.8.pwconv2.bias",
      "stages.3.0.gamma",
      "stages.3.0.large_kernel.lkb_origin.bn.weight",
      "stages.3.0.large_kernel.lkb_origin.bn.bias",
      "stages.3.0.norm.weight",
      "stages.3.0.norm.bias",
      "stages.3.0.pwconv1.bias",
      "stages.3.0.pwconv2.bias",
      "stages.3.1.gamma",
      "stages.3.1.large_kernel.lkb_origin.bn.weight",
      "stages.3.1.large_kernel.lkb_origin.bn.bias",
      "stages.3.1.norm.weight",
      "stages.3.1.norm.bias",
      "stages.3.1.pwconv1.bias",
      "stages.3.1.pwconv2.bias",
      "stages.3.2.gamma",
      "stages.3.2.large_kernel.lkb_origin.bn.weight",
      "stages.3.2.large_kernel.lkb_origin.bn.bias",
      "stages.3.2.norm.weight",
      "stages.3.2.norm.bias",
      "stages.3.2.pwconv1.bias",
      "stages.3.2.pwconv2.bias",
      "norm.weight",
      "norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
criterion = SoftTargetCrossEntropy()
dict_keys(['model', 'optimizer', 'epoch', 'scaler', 'args', 'model_ema'])
Resume checkpoint /gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_ConvNeXtTeacher_STconvnextv1_NKD/checkpoint-best.pth
With optim & sched!
downsample_layers.0.0.weight density is 1.0
downsample_layers.0.0.bias density is 1.0
downsample_layers.0.1.weight density is 1.0
downsample_layers.0.1.bias density is 1.0
downsample_layers.1.0.weight density is 1.0
downsample_layers.1.0.bias density is 1.0
downsample_layers.1.1.weight density is 1.0
downsample_layers.1.1.bias density is 1.0
downsample_layers.2.0.weight density is 1.0
downsample_layers.2.0.bias density is 1.0
downsample_layers.2.1.weight density is 1.0
downsample_layers.2.1.bias density is 1.0
downsample_layers.3.0.weight density is 1.0
downsample_layers.3.0.bias density is 1.0
downsample_layers.3.1.weight density is 1.0
downsample_layers.3.1.bias density is 1.0
stages.0.0.gamma density is 1.0
stages.0.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.0.norm.weight density is 1.0
stages.0.0.norm.bias density is 1.0
stages.0.0.pwconv1.weight density is 1.0
stages.0.0.pwconv1.bias density is 1.0
stages.0.0.pwconv2.weight density is 1.0
stages.0.0.pwconv2.bias density is 1.0
stages.0.1.gamma density is 1.0
stages.0.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.1.norm.weight density is 1.0
stages.0.1.norm.bias density is 1.0
stages.0.1.pwconv1.weight density is 1.0
stages.0.1.pwconv1.bias density is 1.0
stages.0.1.pwconv2.weight density is 1.0
stages.0.1.pwconv2.bias density is 1.0
stages.0.2.gamma density is 1.0
stages.0.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.2.norm.weight density is 1.0
stages.0.2.norm.bias density is 1.0
stages.0.2.pwconv1.weight density is 1.0
stages.0.2.pwconv1.bias density is 1.0
stages.0.2.pwconv2.weight density is 1.0
stages.0.2.pwconv2.bias density is 1.0
stages.1.0.gamma density is 1.0
stages.1.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.0.norm.weight density is 1.0
stages.1.0.norm.bias density is 1.0
stages.1.0.pwconv1.weight density is 1.0
stages.1.0.pwconv1.bias density is 1.0
stages.1.0.pwconv2.weight density is 1.0
stages.1.0.pwconv2.bias density is 1.0
stages.1.1.gamma density is 1.0
stages.1.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.1.norm.weight density is 1.0
stages.1.1.norm.bias density is 1.0
stages.1.1.pwconv1.weight density is 1.0
stages.1.1.pwconv1.bias density is 1.0
stages.1.1.pwconv2.weight density is 1.0
stages.1.1.pwconv2.bias density is 1.0
stages.1.2.gamma density is 1.0
stages.1.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.2.norm.weight density is 1.0
stages.1.2.norm.bias density is 1.0
stages.1.2.pwconv1.weight density is 1.0
stages.1.2.pwconv1.bias density is 1.0
stages.1.2.pwconv2.weight density is 1.0
stages.1.2.pwconv2.bias density is 1.0
stages.2.0.gamma density is 1.0
stages.2.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.0.norm.weight density is 1.0
stages.2.0.norm.bias density is 1.0
stages.2.0.pwconv1.weight density is 1.0
stages.2.0.pwconv1.bias density is 1.0
stages.2.0.pwconv2.weight density is 1.0
stages.2.0.pwconv2.bias density is 1.0
stages.2.1.gamma density is 1.0
stages.2.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.1.norm.weight density is 1.0
stages.2.1.norm.bias density is 1.0
stages.2.1.pwconv1.weight density is 1.0
stages.2.1.pwconv1.bias density is 1.0
stages.2.1.pwconv2.weight density is 1.0
stages.2.1.pwconv2.bias density is 1.0
stages.2.2.gamma density is 1.0
stages.2.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.2.norm.weight density is 1.0
stages.2.2.norm.bias density is 1.0
stages.2.2.pwconv1.weight density is 1.0
stages.2.2.pwconv1.bias density is 1.0
stages.2.2.pwconv2.weight density is 1.0
stages.2.2.pwconv2.bias density is 1.0
stages.2.3.gamma density is 1.0
stages.2.3.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.3.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.3.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.3.norm.weight density is 1.0
stages.2.3.norm.bias density is 1.0
stages.2.3.pwconv1.weight density is 1.0
stages.2.3.pwconv1.bias density is 1.0
stages.2.3.pwconv2.weight density is 1.0
stages.2.3.pwconv2.bias density is 1.0
stages.2.4.gamma density is 1.0
stages.2.4.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.4.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.4.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.4.norm.weight density is 1.0
stages.2.4.norm.bias density is 1.0
stages.2.4.pwconv1.weight density is 1.0
stages.2.4.pwconv1.bias density is 1.0
stages.2.4.pwconv2.weight density is 1.0
stages.2.4.pwconv2.bias density is 1.0
stages.2.5.gamma density is 1.0
stages.2.5.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.5.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.5.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.5.norm.weight density is 1.0
stages.2.5.norm.bias density is 1.0
stages.2.5.pwconv1.weight density is 1.0
stages.2.5.pwconv1.bias density is 1.0
stages.2.5.pwconv2.weight density is 1.0
stages.2.5.pwconv2.bias density is 1.0
stages.2.6.gamma density is 1.0
stages.2.6.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.6.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.6.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.6.norm.weight density is 1.0
stages.2.6.norm.bias density is 1.0
stages.2.6.pwconv1.weight density is 1.0
stages.2.6.pwconv1.bias density is 1.0
stages.2.6.pwconv2.weight density is 1.0
stages.2.6.pwconv2.bias density is 1.0
stages.2.7.gamma density is 1.0
stages.2.7.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.7.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.7.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.7.norm.weight density is 1.0
stages.2.7.norm.bias density is 1.0
stages.2.7.pwconv1.weight density is 1.0
stages.2.7.pwconv1.bias density is 1.0
stages.2.7.pwconv2.weight density is 1.0
stages.2.7.pwconv2.bias density is 1.0
stages.2.8.gamma density is 1.0
stages.2.8.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.8.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.8.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.8.norm.weight density is 1.0
stages.2.8.norm.bias density is 1.0
stages.2.8.pwconv1.weight density is 1.0
stages.2.8.pwconv1.bias density is 1.0
stages.2.8.pwconv2.weight density is 1.0
stages.2.8.pwconv2.bias density is 1.0
stages.3.0.gamma density is 1.0
stages.3.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.0.norm.weight density is 1.0
stages.3.0.norm.bias density is 1.0
stages.3.0.pwconv1.weight density is 1.0
stages.3.0.pwconv1.bias density is 1.0
stages.3.0.pwconv2.weight density is 1.0
stages.3.0.pwconv2.bias density is 1.0
stages.3.1.gamma density is 1.0
stages.3.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.1.norm.weight density is 1.0
stages.3.1.norm.bias density is 1.0
stages.3.1.pwconv1.weight density is 1.0
stages.3.1.pwconv1.bias density is 1.0
stages.3.1.pwconv2.weight density is 1.0
stages.3.1.pwconv2.bias density is 1.0
stages.3.2.gamma density is 1.0
stages.3.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.2.norm.weight density is 1.0
stages.3.2.norm.bias density is 1.0
stages.3.2.pwconv1.weight density is 1.0
stages.3.2.pwconv1.bias density is 1.0
stages.3.2.pwconv2.weight density is 1.0
stages.3.2.pwconv2.bias density is 1.0
norm.weight density is 1.0
norm.bias density is 1.0
head.weight density is 1.0
head.bias density is 1.0
Test:  [  0/531]  eta: 0:30:37  loss: 4.6244 (4.6244)  acc1: 21.8750 (21.8750)  acc5: 47.9167 (47.9167)  time: 3.4607  data: 2.3445  max mem: 2139
Test:  [ 10/531]  eta: 0:03:36  loss: 4.3393 (3.8534)  acc1: 25.0000 (35.0379)  acc5: 48.9583 (53.5985)  time: 0.4160  data: 0.2546  max mem: 2139
Test:  [ 20/531]  eta: 0:02:20  loss: 3.7123 (3.8310)  acc1: 28.1250 (33.4325)  acc5: 52.0833 (55.5060)  time: 0.1165  data: 0.0523  max mem: 2139
Test:  [ 30/531]  eta: 0:02:03  loss: 4.2338 (4.3680)  acc1: 22.9167 (28.9987)  acc5: 46.8750 (49.5632)  time: 0.1525  data: 0.0894  max mem: 2139
Test:  [ 40/531]  eta: 0:01:49  loss: 4.6186 (4.3250)  acc1: 21.8750 (29.2429)  acc5: 43.7500 (50.0508)  time: 0.1658  data: 0.1027  max mem: 2139
Test:  [ 50/531]  eta: 0:01:41  loss: 3.7375 (4.2072)  acc1: 29.1667 (30.6781)  acc5: 52.0833 (51.5319)  time: 0.1574  data: 0.0940  max mem: 2139
Test:  [ 60/531]  eta: 0:01:36  loss: 3.9992 (4.2008)  acc1: 32.2917 (30.8743)  acc5: 52.0833 (51.2466)  time: 0.1715  data: 0.1074  max mem: 2139
Test:  [ 70/531]  eta: 0:01:45  loss: 3.8865 (4.1445)  acc1: 32.2917 (31.2793)  acc5: 54.1667 (51.8193)  time: 0.2765  data: 0.2102  max mem: 2139
Test:  [ 80/531]  eta: 0:01:43  loss: 3.8314 (4.1203)  acc1: 29.1667 (31.4686)  acc5: 50.0000 (51.7361)  time: 0.3020  data: 0.2361  max mem: 2139
Test:  [ 90/531]  eta: 0:01:35  loss: 3.4478 (4.0324)  acc1: 30.2083 (32.2917)  acc5: 55.2083 (53.1479)  time: 0.1717  data: 0.1081  max mem: 2139
Test:  [100/531]  eta: 0:01:30  loss: 2.9328 (3.9301)  acc1: 41.6667 (33.2508)  acc5: 72.9167 (54.8164)  time: 0.1327  data: 0.0688  max mem: 2139
Test:  [110/531]  eta: 0:01:27  loss: 2.9328 (3.8430)  acc1: 42.7083 (33.8213)  acc5: 73.9583 (56.4283)  time: 0.1632  data: 0.0981  max mem: 2139
Test:  [120/531]  eta: 0:01:21  loss: 2.9316 (3.7846)  acc1: 37.5000 (34.2975)  acc5: 72.9167 (57.4380)  time: 0.1370  data: 0.0723  max mem: 2139
Test:  [130/531]  eta: 0:01:18  loss: 3.0923 (3.7453)  acc1: 37.5000 (34.5579)  acc5: 68.7500 (57.8244)  time: 0.1258  data: 0.0621  max mem: 2139
Test:  [140/531]  eta: 0:01:15  loss: 2.9290 (3.6796)  acc1: 39.5833 (35.4093)  acc5: 68.7500 (58.8874)  time: 0.1664  data: 0.1023  max mem: 2139
Test:  [150/531]  eta: 0:01:14  loss: 2.7765 (3.6830)  acc1: 31.2500 (35.2097)  acc5: 66.6667 (58.9197)  time: 0.2003  data: 0.1363  max mem: 2139
Test:  [160/531]  eta: 0:01:12  loss: 3.4895 (3.6821)  acc1: 34.3750 (35.6625)  acc5: 61.4583 (58.9286)  time: 0.2110  data: 0.1473  max mem: 2139
Test:  [170/531]  eta: 0:01:09  loss: 3.5169 (3.6898)  acc1: 37.5000 (35.7334)  acc5: 61.4583 (58.8450)  time: 0.1663  data: 0.1023  max mem: 2139
Test:  [180/531]  eta: 0:01:06  loss: 3.9283 (3.6862)  acc1: 34.3750 (36.0958)  acc5: 52.0833 (58.8168)  time: 0.1357  data: 0.0717  max mem: 2139
Test:  [190/531]  eta: 0:01:04  loss: 3.3100 (3.6768)  acc1: 38.5417 (36.3929)  acc5: 60.4167 (58.8842)  time: 0.1754  data: 0.1120  max mem: 2139
Test:  [200/531]  eta: 0:01:02  loss: 3.3581 (3.6855)  acc1: 35.4167 (36.3080)  acc5: 54.1667 (58.5769)  time: 0.1905  data: 0.1271  max mem: 2139
Test:  [210/531]  eta: 0:01:01  loss: 4.1943 (3.7271)  acc1: 27.0833 (35.9498)  acc5: 48.9583 (57.9483)  time: 0.2002  data: 0.1368  max mem: 2139
Test:  [220/531]  eta: 0:00:59  loss: 4.0303 (3.7298)  acc1: 31.2500 (36.1190)  acc5: 48.9583 (57.7017)  time: 0.2278  data: 0.1644  max mem: 2139
Test:  [230/531]  eta: 0:00:57  loss: 3.6585 (3.7464)  acc1: 37.5000 (35.9623)  acc5: 55.2083 (57.3413)  time: 0.1816  data: 0.1182  max mem: 2139
Test:  [240/531]  eta: 0:00:55  loss: 3.4871 (3.7595)  acc1: 37.5000 (35.8792)  acc5: 55.2083 (57.0583)  time: 0.1623  data: 0.0986  max mem: 2139
Test:  [250/531]  eta: 0:00:53  loss: 4.1093 (3.7896)  acc1: 32.2917 (35.5827)  acc5: 52.0833 (56.5779)  time: 0.2096  data: 0.1459  max mem: 2139
Test:  [260/531]  eta: 0:00:51  loss: 4.1093 (3.7946)  acc1: 27.0833 (35.3807)  acc5: 48.9583 (56.4057)  time: 0.2080  data: 0.1450  max mem: 2139
Test:  [270/531]  eta: 0:00:50  loss: 3.3424 (3.7835)  acc1: 37.5000 (35.6127)  acc5: 56.2500 (56.5383)  time: 0.1894  data: 0.1264  max mem: 2139
Test:  [280/531]  eta: 0:00:47  loss: 3.0969 (3.7550)  acc1: 43.7500 (36.0654)  acc5: 61.4583 (56.8209)  time: 0.1686  data: 0.1052  max mem: 2139
Test:  [290/531]  eta: 0:00:45  loss: 3.5136 (3.7580)  acc1: 38.5417 (36.0109)  acc5: 58.3333 (56.7690)  time: 0.1392  data: 0.0758  max mem: 2139
Test:  [300/531]  eta: 0:00:43  loss: 3.5136 (3.7545)  acc1: 32.2917 (36.1088)  acc5: 57.2917 (56.7968)  time: 0.1479  data: 0.0845  max mem: 2139
Test:  [310/531]  eta: 0:00:41  loss: 3.4261 (3.7501)  acc1: 43.7500 (36.3210)  acc5: 57.2917 (56.8261)  time: 0.1918  data: 0.1284  max mem: 2139
Test:  [320/531]  eta: 0:00:39  loss: 3.5714 (3.7424)  acc1: 43.7500 (36.5038)  acc5: 54.1667 (56.7984)  time: 0.2178  data: 0.1540  max mem: 2139
Test:  [330/531]  eta: 0:00:37  loss: 3.3282 (3.7313)  acc1: 38.5417 (36.4804)  acc5: 56.2500 (56.8920)  time: 0.1612  data: 0.0968  max mem: 2139
Test:  [340/531]  eta: 0:00:35  loss: 3.7555 (3.7436)  acc1: 29.1667 (36.2323)  acc5: 52.0833 (56.7174)  time: 0.1243  data: 0.0595  max mem: 2139
Test:  [350/531]  eta: 0:00:33  loss: 3.9536 (3.7514)  acc1: 27.0833 (36.0844)  acc5: 52.0833 (56.5527)  time: 0.1538  data: 0.0897  max mem: 2139
Test:  [360/531]  eta: 0:00:31  loss: 4.2770 (3.7616)  acc1: 23.9583 (35.8004)  acc5: 53.1250 (56.4231)  time: 0.1879  data: 0.1245  max mem: 2139
Test:  [370/531]  eta: 0:00:30  loss: 4.4389 (3.7737)  acc1: 23.9583 (35.6666)  acc5: 51.0417 (56.1995)  time: 0.2264  data: 0.1632  max mem: 2139
Test:  [380/531]  eta: 0:00:28  loss: 3.9372 (3.7734)  acc1: 31.2500 (35.6983)  acc5: 48.9583 (56.1270)  time: 0.2212  data: 0.1576  max mem: 2139
Test:  [390/531]  eta: 0:00:26  loss: 4.0598 (3.7950)  acc1: 29.1667 (35.4779)  acc5: 45.8333 (55.7625)  time: 0.1612  data: 0.0972  max mem: 2139
Test:  [400/531]  eta: 0:00:24  loss: 4.5279 (3.8056)  acc1: 22.9167 (35.3595)  acc5: 43.7500 (55.5980)  time: 0.1625  data: 0.0992  max mem: 2139
Test:  [410/531]  eta: 0:00:22  loss: 3.6627 (3.8002)  acc1: 36.4583 (35.4826)  acc5: 54.1667 (55.6215)  time: 0.1631  data: 0.0998  max mem: 2139
Test:  [420/531]  eta: 0:00:20  loss: 3.7859 (3.8002)  acc1: 36.4583 (35.4711)  acc5: 54.1667 (55.6042)  time: 0.1548  data: 0.0911  max mem: 2139
Test:  [430/531]  eta: 0:00:18  loss: 3.8537 (3.8075)  acc1: 31.2500 (35.3901)  acc5: 54.1667 (55.4718)  time: 0.1808  data: 0.1174  max mem: 2139
Test:  [440/531]  eta: 0:00:16  loss: 3.5561 (3.7995)  acc1: 36.4583 (35.4804)  acc5: 56.2500 (55.5367)  time: 0.2267  data: 0.1620  max mem: 2139
Test:  [450/531]  eta: 0:00:15  loss: 3.1496 (3.7878)  acc1: 40.6250 (35.5783)  acc5: 61.4583 (55.7003)  time: 0.2521  data: 0.1874  max mem: 2139
Test:  [460/531]  eta: 0:00:13  loss: 3.7415 (3.7910)  acc1: 39.5833 (35.5771)  acc5: 55.2083 (55.6015)  time: 0.1997  data: 0.1357  max mem: 2139
Test:  [470/531]  eta: 0:00:11  loss: 3.7415 (3.7816)  acc1: 39.5833 (35.7816)  acc5: 53.1250 (55.6838)  time: 0.1528  data: 0.0891  max mem: 2139
Test:  [480/531]  eta: 0:00:09  loss: 3.6538 (3.7841)  acc1: 34.3750 (35.6441)  acc5: 55.2083 (55.6934)  time: 0.1393  data: 0.0767  max mem: 2139
Test:  [490/531]  eta: 0:00:07  loss: 4.1271 (3.8038)  acc1: 25.0000 (35.4485)  acc5: 46.8750 (55.4099)  time: 0.2066  data: 0.1436  max mem: 2139
Test:  [500/531]  eta: 0:00:05  loss: 5.2068 (3.8552)  acc1: 16.6667 (35.0320)  acc5: 29.1667 (54.7218)  time: 0.2422  data: 0.1786  max mem: 2139
Test:  [510/531]  eta: 0:00:03  loss: 6.5319 (3.9030)  acc1: 7.2917 (34.6013)  acc5: 15.6250 (54.0933)  time: 0.2061  data: 0.1424  max mem: 2139
Test:  [520/531]  eta: 0:00:02  loss: 5.7833 (3.9382)  acc1: 11.4583 (34.2350)  acc5: 25.0000 (53.6108)  time: 0.2016  data: 0.1382  max mem: 2139
Test:  [530/531]  eta: 0:00:00  loss: 5.6613 (3.9750)  acc1: 11.4583 (33.8207)  acc5: 25.0000 (53.1215)  time: 0.1971  data: 0.1360  max mem: 2139
Test: Total time: 0:01:39 (0.1883 s / it)
* Acc@1 33.821 Acc@5 53.122 loss 3.975
Accuracy of the network on 50889 test images: 33.82067%
Not using distributed mode
Namespace(Decom=False, aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=64, bn=True, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/sliu/project_space/datasets/sketch', data_set='IMNET', data_type='imagenet-s', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.1, enable_wandb=False, epochs=300, eval=True, eval_data_path=None, finetune='', fix=False, growth='random', head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, kernel_size=[7, 7, 7, 7, 100], layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=-1, log_dir=None, lr=0.004, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='SLaK_tiny', model_ema=False, model_ema_decay=0.9999, model_ema_eval=False, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_workers=10, only_L=False, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', parallel=True, pin_mem=True, project='SLaK', prune='magnitude', prune_rate=0.3, recount=1, redistribution='none', remode='pixel', reprob=0.25, resplit=False, resume='/gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_SlakT_Teacher_STConvNext_NKD/checkpoint-best.pth', save_ckpt=True, save_ckpt_freq=10, save_ckpt_num=3, seed=0, smoothing=0.1, sparse=False, sparse_init='snip', sparsity=0.4, start_epoch=0, train_interpolation='bicubic', update_freq=1, update_frequency=100, use_amp=False, verbose=False, wandb_ckpt=False, warmup_epochs=20, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, width_factor=1.0, world_size=1)
Transform = 
Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /home/sliu/project_space/datasets/sketch
Number of the class = 1000
Mixup is activated!
Model = SLaK(
  (downsample_layers): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (1): LayerNorm()
    )
    (1): Sequential(
      (0): LayerNorm()
      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))
    )
    (2): Sequential(
      (0): LayerNorm()
      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))
    )
    (3): Sequential(
      (0): LayerNorm()
      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (stages): ModuleList(
    (0): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): Identity()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): DropPath()
      )
    )
    (1): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
    )
    (2): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (3): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (4): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (5): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (6): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (7): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (8): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
    )
    (3): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
number of params: 28595752
LR = 0.00400000
Batch size = 64
Update frequent = 1
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "downsample_layers.0.0.weight",
      "downsample_layers.1.1.weight",
      "downsample_layers.2.1.weight",
      "downsample_layers.3.1.weight",
      "stages.0.0.large_kernel.lkb_origin.conv.weight",
      "stages.0.0.pwconv1.weight",
      "stages.0.0.pwconv2.weight",
      "stages.0.1.large_kernel.lkb_origin.conv.weight",
      "stages.0.1.pwconv1.weight",
      "stages.0.1.pwconv2.weight",
      "stages.0.2.large_kernel.lkb_origin.conv.weight",
      "stages.0.2.pwconv1.weight",
      "stages.0.2.pwconv2.weight",
      "stages.1.0.large_kernel.lkb_origin.conv.weight",
      "stages.1.0.pwconv1.weight",
      "stages.1.0.pwconv2.weight",
      "stages.1.1.large_kernel.lkb_origin.conv.weight",
      "stages.1.1.pwconv1.weight",
      "stages.1.1.pwconv2.weight",
      "stages.1.2.large_kernel.lkb_origin.conv.weight",
      "stages.1.2.pwconv1.weight",
      "stages.1.2.pwconv2.weight",
      "stages.2.0.large_kernel.lkb_origin.conv.weight",
      "stages.2.0.pwconv1.weight",
      "stages.2.0.pwconv2.weight",
      "stages.2.1.large_kernel.lkb_origin.conv.weight",
      "stages.2.1.pwconv1.weight",
      "stages.2.1.pwconv2.weight",
      "stages.2.2.large_kernel.lkb_origin.conv.weight",
      "stages.2.2.pwconv1.weight",
      "stages.2.2.pwconv2.weight",
      "stages.2.3.large_kernel.lkb_origin.conv.weight",
      "stages.2.3.pwconv1.weight",
      "stages.2.3.pwconv2.weight",
      "stages.2.4.large_kernel.lkb_origin.conv.weight",
      "stages.2.4.pwconv1.weight",
      "stages.2.4.pwconv2.weight",
      "stages.2.5.large_kernel.lkb_origin.conv.weight",
      "stages.2.5.pwconv1.weight",
      "stages.2.5.pwconv2.weight",
      "stages.2.6.large_kernel.lkb_origin.conv.weight",
      "stages.2.6.pwconv1.weight",
      "stages.2.6.pwconv2.weight",
      "stages.2.7.large_kernel.lkb_origin.conv.weight",
      "stages.2.7.pwconv1.weight",
      "stages.2.7.pwconv2.weight",
      "stages.2.8.large_kernel.lkb_origin.conv.weight",
      "stages.2.8.pwconv1.weight",
      "stages.2.8.pwconv2.weight",
      "stages.3.0.large_kernel.lkb_origin.conv.weight",
      "stages.3.0.pwconv1.weight",
      "stages.3.0.pwconv2.weight",
      "stages.3.1.large_kernel.lkb_origin.conv.weight",
      "stages.3.1.pwconv1.weight",
      "stages.3.1.pwconv2.weight",
      "stages.3.2.large_kernel.lkb_origin.conv.weight",
      "stages.3.2.pwconv1.weight",
      "stages.3.2.pwconv2.weight",
      "head.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "downsample_layers.0.0.bias",
      "downsample_layers.0.1.weight",
      "downsample_layers.0.1.bias",
      "downsample_layers.1.0.weight",
      "downsample_layers.1.0.bias",
      "downsample_layers.1.1.bias",
      "downsample_layers.2.0.weight",
      "downsample_layers.2.0.bias",
      "downsample_layers.2.1.bias",
      "downsample_layers.3.0.weight",
      "downsample_layers.3.0.bias",
      "downsample_layers.3.1.bias",
      "stages.0.0.gamma",
      "stages.0.0.large_kernel.lkb_origin.bn.weight",
      "stages.0.0.large_kernel.lkb_origin.bn.bias",
      "stages.0.0.norm.weight",
      "stages.0.0.norm.bias",
      "stages.0.0.pwconv1.bias",
      "stages.0.0.pwconv2.bias",
      "stages.0.1.gamma",
      "stages.0.1.large_kernel.lkb_origin.bn.weight",
      "stages.0.1.large_kernel.lkb_origin.bn.bias",
      "stages.0.1.norm.weight",
      "stages.0.1.norm.bias",
      "stages.0.1.pwconv1.bias",
      "stages.0.1.pwconv2.bias",
      "stages.0.2.gamma",
      "stages.0.2.large_kernel.lkb_origin.bn.weight",
      "stages.0.2.large_kernel.lkb_origin.bn.bias",
      "stages.0.2.norm.weight",
      "stages.0.2.norm.bias",
      "stages.0.2.pwconv1.bias",
      "stages.0.2.pwconv2.bias",
      "stages.1.0.gamma",
      "stages.1.0.large_kernel.lkb_origin.bn.weight",
      "stages.1.0.large_kernel.lkb_origin.bn.bias",
      "stages.1.0.norm.weight",
      "stages.1.0.norm.bias",
      "stages.1.0.pwconv1.bias",
      "stages.1.0.pwconv2.bias",
      "stages.1.1.gamma",
      "stages.1.1.large_kernel.lkb_origin.bn.weight",
      "stages.1.1.large_kernel.lkb_origin.bn.bias",
      "stages.1.1.norm.weight",
      "stages.1.1.norm.bias",
      "stages.1.1.pwconv1.bias",
      "stages.1.1.pwconv2.bias",
      "stages.1.2.gamma",
      "stages.1.2.large_kernel.lkb_origin.bn.weight",
      "stages.1.2.large_kernel.lkb_origin.bn.bias",
      "stages.1.2.norm.weight",
      "stages.1.2.norm.bias",
      "stages.1.2.pwconv1.bias",
      "stages.1.2.pwconv2.bias",
      "stages.2.0.gamma",
      "stages.2.0.large_kernel.lkb_origin.bn.weight",
      "stages.2.0.large_kernel.lkb_origin.bn.bias",
      "stages.2.0.norm.weight",
      "stages.2.0.norm.bias",
      "stages.2.0.pwconv1.bias",
      "stages.2.0.pwconv2.bias",
      "stages.2.1.gamma",
      "stages.2.1.large_kernel.lkb_origin.bn.weight",
      "stages.2.1.large_kernel.lkb_origin.bn.bias",
      "stages.2.1.norm.weight",
      "stages.2.1.norm.bias",
      "stages.2.1.pwconv1.bias",
      "stages.2.1.pwconv2.bias",
      "stages.2.2.gamma",
      "stages.2.2.large_kernel.lkb_origin.bn.weight",
      "stages.2.2.large_kernel.lkb_origin.bn.bias",
      "stages.2.2.norm.weight",
      "stages.2.2.norm.bias",
      "stages.2.2.pwconv1.bias",
      "stages.2.2.pwconv2.bias",
      "stages.2.3.gamma",
      "stages.2.3.large_kernel.lkb_origin.bn.weight",
      "stages.2.3.large_kernel.lkb_origin.bn.bias",
      "stages.2.3.norm.weight",
      "stages.2.3.norm.bias",
      "stages.2.3.pwconv1.bias",
      "stages.2.3.pwconv2.bias",
      "stages.2.4.gamma",
      "stages.2.4.large_kernel.lkb_origin.bn.weight",
      "stages.2.4.large_kernel.lkb_origin.bn.bias",
      "stages.2.4.norm.weight",
      "stages.2.4.norm.bias",
      "stages.2.4.pwconv1.bias",
      "stages.2.4.pwconv2.bias",
      "stages.2.5.gamma",
      "stages.2.5.large_kernel.lkb_origin.bn.weight",
      "stages.2.5.large_kernel.lkb_origin.bn.bias",
      "stages.2.5.norm.weight",
      "stages.2.5.norm.bias",
      "stages.2.5.pwconv1.bias",
      "stages.2.5.pwconv2.bias",
      "stages.2.6.gamma",
      "stages.2.6.large_kernel.lkb_origin.bn.weight",
      "stages.2.6.large_kernel.lkb_origin.bn.bias",
      "stages.2.6.norm.weight",
      "stages.2.6.norm.bias",
      "stages.2.6.pwconv1.bias",
      "stages.2.6.pwconv2.bias",
      "stages.2.7.gamma",
      "stages.2.7.large_kernel.lkb_origin.bn.weight",
      "stages.2.7.large_kernel.lkb_origin.bn.bias",
      "stages.2.7.norm.weight",
      "stages.2.7.norm.bias",
      "stages.2.7.pwconv1.bias",
      "stages.2.7.pwconv2.bias",
      "stages.2.8.gamma",
      "stages.2.8.large_kernel.lkb_origin.bn.weight",
      "stages.2.8.large_kernel.lkb_origin.bn.bias",
      "stages.2.8.norm.weight",
      "stages.2.8.norm.bias",
      "stages.2.8.pwconv1.bias",
      "stages.2.8.pwconv2.bias",
      "stages.3.0.gamma",
      "stages.3.0.large_kernel.lkb_origin.bn.weight",
      "stages.3.0.large_kernel.lkb_origin.bn.bias",
      "stages.3.0.norm.weight",
      "stages.3.0.norm.bias",
      "stages.3.0.pwconv1.bias",
      "stages.3.0.pwconv2.bias",
      "stages.3.1.gamma",
      "stages.3.1.large_kernel.lkb_origin.bn.weight",
      "stages.3.1.large_kernel.lkb_origin.bn.bias",
      "stages.3.1.norm.weight",
      "stages.3.1.norm.bias",
      "stages.3.1.pwconv1.bias",
      "stages.3.1.pwconv2.bias",
      "stages.3.2.gamma",
      "stages.3.2.large_kernel.lkb_origin.bn.weight",
      "stages.3.2.large_kernel.lkb_origin.bn.bias",
      "stages.3.2.norm.weight",
      "stages.3.2.norm.bias",
      "stages.3.2.pwconv1.bias",
      "stages.3.2.pwconv2.bias",
      "norm.weight",
      "norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
criterion = SoftTargetCrossEntropy()
dict_keys(['model', 'optimizer', 'epoch', 'scaler', 'args', 'model_ema'])
Resume checkpoint /gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_SlakT_Teacher_STConvNext_NKD/checkpoint-best.pth
With optim & sched!
downsample_layers.0.0.weight density is 1.0
downsample_layers.0.0.bias density is 1.0
downsample_layers.0.1.weight density is 1.0
downsample_layers.0.1.bias density is 1.0
downsample_layers.1.0.weight density is 1.0
downsample_layers.1.0.bias density is 1.0
downsample_layers.1.1.weight density is 1.0
downsample_layers.1.1.bias density is 1.0
downsample_layers.2.0.weight density is 1.0
downsample_layers.2.0.bias density is 1.0
downsample_layers.2.1.weight density is 1.0
downsample_layers.2.1.bias density is 1.0
downsample_layers.3.0.weight density is 1.0
downsample_layers.3.0.bias density is 1.0
downsample_layers.3.1.weight density is 1.0
downsample_layers.3.1.bias density is 1.0
stages.0.0.gamma density is 1.0
stages.0.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.0.norm.weight density is 1.0
stages.0.0.norm.bias density is 1.0
stages.0.0.pwconv1.weight density is 1.0
stages.0.0.pwconv1.bias density is 1.0
stages.0.0.pwconv2.weight density is 1.0
stages.0.0.pwconv2.bias density is 1.0
stages.0.1.gamma density is 1.0
stages.0.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.1.norm.weight density is 1.0
stages.0.1.norm.bias density is 1.0
stages.0.1.pwconv1.weight density is 1.0
stages.0.1.pwconv1.bias density is 1.0
stages.0.1.pwconv2.weight density is 1.0
stages.0.1.pwconv2.bias density is 1.0
stages.0.2.gamma density is 1.0
stages.0.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.2.norm.weight density is 1.0
stages.0.2.norm.bias density is 1.0
stages.0.2.pwconv1.weight density is 1.0
stages.0.2.pwconv1.bias density is 1.0
stages.0.2.pwconv2.weight density is 1.0
stages.0.2.pwconv2.bias density is 1.0
stages.1.0.gamma density is 1.0
stages.1.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.0.norm.weight density is 1.0
stages.1.0.norm.bias density is 1.0
stages.1.0.pwconv1.weight density is 1.0
stages.1.0.pwconv1.bias density is 1.0
stages.1.0.pwconv2.weight density is 1.0
stages.1.0.pwconv2.bias density is 1.0
stages.1.1.gamma density is 1.0
stages.1.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.1.norm.weight density is 1.0
stages.1.1.norm.bias density is 1.0
stages.1.1.pwconv1.weight density is 1.0
stages.1.1.pwconv1.bias density is 1.0
stages.1.1.pwconv2.weight density is 1.0
stages.1.1.pwconv2.bias density is 1.0
stages.1.2.gamma density is 1.0
stages.1.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.2.norm.weight density is 1.0
stages.1.2.norm.bias density is 1.0
stages.1.2.pwconv1.weight density is 1.0
stages.1.2.pwconv1.bias density is 1.0
stages.1.2.pwconv2.weight density is 1.0
stages.1.2.pwconv2.bias density is 1.0
stages.2.0.gamma density is 1.0
stages.2.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.0.norm.weight density is 1.0
stages.2.0.norm.bias density is 1.0
stages.2.0.pwconv1.weight density is 1.0
stages.2.0.pwconv1.bias density is 1.0
stages.2.0.pwconv2.weight density is 1.0
stages.2.0.pwconv2.bias density is 1.0
stages.2.1.gamma density is 1.0
stages.2.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.1.norm.weight density is 1.0
stages.2.1.norm.bias density is 1.0
stages.2.1.pwconv1.weight density is 1.0
stages.2.1.pwconv1.bias density is 1.0
stages.2.1.pwconv2.weight density is 1.0
stages.2.1.pwconv2.bias density is 1.0
stages.2.2.gamma density is 1.0
stages.2.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.2.norm.weight density is 1.0
stages.2.2.norm.bias density is 1.0
stages.2.2.pwconv1.weight density is 1.0
stages.2.2.pwconv1.bias density is 1.0
stages.2.2.pwconv2.weight density is 1.0
stages.2.2.pwconv2.bias density is 1.0
stages.2.3.gamma density is 1.0
stages.2.3.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.3.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.3.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.3.norm.weight density is 1.0
stages.2.3.norm.bias density is 1.0
stages.2.3.pwconv1.weight density is 1.0
stages.2.3.pwconv1.bias density is 1.0
stages.2.3.pwconv2.weight density is 1.0
stages.2.3.pwconv2.bias density is 1.0
stages.2.4.gamma density is 1.0
stages.2.4.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.4.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.4.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.4.norm.weight density is 1.0
stages.2.4.norm.bias density is 1.0
stages.2.4.pwconv1.weight density is 1.0
stages.2.4.pwconv1.bias density is 1.0
stages.2.4.pwconv2.weight density is 1.0
stages.2.4.pwconv2.bias density is 1.0
stages.2.5.gamma density is 1.0
stages.2.5.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.5.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.5.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.5.norm.weight density is 1.0
stages.2.5.norm.bias density is 1.0
stages.2.5.pwconv1.weight density is 1.0
stages.2.5.pwconv1.bias density is 1.0
stages.2.5.pwconv2.weight density is 1.0
stages.2.5.pwconv2.bias density is 1.0
stages.2.6.gamma density is 1.0
stages.2.6.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.6.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.6.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.6.norm.weight density is 1.0
stages.2.6.norm.bias density is 1.0
stages.2.6.pwconv1.weight density is 1.0
stages.2.6.pwconv1.bias density is 1.0
stages.2.6.pwconv2.weight density is 1.0
stages.2.6.pwconv2.bias density is 1.0
stages.2.7.gamma density is 1.0
stages.2.7.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.7.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.7.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.7.norm.weight density is 1.0
stages.2.7.norm.bias density is 1.0
stages.2.7.pwconv1.weight density is 1.0
stages.2.7.pwconv1.bias density is 1.0
stages.2.7.pwconv2.weight density is 1.0
stages.2.7.pwconv2.bias density is 1.0
stages.2.8.gamma density is 1.0
stages.2.8.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.8.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.8.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.8.norm.weight density is 1.0
stages.2.8.norm.bias density is 1.0
stages.2.8.pwconv1.weight density is 1.0
stages.2.8.pwconv1.bias density is 1.0
stages.2.8.pwconv2.weight density is 1.0
stages.2.8.pwconv2.bias density is 1.0
stages.3.0.gamma density is 1.0
stages.3.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.0.norm.weight density is 1.0
stages.3.0.norm.bias density is 1.0
stages.3.0.pwconv1.weight density is 1.0
stages.3.0.pwconv1.bias density is 1.0
stages.3.0.pwconv2.weight density is 1.0
stages.3.0.pwconv2.bias density is 1.0
stages.3.1.gamma density is 1.0
stages.3.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.1.norm.weight density is 1.0
stages.3.1.norm.bias density is 1.0
stages.3.1.pwconv1.weight density is 1.0
stages.3.1.pwconv1.bias density is 1.0
stages.3.1.pwconv2.weight density is 1.0
stages.3.1.pwconv2.bias density is 1.0
stages.3.2.gamma density is 1.0
stages.3.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.2.norm.weight density is 1.0
stages.3.2.norm.bias density is 1.0
stages.3.2.pwconv1.weight density is 1.0
stages.3.2.pwconv1.bias density is 1.0
stages.3.2.pwconv2.weight density is 1.0
stages.3.2.pwconv2.bias density is 1.0
norm.weight density is 1.0
norm.bias density is 1.0
head.weight density is 1.0
head.bias density is 1.0
Test:  [  0/531]  eta: 0:34:10  loss: 4.1755 (4.1755)  acc1: 28.1250 (28.1250)  acc5: 54.1667 (54.1667)  time: 3.8619  data: 2.7591  max mem: 2139
Test:  [ 10/531]  eta: 0:04:09  loss: 4.1755 (3.6799)  acc1: 30.2083 (37.6894)  acc5: 54.1667 (57.1023)  time: 0.4782  data: 0.3188  max mem: 2139
Test:  [ 20/531]  eta: 0:02:35  loss: 3.5325 (3.5898)  acc1: 31.2500 (36.0119)  acc5: 56.2500 (59.1270)  time: 0.1256  data: 0.0614  max mem: 2139
Test:  [ 30/531]  eta: 0:02:11  loss: 4.2578 (4.1650)  acc1: 26.0417 (31.1828)  acc5: 48.9583 (51.6465)  time: 0.1437  data: 0.0807  max mem: 2139
Test:  [ 40/531]  eta: 0:01:56  loss: 4.5679 (4.1855)  acc1: 25.0000 (30.6911)  acc5: 45.8333 (51.6514)  time: 0.1662  data: 0.1032  max mem: 2139
Test:  [ 50/531]  eta: 0:01:52  loss: 3.7570 (4.0812)  acc1: 32.2917 (32.2712)  acc5: 50.0000 (52.9412)  time: 0.1873  data: 0.1231  max mem: 2139
Test:  [ 60/531]  eta: 0:01:47  loss: 3.8423 (4.0724)  acc1: 32.2917 (32.2746)  acc5: 52.0833 (52.6298)  time: 0.2080  data: 0.1435  max mem: 2139
Test:  [ 70/531]  eta: 0:01:56  loss: 3.7475 (4.0269)  acc1: 32.2917 (32.5998)  acc5: 53.1250 (53.2277)  time: 0.3033  data: 0.2387  max mem: 2139
Test:  [ 80/531]  eta: 0:01:52  loss: 4.0320 (4.0078)  acc1: 29.1667 (33.0118)  acc5: 54.1667 (53.4208)  time: 0.3209  data: 0.2555  max mem: 2139
Test:  [ 90/531]  eta: 0:01:44  loss: 3.6360 (3.9217)  acc1: 29.1667 (33.8942)  acc5: 59.3750 (54.7161)  time: 0.1800  data: 0.1151  max mem: 2139
Test:  [100/531]  eta: 0:01:37  loss: 2.7220 (3.8123)  acc1: 43.7500 (35.0144)  acc5: 72.9167 (56.3531)  time: 0.1255  data: 0.0618  max mem: 2139
Test:  [110/531]  eta: 0:01:32  loss: 2.7220 (3.7327)  acc1: 43.7500 (35.6982)  acc5: 72.9167 (57.8735)  time: 0.1391  data: 0.0757  max mem: 2139
Test:  [120/531]  eta: 0:01:26  loss: 2.7938 (3.6708)  acc1: 42.7083 (36.2603)  acc5: 71.8750 (58.9015)  time: 0.1270  data: 0.0630  max mem: 2139
Test:  [130/531]  eta: 0:01:21  loss: 2.9673 (3.6232)  acc1: 42.7083 (36.7446)  acc5: 69.7917 (59.6454)  time: 0.1203  data: 0.0563  max mem: 2139
Test:  [140/531]  eta: 0:01:18  loss: 2.8985 (3.5570)  acc1: 42.7083 (37.5960)  acc5: 72.9167 (60.5866)  time: 0.1456  data: 0.0826  max mem: 2139
Test:  [150/531]  eta: 0:01:18  loss: 2.7930 (3.5645)  acc1: 33.3333 (37.2724)  acc5: 68.7500 (60.5132)  time: 0.2183  data: 0.1550  max mem: 2139
Test:  [160/531]  eta: 0:01:15  loss: 3.0428 (3.5543)  acc1: 35.4167 (37.6229)  acc5: 63.5417 (60.6755)  time: 0.2353  data: 0.1709  max mem: 2139
Test:  [170/531]  eta: 0:01:12  loss: 3.3290 (3.5677)  acc1: 35.4167 (37.6523)  acc5: 61.4583 (60.5141)  time: 0.1647  data: 0.1006  max mem: 2139
Test:  [180/531]  eta: 0:01:09  loss: 3.4248 (3.5684)  acc1: 34.3750 (37.8798)  acc5: 57.2917 (60.3821)  time: 0.1404  data: 0.0770  max mem: 2139
Test:  [190/531]  eta: 0:01:07  loss: 2.9806 (3.5540)  acc1: 36.4583 (38.1326)  acc5: 66.6667 (60.5694)  time: 0.1843  data: 0.1209  max mem: 2139
Test:  [200/531]  eta: 0:01:05  loss: 3.2192 (3.5593)  acc1: 36.4583 (37.9871)  acc5: 61.4583 (60.2197)  time: 0.2024  data: 0.1394  max mem: 2139
Test:  [210/531]  eta: 0:01:03  loss: 3.9624 (3.5955)  acc1: 29.1667 (37.6481)  acc5: 50.0000 (59.6169)  time: 0.1797  data: 0.1167  max mem: 2139
Test:  [220/531]  eta: 0:01:01  loss: 3.9624 (3.6039)  acc1: 35.4167 (37.7168)  acc5: 50.0000 (59.2525)  time: 0.2007  data: 0.1373  max mem: 2139
Test:  [230/531]  eta: 0:00:58  loss: 3.9269 (3.6200)  acc1: 37.5000 (37.5271)  acc5: 52.0833 (58.9962)  time: 0.1753  data: 0.1115  max mem: 2139
Test:  [240/531]  eta: 0:00:57  loss: 3.5755 (3.6316)  acc1: 37.5000 (37.4049)  acc5: 56.2500 (58.6834)  time: 0.1809  data: 0.1169  max mem: 2139
Test:  [250/531]  eta: 0:00:55  loss: 3.8445 (3.6605)  acc1: 31.2500 (37.0725)  acc5: 53.1250 (58.2337)  time: 0.2405  data: 0.1765  max mem: 2139
Test:  [260/531]  eta: 0:00:53  loss: 4.2616 (3.6707)  acc1: 29.1667 (36.8175)  acc5: 48.9583 (58.0101)  time: 0.2187  data: 0.1553  max mem: 2139
Test:  [270/531]  eta: 0:00:51  loss: 3.3866 (3.6630)  acc1: 36.4583 (36.9696)  acc5: 59.3750 (58.0720)  time: 0.1830  data: 0.1199  max mem: 2139
Test:  [280/531]  eta: 0:00:50  loss: 3.3043 (3.6391)  acc1: 40.6250 (37.3888)  acc5: 61.4583 (58.2703)  time: 0.2214  data: 0.1558  max mem: 2139
Test:  [290/531]  eta: 0:00:47  loss: 3.7285 (3.6463)  acc1: 35.4167 (37.2244)  acc5: 53.1250 (58.1723)  time: 0.1891  data: 0.1237  max mem: 2139
Test:  [300/531]  eta: 0:00:45  loss: 3.4713 (3.6422)  acc1: 33.3333 (37.3720)  acc5: 53.1250 (58.2157)  time: 0.1469  data: 0.0836  max mem: 2139
Test:  [310/531]  eta: 0:00:43  loss: 3.4110 (3.6399)  acc1: 42.7083 (37.5569)  acc5: 60.4167 (58.2195)  time: 0.1931  data: 0.1293  max mem: 2139
Test:  [320/531]  eta: 0:00:41  loss: 3.4135 (3.6345)  acc1: 42.7083 (37.7401)  acc5: 57.2917 (58.2100)  time: 0.1972  data: 0.1334  max mem: 2139
Test:  [330/531]  eta: 0:00:39  loss: 3.0675 (3.6217)  acc1: 41.6667 (37.7864)  acc5: 61.4583 (58.3585)  time: 0.1750  data: 0.1113  max mem: 2139
Test:  [340/531]  eta: 0:00:37  loss: 3.4111 (3.6370)  acc1: 36.4583 (37.5397)  acc5: 59.3750 (58.0859)  time: 0.1667  data: 0.1030  max mem: 2139
Test:  [350/531]  eta: 0:00:35  loss: 3.9359 (3.6457)  acc1: 26.0417 (37.3932)  acc5: 53.1250 (57.8911)  time: 0.1721  data: 0.1080  max mem: 2139
Test:  [360/531]  eta: 0:00:32  loss: 4.0308 (3.6507)  acc1: 27.0833 (37.1422)  acc5: 55.2083 (57.8515)  time: 0.1602  data: 0.0957  max mem: 2139
Test:  [370/531]  eta: 0:00:31  loss: 4.0308 (3.6635)  acc1: 27.0833 (37.0423)  acc5: 56.2500 (57.6454)  time: 0.1608  data: 0.0970  max mem: 2139
Test:  [380/531]  eta: 0:00:28  loss: 3.6064 (3.6626)  acc1: 32.2917 (37.0954)  acc5: 56.2500 (57.6061)  time: 0.1747  data: 0.1104  max mem: 2139
Test:  [390/531]  eta: 0:00:27  loss: 3.6638 (3.6835)  acc1: 30.2083 (36.8739)  acc5: 51.0417 (57.2251)  time: 0.1744  data: 0.1098  max mem: 2139
Test:  [400/531]  eta: 0:00:25  loss: 4.1989 (3.6938)  acc1: 25.0000 (36.7467)  acc5: 44.7917 (57.0137)  time: 0.1840  data: 0.1202  max mem: 2139
Test:  [410/531]  eta: 0:00:23  loss: 3.6254 (3.6906)  acc1: 37.5000 (36.8664)  acc5: 54.1667 (57.0002)  time: 0.1864  data: 0.1229  max mem: 2139
Test:  [420/531]  eta: 0:00:21  loss: 3.7707 (3.6932)  acc1: 37.5000 (36.8616)  acc5: 51.0417 (56.8958)  time: 0.1746  data: 0.1107  max mem: 2139
Test:  [430/531]  eta: 0:00:19  loss: 3.8626 (3.7021)  acc1: 34.3750 (36.7846)  acc5: 51.0417 (56.7334)  time: 0.1681  data: 0.1043  max mem: 2139
Test:  [440/531]  eta: 0:00:17  loss: 3.6071 (3.6931)  acc1: 37.5000 (36.8977)  acc5: 58.3333 (56.8311)  time: 0.2161  data: 0.1527  max mem: 2139
Test:  [450/531]  eta: 0:00:15  loss: 3.1299 (3.6816)  acc1: 38.5417 (37.0173)  acc5: 61.4583 (57.0053)  time: 0.2282  data: 0.1644  max mem: 2139
Test:  [460/531]  eta: 0:00:13  loss: 3.3828 (3.6847)  acc1: 38.5417 (37.0300)  acc5: 55.2083 (56.9053)  time: 0.1864  data: 0.1227  max mem: 2139
Test:  [470/531]  eta: 0:00:11  loss: 3.4049 (3.6758)  acc1: 40.6250 (37.2236)  acc5: 53.1250 (56.9577)  time: 0.1825  data: 0.1190  max mem: 2139
Test:  [480/531]  eta: 0:00:09  loss: 3.3924 (3.6782)  acc1: 38.5417 (37.0734)  acc5: 55.2083 (56.9625)  time: 0.1683  data: 0.1045  max mem: 2139
Test:  [490/531]  eta: 0:00:07  loss: 3.7488 (3.6970)  acc1: 28.1250 (36.8657)  acc5: 48.9583 (56.6658)  time: 0.1878  data: 0.1240  max mem: 2139
Test:  [500/531]  eta: 0:00:05  loss: 5.3386 (3.7488)  acc1: 18.7500 (36.3835)  acc5: 34.3750 (55.9818)  time: 0.2190  data: 0.1552  max mem: 2139
Test:  [510/531]  eta: 0:00:04  loss: 6.2300 (3.7928)  acc1: 9.3750 (35.9569)  acc5: 18.7500 (55.3816)  time: 0.2099  data: 0.1457  max mem: 2139
Test:  [520/531]  eta: 0:00:02  loss: 5.5593 (3.8255)  acc1: 11.4583 (35.5366)  acc5: 28.1250 (54.9024)  time: 0.2174  data: 0.1533  max mem: 2139
Test:  [530/531]  eta: 0:00:00  loss: 5.4205 (3.8611)  acc1: 9.3750 (35.1274)  acc5: 27.0833 (54.4027)  time: 0.2186  data: 0.1567  max mem: 2139
Test: Total time: 0:01:42 (0.1934 s / it)
* Acc@1 35.127 Acc@5 54.403 loss 3.861
Accuracy of the network on 50889 test images: 35.12744%
SLak_kernel_distill_s.sh: line 162: source: deactivate: file not found
