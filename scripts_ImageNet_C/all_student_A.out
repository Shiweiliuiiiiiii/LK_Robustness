Not using distributed mode
Namespace(Decom=False, aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=64, bn=False, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/sliu/project_space/datasets/imagenet-a', data_set='IMNET', data_type='imagenet-a', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.1, enable_wandb=False, epochs=300, eval=True, eval_data_path=None, finetune='', fix=False, growth='random', head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, kernel_size=[7, 7, 7, 7, 100], layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=-1, log_dir=None, lr=0.004, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='resnet50', model_ema=False, model_ema_decay=0.9999, model_ema_eval=False, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_workers=10, only_L=False, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', parallel=True, pin_mem=True, project='SLaK', prune='magnitude', prune_rate=0.3, recount=1, redistribution='none', remode='pixel', reprob=0.25, resplit=False, resume='/gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_VitTeacher_STRN50_NKD/checkpoint-best.pth', save_ckpt=True, save_ckpt_freq=10, save_ckpt_num=3, seed=0, smoothing=0.1, sparse=False, sparse_init='snip', sparsity=0.4, start_epoch=0, train_interpolation='bicubic', update_freq=1, update_frequency=100, use_amp=False, verbose=False, wandb_ckpt=False, warmup_epochs=20, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, width_factor=1.0, world_size=1)
Transform = 
Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /home/sliu/project_space/datasets/imagenet-a
Number of the class = 1000
Mixup is activated!
Model = ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=True)
  (fc): Linear(in_features=2048, out_features=1000, bias=True)
)
number of params: 25557032
LR = 0.00400000
Batch size = 64
Update frequent = 1
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "conv1.weight",
      "layer1.0.conv1.weight",
      "layer1.0.conv2.weight",
      "layer1.0.conv3.weight",
      "layer1.0.downsample.0.weight",
      "layer1.1.conv1.weight",
      "layer1.1.conv2.weight",
      "layer1.1.conv3.weight",
      "layer1.2.conv1.weight",
      "layer1.2.conv2.weight",
      "layer1.2.conv3.weight",
      "layer2.0.conv1.weight",
      "layer2.0.conv2.weight",
      "layer2.0.conv3.weight",
      "layer2.0.downsample.0.weight",
      "layer2.1.conv1.weight",
      "layer2.1.conv2.weight",
      "layer2.1.conv3.weight",
      "layer2.2.conv1.weight",
      "layer2.2.conv2.weight",
      "layer2.2.conv3.weight",
      "layer2.3.conv1.weight",
      "layer2.3.conv2.weight",
      "layer2.3.conv3.weight",
      "layer3.0.conv1.weight",
      "layer3.0.conv2.weight",
      "layer3.0.conv3.weight",
      "layer3.0.downsample.0.weight",
      "layer3.1.conv1.weight",
      "layer3.1.conv2.weight",
      "layer3.1.conv3.weight",
      "layer3.2.conv1.weight",
      "layer3.2.conv2.weight",
      "layer3.2.conv3.weight",
      "layer3.3.conv1.weight",
      "layer3.3.conv2.weight",
      "layer3.3.conv3.weight",
      "layer3.4.conv1.weight",
      "layer3.4.conv2.weight",
      "layer3.4.conv3.weight",
      "layer3.5.conv1.weight",
      "layer3.5.conv2.weight",
      "layer3.5.conv3.weight",
      "layer4.0.conv1.weight",
      "layer4.0.conv2.weight",
      "layer4.0.conv3.weight",
      "layer4.0.downsample.0.weight",
      "layer4.1.conv1.weight",
      "layer4.1.conv2.weight",
      "layer4.1.conv3.weight",
      "layer4.2.conv1.weight",
      "layer4.2.conv2.weight",
      "layer4.2.conv3.weight",
      "fc.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "bn1.weight",
      "bn1.bias",
      "layer1.0.bn1.weight",
      "layer1.0.bn1.bias",
      "layer1.0.bn2.weight",
      "layer1.0.bn2.bias",
      "layer1.0.bn3.weight",
      "layer1.0.bn3.bias",
      "layer1.0.downsample.1.weight",
      "layer1.0.downsample.1.bias",
      "layer1.1.bn1.weight",
      "layer1.1.bn1.bias",
      "layer1.1.bn2.weight",
      "layer1.1.bn2.bias",
      "layer1.1.bn3.weight",
      "layer1.1.bn3.bias",
      "layer1.2.bn1.weight",
      "layer1.2.bn1.bias",
      "layer1.2.bn2.weight",
      "layer1.2.bn2.bias",
      "layer1.2.bn3.weight",
      "layer1.2.bn3.bias",
      "layer2.0.bn1.weight",
      "layer2.0.bn1.bias",
      "layer2.0.bn2.weight",
      "layer2.0.bn2.bias",
      "layer2.0.bn3.weight",
      "layer2.0.bn3.bias",
      "layer2.0.downsample.1.weight",
      "layer2.0.downsample.1.bias",
      "layer2.1.bn1.weight",
      "layer2.1.bn1.bias",
      "layer2.1.bn2.weight",
      "layer2.1.bn2.bias",
      "layer2.1.bn3.weight",
      "layer2.1.bn3.bias",
      "layer2.2.bn1.weight",
      "layer2.2.bn1.bias",
      "layer2.2.bn2.weight",
      "layer2.2.bn2.bias",
      "layer2.2.bn3.weight",
      "layer2.2.bn3.bias",
      "layer2.3.bn1.weight",
      "layer2.3.bn1.bias",
      "layer2.3.bn2.weight",
      "layer2.3.bn2.bias",
      "layer2.3.bn3.weight",
      "layer2.3.bn3.bias",
      "layer3.0.bn1.weight",
      "layer3.0.bn1.bias",
      "layer3.0.bn2.weight",
      "layer3.0.bn2.bias",
      "layer3.0.bn3.weight",
      "layer3.0.bn3.bias",
      "layer3.0.downsample.1.weight",
      "layer3.0.downsample.1.bias",
      "layer3.1.bn1.weight",
      "layer3.1.bn1.bias",
      "layer3.1.bn2.weight",
      "layer3.1.bn2.bias",
      "layer3.1.bn3.weight",
      "layer3.1.bn3.bias",
      "layer3.2.bn1.weight",
      "layer3.2.bn1.bias",
      "layer3.2.bn2.weight",
      "layer3.2.bn2.bias",
      "layer3.2.bn3.weight",
      "layer3.2.bn3.bias",
      "layer3.3.bn1.weight",
      "layer3.3.bn1.bias",
      "layer3.3.bn2.weight",
      "layer3.3.bn2.bias",
      "layer3.3.bn3.weight",
      "layer3.3.bn3.bias",
      "layer3.4.bn1.weight",
      "layer3.4.bn1.bias",
      "layer3.4.bn2.weight",
      "layer3.4.bn2.bias",
      "layer3.4.bn3.weight",
      "layer3.4.bn3.bias",
      "layer3.5.bn1.weight",
      "layer3.5.bn1.bias",
      "layer3.5.bn2.weight",
      "layer3.5.bn2.bias",
      "layer3.5.bn3.weight",
      "layer3.5.bn3.bias",
      "layer4.0.bn1.weight",
      "layer4.0.bn1.bias",
      "layer4.0.bn2.weight",
      "layer4.0.bn2.bias",
      "layer4.0.bn3.weight",
      "layer4.0.bn3.bias",
      "layer4.0.downsample.1.weight",
      "layer4.0.downsample.1.bias",
      "layer4.1.bn1.weight",
      "layer4.1.bn1.bias",
      "layer4.1.bn2.weight",
      "layer4.1.bn2.bias",
      "layer4.1.bn3.weight",
      "layer4.1.bn3.bias",
      "layer4.2.bn1.weight",
      "layer4.2.bn1.bias",
      "layer4.2.bn2.weight",
      "layer4.2.bn2.bias",
      "layer4.2.bn3.weight",
      "layer4.2.bn3.bias",
      "fc.bias"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
criterion = SoftTargetCrossEntropy()
dict_keys(['model', 'optimizer', 'epoch', 'scaler', 'args', 'model_ema'])
Resume checkpoint /gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_VitTeacher_STRN50_NKD/checkpoint-best.pth
With optim & sched!
conv1.weight density is 1.0
bn1.weight density is 1.0
bn1.bias density is 1.0
layer1.0.conv1.weight density is 1.0
layer1.0.bn1.weight density is 1.0
layer1.0.bn1.bias density is 1.0
layer1.0.conv2.weight density is 1.0
layer1.0.bn2.weight density is 1.0
layer1.0.bn2.bias density is 1.0
layer1.0.conv3.weight density is 1.0
layer1.0.bn3.weight density is 1.0
layer1.0.bn3.bias density is 1.0
layer1.0.downsample.0.weight density is 1.0
layer1.0.downsample.1.weight density is 1.0
layer1.0.downsample.1.bias density is 1.0
layer1.1.conv1.weight density is 1.0
layer1.1.bn1.weight density is 1.0
layer1.1.bn1.bias density is 1.0
layer1.1.conv2.weight density is 1.0
layer1.1.bn2.weight density is 1.0
layer1.1.bn2.bias density is 1.0
layer1.1.conv3.weight density is 1.0
layer1.1.bn3.weight density is 1.0
layer1.1.bn3.bias density is 1.0
layer1.2.conv1.weight density is 1.0
layer1.2.bn1.weight density is 1.0
layer1.2.bn1.bias density is 1.0
layer1.2.conv2.weight density is 1.0
layer1.2.bn2.weight density is 1.0
layer1.2.bn2.bias density is 1.0
layer1.2.conv3.weight density is 1.0
layer1.2.bn3.weight density is 1.0
layer1.2.bn3.bias density is 1.0
layer2.0.conv1.weight density is 1.0
layer2.0.bn1.weight density is 1.0
layer2.0.bn1.bias density is 1.0
layer2.0.conv2.weight density is 1.0
layer2.0.bn2.weight density is 1.0
layer2.0.bn2.bias density is 1.0
layer2.0.conv3.weight density is 1.0
layer2.0.bn3.weight density is 1.0
layer2.0.bn3.bias density is 1.0
layer2.0.downsample.0.weight density is 1.0
layer2.0.downsample.1.weight density is 1.0
layer2.0.downsample.1.bias density is 1.0
layer2.1.conv1.weight density is 1.0
layer2.1.bn1.weight density is 1.0
layer2.1.bn1.bias density is 1.0
layer2.1.conv2.weight density is 1.0
layer2.1.bn2.weight density is 1.0
layer2.1.bn2.bias density is 1.0
layer2.1.conv3.weight density is 1.0
layer2.1.bn3.weight density is 1.0
layer2.1.bn3.bias density is 1.0
layer2.2.conv1.weight density is 1.0
layer2.2.bn1.weight density is 1.0
layer2.2.bn1.bias density is 1.0
layer2.2.conv2.weight density is 1.0
layer2.2.bn2.weight density is 1.0
layer2.2.bn2.bias density is 1.0
layer2.2.conv3.weight density is 1.0
layer2.2.bn3.weight density is 1.0
layer2.2.bn3.bias density is 1.0
layer2.3.conv1.weight density is 1.0
layer2.3.bn1.weight density is 1.0
layer2.3.bn1.bias density is 1.0
layer2.3.conv2.weight density is 1.0
layer2.3.bn2.weight density is 1.0
layer2.3.bn2.bias density is 1.0
layer2.3.conv3.weight density is 1.0
layer2.3.bn3.weight density is 1.0
layer2.3.bn3.bias density is 1.0
layer3.0.conv1.weight density is 1.0
layer3.0.bn1.weight density is 1.0
layer3.0.bn1.bias density is 1.0
layer3.0.conv2.weight density is 1.0
layer3.0.bn2.weight density is 1.0
layer3.0.bn2.bias density is 1.0
layer3.0.conv3.weight density is 1.0
layer3.0.bn3.weight density is 1.0
layer3.0.bn3.bias density is 1.0
layer3.0.downsample.0.weight density is 1.0
layer3.0.downsample.1.weight density is 1.0
layer3.0.downsample.1.bias density is 1.0
layer3.1.conv1.weight density is 1.0
layer3.1.bn1.weight density is 1.0
layer3.1.bn1.bias density is 1.0
layer3.1.conv2.weight density is 1.0
layer3.1.bn2.weight density is 1.0
layer3.1.bn2.bias density is 1.0
layer3.1.conv3.weight density is 1.0
layer3.1.bn3.weight density is 1.0
layer3.1.bn3.bias density is 1.0
layer3.2.conv1.weight density is 1.0
layer3.2.bn1.weight density is 1.0
layer3.2.bn1.bias density is 1.0
layer3.2.conv2.weight density is 1.0
layer3.2.bn2.weight density is 1.0
layer3.2.bn2.bias density is 1.0
layer3.2.conv3.weight density is 1.0
layer3.2.bn3.weight density is 1.0
layer3.2.bn3.bias density is 1.0
layer3.3.conv1.weight density is 1.0
layer3.3.bn1.weight density is 1.0
layer3.3.bn1.bias density is 1.0
layer3.3.conv2.weight density is 1.0
layer3.3.bn2.weight density is 1.0
layer3.3.bn2.bias density is 1.0
layer3.3.conv3.weight density is 1.0
layer3.3.bn3.weight density is 1.0
layer3.3.bn3.bias density is 1.0
layer3.4.conv1.weight density is 1.0
layer3.4.bn1.weight density is 1.0
layer3.4.bn1.bias density is 1.0
layer3.4.conv2.weight density is 1.0
layer3.4.bn2.weight density is 1.0
layer3.4.bn2.bias density is 1.0
layer3.4.conv3.weight density is 1.0
layer3.4.bn3.weight density is 1.0
layer3.4.bn3.bias density is 1.0
layer3.5.conv1.weight density is 1.0
layer3.5.bn1.weight density is 1.0
layer3.5.bn1.bias density is 1.0
layer3.5.conv2.weight density is 1.0
layer3.5.bn2.weight density is 1.0
layer3.5.bn2.bias density is 1.0
layer3.5.conv3.weight density is 1.0
layer3.5.bn3.weight density is 1.0
layer3.5.bn3.bias density is 1.0
layer4.0.conv1.weight density is 1.0
layer4.0.bn1.weight density is 1.0
layer4.0.bn1.bias density is 1.0
layer4.0.conv2.weight density is 1.0
layer4.0.bn2.weight density is 1.0
layer4.0.bn2.bias density is 1.0
layer4.0.conv3.weight density is 1.0
layer4.0.bn3.weight density is 1.0
layer4.0.bn3.bias density is 1.0
layer4.0.downsample.0.weight density is 1.0
layer4.0.downsample.1.weight density is 1.0
layer4.0.downsample.1.bias density is 1.0
layer4.1.conv1.weight density is 1.0
layer4.1.bn1.weight density is 1.0
layer4.1.bn1.bias density is 1.0
layer4.1.conv2.weight density is 1.0
layer4.1.bn2.weight density is 1.0
layer4.1.bn2.bias density is 1.0
layer4.1.conv3.weight density is 1.0
layer4.1.bn3.weight density is 1.0
layer4.1.bn3.bias density is 1.0
layer4.2.conv1.weight density is 1.0
layer4.2.bn1.weight density is 1.0
layer4.2.bn1.bias density is 1.0
layer4.2.conv2.weight density is 1.0
layer4.2.bn2.weight density is 1.0
layer4.2.bn2.bias density is 1.0
layer4.2.conv3.weight density is 1.0
layer4.2.bn3.weight density is 1.0
layer4.2.bn3.bias density is 1.0
fc.weight density is 1.0
fc.bias density is 1.0
Test:  [ 0/79]  eta: 0:05:24  loss: 4.5857 (4.5857)  acc1: 3.1250 (3.1250)  acc5: 38.5417 (38.5417)  time: 4.1051  data: 2.1855  max mem: 3726
Test:  [10/79]  eta: 0:00:29  loss: 4.7221 (4.7141)  acc1: 4.1667 (4.9242)  acc5: 29.1667 (27.4621)  time: 0.4277  data: 0.1988  max mem: 3726
Test:  [20/79]  eta: 0:00:15  loss: 4.7221 (4.7453)  acc1: 4.1667 (4.7619)  acc5: 25.0000 (26.8353)  time: 0.0668  data: 0.0001  max mem: 3726
Test:  [30/79]  eta: 0:00:09  loss: 4.8674 (4.7451)  acc1: 4.1667 (4.7043)  acc5: 26.0417 (28.7970)  time: 0.0624  data: 0.0006  max mem: 3726
Test:  [40/79]  eta: 0:00:06  loss: 4.8872 (4.8731)  acc1: 3.1250 (4.2429)  acc5: 25.0000 (27.1087)  time: 0.0859  data: 0.0369  max mem: 3726
Test:  [50/79]  eta: 0:00:04  loss: 4.9814 (4.8877)  acc1: 3.1250 (4.2484)  acc5: 22.9167 (26.7565)  time: 0.1090  data: 0.0611  max mem: 3726
Test:  [60/79]  eta: 0:00:03  loss: 4.9554 (4.9058)  acc1: 3.1250 (4.0301)  acc5: 25.0000 (27.2199)  time: 0.1256  data: 0.0711  max mem: 3726
Test:  [70/79]  eta: 0:00:01  loss: 5.0065 (4.9270)  acc1: 2.0833 (3.8586)  acc5: 26.0417 (27.0540)  time: 0.1334  data: 0.0820  max mem: 3726
Test:  [78/79]  eta: 0:00:00  loss: 5.1376 (4.9558)  acc1: 2.0833 (3.7200)  acc5: 23.9583 (26.8800)  time: 0.1140  data: 0.0556  max mem: 3726
Test: Total time: 0:00:11 (0.1514 s / it)
* Acc@1 3.720 Acc@5 26.880 loss 4.956
Accuracy of the network on 7500 test images: 3.72000%
Not using distributed mode
Namespace(Decom=False, aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=64, bn=False, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/sliu/project_space/datasets/imagenet-a', data_set='IMNET', data_type='imagenet-a', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.1, enable_wandb=False, epochs=300, eval=True, eval_data_path=None, finetune='', fix=False, growth='random', head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, kernel_size=[7, 7, 7, 7, 100], layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=-1, log_dir=None, lr=0.004, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='resnet50', model_ema=False, model_ema_decay=0.9999, model_ema_eval=False, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_workers=10, only_L=False, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', parallel=True, pin_mem=True, project='SLaK', prune='magnitude', prune_rate=0.3, recount=1, redistribution='none', remode='pixel', reprob=0.25, resplit=False, resume='/gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_SwinTeacher_STRN50_NKD/checkpoint-best.pth', save_ckpt=True, save_ckpt_freq=10, save_ckpt_num=3, seed=0, smoothing=0.1, sparse=False, sparse_init='snip', sparsity=0.4, start_epoch=0, train_interpolation='bicubic', update_freq=1, update_frequency=100, use_amp=False, verbose=False, wandb_ckpt=False, warmup_epochs=20, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, width_factor=1.0, world_size=1)
Transform = 
Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /home/sliu/project_space/datasets/imagenet-a
Number of the class = 1000
Mixup is activated!
Model = ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=True)
  (fc): Linear(in_features=2048, out_features=1000, bias=True)
)
number of params: 25557032
LR = 0.00400000
Batch size = 64
Update frequent = 1
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "conv1.weight",
      "layer1.0.conv1.weight",
      "layer1.0.conv2.weight",
      "layer1.0.conv3.weight",
      "layer1.0.downsample.0.weight",
      "layer1.1.conv1.weight",
      "layer1.1.conv2.weight",
      "layer1.1.conv3.weight",
      "layer1.2.conv1.weight",
      "layer1.2.conv2.weight",
      "layer1.2.conv3.weight",
      "layer2.0.conv1.weight",
      "layer2.0.conv2.weight",
      "layer2.0.conv3.weight",
      "layer2.0.downsample.0.weight",
      "layer2.1.conv1.weight",
      "layer2.1.conv2.weight",
      "layer2.1.conv3.weight",
      "layer2.2.conv1.weight",
      "layer2.2.conv2.weight",
      "layer2.2.conv3.weight",
      "layer2.3.conv1.weight",
      "layer2.3.conv2.weight",
      "layer2.3.conv3.weight",
      "layer3.0.conv1.weight",
      "layer3.0.conv2.weight",
      "layer3.0.conv3.weight",
      "layer3.0.downsample.0.weight",
      "layer3.1.conv1.weight",
      "layer3.1.conv2.weight",
      "layer3.1.conv3.weight",
      "layer3.2.conv1.weight",
      "layer3.2.conv2.weight",
      "layer3.2.conv3.weight",
      "layer3.3.conv1.weight",
      "layer3.3.conv2.weight",
      "layer3.3.conv3.weight",
      "layer3.4.conv1.weight",
      "layer3.4.conv2.weight",
      "layer3.4.conv3.weight",
      "layer3.5.conv1.weight",
      "layer3.5.conv2.weight",
      "layer3.5.conv3.weight",
      "layer4.0.conv1.weight",
      "layer4.0.conv2.weight",
      "layer4.0.conv3.weight",
      "layer4.0.downsample.0.weight",
      "layer4.1.conv1.weight",
      "layer4.1.conv2.weight",
      "layer4.1.conv3.weight",
      "layer4.2.conv1.weight",
      "layer4.2.conv2.weight",
      "layer4.2.conv3.weight",
      "fc.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "bn1.weight",
      "bn1.bias",
      "layer1.0.bn1.weight",
      "layer1.0.bn1.bias",
      "layer1.0.bn2.weight",
      "layer1.0.bn2.bias",
      "layer1.0.bn3.weight",
      "layer1.0.bn3.bias",
      "layer1.0.downsample.1.weight",
      "layer1.0.downsample.1.bias",
      "layer1.1.bn1.weight",
      "layer1.1.bn1.bias",
      "layer1.1.bn2.weight",
      "layer1.1.bn2.bias",
      "layer1.1.bn3.weight",
      "layer1.1.bn3.bias",
      "layer1.2.bn1.weight",
      "layer1.2.bn1.bias",
      "layer1.2.bn2.weight",
      "layer1.2.bn2.bias",
      "layer1.2.bn3.weight",
      "layer1.2.bn3.bias",
      "layer2.0.bn1.weight",
      "layer2.0.bn1.bias",
      "layer2.0.bn2.weight",
      "layer2.0.bn2.bias",
      "layer2.0.bn3.weight",
      "layer2.0.bn3.bias",
      "layer2.0.downsample.1.weight",
      "layer2.0.downsample.1.bias",
      "layer2.1.bn1.weight",
      "layer2.1.bn1.bias",
      "layer2.1.bn2.weight",
      "layer2.1.bn2.bias",
      "layer2.1.bn3.weight",
      "layer2.1.bn3.bias",
      "layer2.2.bn1.weight",
      "layer2.2.bn1.bias",
      "layer2.2.bn2.weight",
      "layer2.2.bn2.bias",
      "layer2.2.bn3.weight",
      "layer2.2.bn3.bias",
      "layer2.3.bn1.weight",
      "layer2.3.bn1.bias",
      "layer2.3.bn2.weight",
      "layer2.3.bn2.bias",
      "layer2.3.bn3.weight",
      "layer2.3.bn3.bias",
      "layer3.0.bn1.weight",
      "layer3.0.bn1.bias",
      "layer3.0.bn2.weight",
      "layer3.0.bn2.bias",
      "layer3.0.bn3.weight",
      "layer3.0.bn3.bias",
      "layer3.0.downsample.1.weight",
      "layer3.0.downsample.1.bias",
      "layer3.1.bn1.weight",
      "layer3.1.bn1.bias",
      "layer3.1.bn2.weight",
      "layer3.1.bn2.bias",
      "layer3.1.bn3.weight",
      "layer3.1.bn3.bias",
      "layer3.2.bn1.weight",
      "layer3.2.bn1.bias",
      "layer3.2.bn2.weight",
      "layer3.2.bn2.bias",
      "layer3.2.bn3.weight",
      "layer3.2.bn3.bias",
      "layer3.3.bn1.weight",
      "layer3.3.bn1.bias",
      "layer3.3.bn2.weight",
      "layer3.3.bn2.bias",
      "layer3.3.bn3.weight",
      "layer3.3.bn3.bias",
      "layer3.4.bn1.weight",
      "layer3.4.bn1.bias",
      "layer3.4.bn2.weight",
      "layer3.4.bn2.bias",
      "layer3.4.bn3.weight",
      "layer3.4.bn3.bias",
      "layer3.5.bn1.weight",
      "layer3.5.bn1.bias",
      "layer3.5.bn2.weight",
      "layer3.5.bn2.bias",
      "layer3.5.bn3.weight",
      "layer3.5.bn3.bias",
      "layer4.0.bn1.weight",
      "layer4.0.bn1.bias",
      "layer4.0.bn2.weight",
      "layer4.0.bn2.bias",
      "layer4.0.bn3.weight",
      "layer4.0.bn3.bias",
      "layer4.0.downsample.1.weight",
      "layer4.0.downsample.1.bias",
      "layer4.1.bn1.weight",
      "layer4.1.bn1.bias",
      "layer4.1.bn2.weight",
      "layer4.1.bn2.bias",
      "layer4.1.bn3.weight",
      "layer4.1.bn3.bias",
      "layer4.2.bn1.weight",
      "layer4.2.bn1.bias",
      "layer4.2.bn2.weight",
      "layer4.2.bn2.bias",
      "layer4.2.bn3.weight",
      "layer4.2.bn3.bias",
      "fc.bias"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
criterion = SoftTargetCrossEntropy()
dict_keys(['model', 'optimizer', 'epoch', 'scaler', 'args', 'model_ema'])
Resume checkpoint /gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_SwinTeacher_STRN50_NKD/checkpoint-best.pth
With optim & sched!
conv1.weight density is 1.0
bn1.weight density is 1.0
bn1.bias density is 1.0
layer1.0.conv1.weight density is 1.0
layer1.0.bn1.weight density is 1.0
layer1.0.bn1.bias density is 1.0
layer1.0.conv2.weight density is 1.0
layer1.0.bn2.weight density is 1.0
layer1.0.bn2.bias density is 1.0
layer1.0.conv3.weight density is 1.0
layer1.0.bn3.weight density is 1.0
layer1.0.bn3.bias density is 1.0
layer1.0.downsample.0.weight density is 1.0
layer1.0.downsample.1.weight density is 1.0
layer1.0.downsample.1.bias density is 1.0
layer1.1.conv1.weight density is 1.0
layer1.1.bn1.weight density is 1.0
layer1.1.bn1.bias density is 1.0
layer1.1.conv2.weight density is 1.0
layer1.1.bn2.weight density is 1.0
layer1.1.bn2.bias density is 1.0
layer1.1.conv3.weight density is 1.0
layer1.1.bn3.weight density is 1.0
layer1.1.bn3.bias density is 1.0
layer1.2.conv1.weight density is 1.0
layer1.2.bn1.weight density is 1.0
layer1.2.bn1.bias density is 1.0
layer1.2.conv2.weight density is 1.0
layer1.2.bn2.weight density is 1.0
layer1.2.bn2.bias density is 1.0
layer1.2.conv3.weight density is 1.0
layer1.2.bn3.weight density is 1.0
layer1.2.bn3.bias density is 1.0
layer2.0.conv1.weight density is 1.0
layer2.0.bn1.weight density is 1.0
layer2.0.bn1.bias density is 1.0
layer2.0.conv2.weight density is 1.0
layer2.0.bn2.weight density is 1.0
layer2.0.bn2.bias density is 1.0
layer2.0.conv3.weight density is 1.0
layer2.0.bn3.weight density is 1.0
layer2.0.bn3.bias density is 1.0
layer2.0.downsample.0.weight density is 1.0
layer2.0.downsample.1.weight density is 1.0
layer2.0.downsample.1.bias density is 1.0
layer2.1.conv1.weight density is 1.0
layer2.1.bn1.weight density is 1.0
layer2.1.bn1.bias density is 1.0
layer2.1.conv2.weight density is 1.0
layer2.1.bn2.weight density is 1.0
layer2.1.bn2.bias density is 1.0
layer2.1.conv3.weight density is 1.0
layer2.1.bn3.weight density is 1.0
layer2.1.bn3.bias density is 1.0
layer2.2.conv1.weight density is 1.0
layer2.2.bn1.weight density is 1.0
layer2.2.bn1.bias density is 1.0
layer2.2.conv2.weight density is 1.0
layer2.2.bn2.weight density is 1.0
layer2.2.bn2.bias density is 1.0
layer2.2.conv3.weight density is 1.0
layer2.2.bn3.weight density is 1.0
layer2.2.bn3.bias density is 1.0
layer2.3.conv1.weight density is 1.0
layer2.3.bn1.weight density is 1.0
layer2.3.bn1.bias density is 1.0
layer2.3.conv2.weight density is 1.0
layer2.3.bn2.weight density is 1.0
layer2.3.bn2.bias density is 1.0
layer2.3.conv3.weight density is 1.0
layer2.3.bn3.weight density is 1.0
layer2.3.bn3.bias density is 1.0
layer3.0.conv1.weight density is 1.0
layer3.0.bn1.weight density is 1.0
layer3.0.bn1.bias density is 1.0
layer3.0.conv2.weight density is 1.0
layer3.0.bn2.weight density is 1.0
layer3.0.bn2.bias density is 1.0
layer3.0.conv3.weight density is 1.0
layer3.0.bn3.weight density is 1.0
layer3.0.bn3.bias density is 1.0
layer3.0.downsample.0.weight density is 1.0
layer3.0.downsample.1.weight density is 1.0
layer3.0.downsample.1.bias density is 1.0
layer3.1.conv1.weight density is 1.0
layer3.1.bn1.weight density is 1.0
layer3.1.bn1.bias density is 1.0
layer3.1.conv2.weight density is 1.0
layer3.1.bn2.weight density is 1.0
layer3.1.bn2.bias density is 1.0
layer3.1.conv3.weight density is 1.0
layer3.1.bn3.weight density is 1.0
layer3.1.bn3.bias density is 1.0
layer3.2.conv1.weight density is 1.0
layer3.2.bn1.weight density is 1.0
layer3.2.bn1.bias density is 1.0
layer3.2.conv2.weight density is 1.0
layer3.2.bn2.weight density is 1.0
layer3.2.bn2.bias density is 1.0
layer3.2.conv3.weight density is 1.0
layer3.2.bn3.weight density is 1.0
layer3.2.bn3.bias density is 1.0
layer3.3.conv1.weight density is 1.0
layer3.3.bn1.weight density is 1.0
layer3.3.bn1.bias density is 1.0
layer3.3.conv2.weight density is 1.0
layer3.3.bn2.weight density is 1.0
layer3.3.bn2.bias density is 1.0
layer3.3.conv3.weight density is 1.0
layer3.3.bn3.weight density is 1.0
layer3.3.bn3.bias density is 1.0
layer3.4.conv1.weight density is 1.0
layer3.4.bn1.weight density is 1.0
layer3.4.bn1.bias density is 1.0
layer3.4.conv2.weight density is 1.0
layer3.4.bn2.weight density is 1.0
layer3.4.bn2.bias density is 1.0
layer3.4.conv3.weight density is 1.0
layer3.4.bn3.weight density is 1.0
layer3.4.bn3.bias density is 1.0
layer3.5.conv1.weight density is 1.0
layer3.5.bn1.weight density is 1.0
layer3.5.bn1.bias density is 1.0
layer3.5.conv2.weight density is 1.0
layer3.5.bn2.weight density is 1.0
layer3.5.bn2.bias density is 1.0
layer3.5.conv3.weight density is 1.0
layer3.5.bn3.weight density is 1.0
layer3.5.bn3.bias density is 1.0
layer4.0.conv1.weight density is 1.0
layer4.0.bn1.weight density is 1.0
layer4.0.bn1.bias density is 1.0
layer4.0.conv2.weight density is 1.0
layer4.0.bn2.weight density is 1.0
layer4.0.bn2.bias density is 1.0
layer4.0.conv3.weight density is 1.0
layer4.0.bn3.weight density is 1.0
layer4.0.bn3.bias density is 1.0
layer4.0.downsample.0.weight density is 1.0
layer4.0.downsample.1.weight density is 1.0
layer4.0.downsample.1.bias density is 1.0
layer4.1.conv1.weight density is 1.0
layer4.1.bn1.weight density is 1.0
layer4.1.bn1.bias density is 1.0
layer4.1.conv2.weight density is 1.0
layer4.1.bn2.weight density is 1.0
layer4.1.bn2.bias density is 1.0
layer4.1.conv3.weight density is 1.0
layer4.1.bn3.weight density is 1.0
layer4.1.bn3.bias density is 1.0
layer4.2.conv1.weight density is 1.0
layer4.2.bn1.weight density is 1.0
layer4.2.bn1.bias density is 1.0
layer4.2.conv2.weight density is 1.0
layer4.2.bn2.weight density is 1.0
layer4.2.bn2.bias density is 1.0
layer4.2.conv3.weight density is 1.0
layer4.2.bn3.weight density is 1.0
layer4.2.bn3.bias density is 1.0
fc.weight density is 1.0
fc.bias density is 1.0
Test:  [ 0/79]  eta: 0:06:02  loss: 4.3845 (4.3845)  acc1: 8.3333 (8.3333)  acc5: 41.6667 (41.6667)  time: 4.5901  data: 1.7227  max mem: 3726
Test:  [10/79]  eta: 0:00:36  loss: 4.4061 (4.5460)  acc1: 8.3333 (7.6705)  acc5: 32.2917 (32.4811)  time: 0.5249  data: 0.1567  max mem: 3726
Test:  [20/79]  eta: 0:00:19  loss: 4.6112 (4.5677)  acc1: 6.2500 (7.3909)  acc5: 28.1250 (32.2917)  time: 0.1120  data: 0.0006  max mem: 3726
Test:  [30/79]  eta: 0:00:12  loss: 4.5837 (4.5339)  acc1: 6.2500 (7.0228)  acc5: 29.1667 (34.5094)  time: 0.0961  data: 0.0006  max mem: 3726
Test:  [40/79]  eta: 0:00:07  loss: 4.6586 (4.6476)  acc1: 4.1667 (6.4024)  acc5: 27.0833 (32.4695)  time: 0.0656  data: 0.0001  max mem: 3726
Test:  [50/79]  eta: 0:00:04  loss: 4.7406 (4.6780)  acc1: 5.2083 (6.3930)  acc5: 27.0833 (31.6789)  time: 0.0446  data: 0.0001  max mem: 3726
Test:  [60/79]  eta: 0:00:02  loss: 4.8331 (4.7162)  acc1: 5.2083 (6.0451)  acc5: 28.1250 (31.6598)  time: 0.0534  data: 0.0095  max mem: 3726
Test:  [70/79]  eta: 0:00:01  loss: 4.9452 (4.7563)  acc1: 4.1667 (5.7218)  acc5: 28.1250 (31.1033)  time: 0.0682  data: 0.0101  max mem: 3726
Test:  [78/79]  eta: 0:00:00  loss: 4.9439 (4.7778)  acc1: 3.1250 (5.4533)  acc5: 28.1250 (31.0133)  time: 0.0702  data: 0.0006  max mem: 3726
Test: Total time: 0:00:10 (0.1342 s / it)
* Acc@1 5.453 Acc@5 31.013 loss 4.778
Accuracy of the network on 7500 test images: 5.45333%
Not using distributed mode
Namespace(Decom=False, aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=64, bn=False, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/sliu/project_space/datasets/imagenet-a', data_set='IMNET', data_type='imagenet-a', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.1, enable_wandb=False, epochs=300, eval=True, eval_data_path=None, finetune='', fix=False, growth='random', head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, kernel_size=[7, 7, 7, 7, 100], layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=-1, log_dir=None, lr=0.004, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='resnet50', model_ema=False, model_ema_decay=0.9999, model_ema_eval=False, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_workers=10, only_L=False, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', parallel=True, pin_mem=True, project='SLaK', prune='magnitude', prune_rate=0.3, recount=1, redistribution='none', remode='pixel', reprob=0.25, resplit=False, resume='/gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_Teacherscswin_STRN50_NKD/checkpoint-best.pth', save_ckpt=True, save_ckpt_freq=10, save_ckpt_num=3, seed=0, smoothing=0.1, sparse=False, sparse_init='snip', sparsity=0.4, start_epoch=0, train_interpolation='bicubic', update_freq=1, update_frequency=100, use_amp=False, verbose=False, wandb_ckpt=False, warmup_epochs=20, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, width_factor=1.0, world_size=1)
Transform = 
Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /home/sliu/project_space/datasets/imagenet-a
Number of the class = 1000
Mixup is activated!
Model = ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=True)
  (fc): Linear(in_features=2048, out_features=1000, bias=True)
)
number of params: 25557032
LR = 0.00400000
Batch size = 64
Update frequent = 1
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "conv1.weight",
      "layer1.0.conv1.weight",
      "layer1.0.conv2.weight",
      "layer1.0.conv3.weight",
      "layer1.0.downsample.0.weight",
      "layer1.1.conv1.weight",
      "layer1.1.conv2.weight",
      "layer1.1.conv3.weight",
      "layer1.2.conv1.weight",
      "layer1.2.conv2.weight",
      "layer1.2.conv3.weight",
      "layer2.0.conv1.weight",
      "layer2.0.conv2.weight",
      "layer2.0.conv3.weight",
      "layer2.0.downsample.0.weight",
      "layer2.1.conv1.weight",
      "layer2.1.conv2.weight",
      "layer2.1.conv3.weight",
      "layer2.2.conv1.weight",
      "layer2.2.conv2.weight",
      "layer2.2.conv3.weight",
      "layer2.3.conv1.weight",
      "layer2.3.conv2.weight",
      "layer2.3.conv3.weight",
      "layer3.0.conv1.weight",
      "layer3.0.conv2.weight",
      "layer3.0.conv3.weight",
      "layer3.0.downsample.0.weight",
      "layer3.1.conv1.weight",
      "layer3.1.conv2.weight",
      "layer3.1.conv3.weight",
      "layer3.2.conv1.weight",
      "layer3.2.conv2.weight",
      "layer3.2.conv3.weight",
      "layer3.3.conv1.weight",
      "layer3.3.conv2.weight",
      "layer3.3.conv3.weight",
      "layer3.4.conv1.weight",
      "layer3.4.conv2.weight",
      "layer3.4.conv3.weight",
      "layer3.5.conv1.weight",
      "layer3.5.conv2.weight",
      "layer3.5.conv3.weight",
      "layer4.0.conv1.weight",
      "layer4.0.conv2.weight",
      "layer4.0.conv3.weight",
      "layer4.0.downsample.0.weight",
      "layer4.1.conv1.weight",
      "layer4.1.conv2.weight",
      "layer4.1.conv3.weight",
      "layer4.2.conv1.weight",
      "layer4.2.conv2.weight",
      "layer4.2.conv3.weight",
      "fc.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "bn1.weight",
      "bn1.bias",
      "layer1.0.bn1.weight",
      "layer1.0.bn1.bias",
      "layer1.0.bn2.weight",
      "layer1.0.bn2.bias",
      "layer1.0.bn3.weight",
      "layer1.0.bn3.bias",
      "layer1.0.downsample.1.weight",
      "layer1.0.downsample.1.bias",
      "layer1.1.bn1.weight",
      "layer1.1.bn1.bias",
      "layer1.1.bn2.weight",
      "layer1.1.bn2.bias",
      "layer1.1.bn3.weight",
      "layer1.1.bn3.bias",
      "layer1.2.bn1.weight",
      "layer1.2.bn1.bias",
      "layer1.2.bn2.weight",
      "layer1.2.bn2.bias",
      "layer1.2.bn3.weight",
      "layer1.2.bn3.bias",
      "layer2.0.bn1.weight",
      "layer2.0.bn1.bias",
      "layer2.0.bn2.weight",
      "layer2.0.bn2.bias",
      "layer2.0.bn3.weight",
      "layer2.0.bn3.bias",
      "layer2.0.downsample.1.weight",
      "layer2.0.downsample.1.bias",
      "layer2.1.bn1.weight",
      "layer2.1.bn1.bias",
      "layer2.1.bn2.weight",
      "layer2.1.bn2.bias",
      "layer2.1.bn3.weight",
      "layer2.1.bn3.bias",
      "layer2.2.bn1.weight",
      "layer2.2.bn1.bias",
      "layer2.2.bn2.weight",
      "layer2.2.bn2.bias",
      "layer2.2.bn3.weight",
      "layer2.2.bn3.bias",
      "layer2.3.bn1.weight",
      "layer2.3.bn1.bias",
      "layer2.3.bn2.weight",
      "layer2.3.bn2.bias",
      "layer2.3.bn3.weight",
      "layer2.3.bn3.bias",
      "layer3.0.bn1.weight",
      "layer3.0.bn1.bias",
      "layer3.0.bn2.weight",
      "layer3.0.bn2.bias",
      "layer3.0.bn3.weight",
      "layer3.0.bn3.bias",
      "layer3.0.downsample.1.weight",
      "layer3.0.downsample.1.bias",
      "layer3.1.bn1.weight",
      "layer3.1.bn1.bias",
      "layer3.1.bn2.weight",
      "layer3.1.bn2.bias",
      "layer3.1.bn3.weight",
      "layer3.1.bn3.bias",
      "layer3.2.bn1.weight",
      "layer3.2.bn1.bias",
      "layer3.2.bn2.weight",
      "layer3.2.bn2.bias",
      "layer3.2.bn3.weight",
      "layer3.2.bn3.bias",
      "layer3.3.bn1.weight",
      "layer3.3.bn1.bias",
      "layer3.3.bn2.weight",
      "layer3.3.bn2.bias",
      "layer3.3.bn3.weight",
      "layer3.3.bn3.bias",
      "layer3.4.bn1.weight",
      "layer3.4.bn1.bias",
      "layer3.4.bn2.weight",
      "layer3.4.bn2.bias",
      "layer3.4.bn3.weight",
      "layer3.4.bn3.bias",
      "layer3.5.bn1.weight",
      "layer3.5.bn1.bias",
      "layer3.5.bn2.weight",
      "layer3.5.bn2.bias",
      "layer3.5.bn3.weight",
      "layer3.5.bn3.bias",
      "layer4.0.bn1.weight",
      "layer4.0.bn1.bias",
      "layer4.0.bn2.weight",
      "layer4.0.bn2.bias",
      "layer4.0.bn3.weight",
      "layer4.0.bn3.bias",
      "layer4.0.downsample.1.weight",
      "layer4.0.downsample.1.bias",
      "layer4.1.bn1.weight",
      "layer4.1.bn1.bias",
      "layer4.1.bn2.weight",
      "layer4.1.bn2.bias",
      "layer4.1.bn3.weight",
      "layer4.1.bn3.bias",
      "layer4.2.bn1.weight",
      "layer4.2.bn1.bias",
      "layer4.2.bn2.weight",
      "layer4.2.bn2.bias",
      "layer4.2.bn3.weight",
      "layer4.2.bn3.bias",
      "fc.bias"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
criterion = SoftTargetCrossEntropy()
dict_keys(['model', 'optimizer', 'epoch', 'scaler', 'args', 'model_ema'])
Resume checkpoint /gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_Teacherscswin_STRN50_NKD/checkpoint-best.pth
With optim & sched!
conv1.weight density is 1.0
bn1.weight density is 1.0
bn1.bias density is 1.0
layer1.0.conv1.weight density is 1.0
layer1.0.bn1.weight density is 1.0
layer1.0.bn1.bias density is 1.0
layer1.0.conv2.weight density is 1.0
layer1.0.bn2.weight density is 1.0
layer1.0.bn2.bias density is 1.0
layer1.0.conv3.weight density is 1.0
layer1.0.bn3.weight density is 1.0
layer1.0.bn3.bias density is 1.0
layer1.0.downsample.0.weight density is 1.0
layer1.0.downsample.1.weight density is 1.0
layer1.0.downsample.1.bias density is 1.0
layer1.1.conv1.weight density is 1.0
layer1.1.bn1.weight density is 1.0
layer1.1.bn1.bias density is 1.0
layer1.1.conv2.weight density is 1.0
layer1.1.bn2.weight density is 1.0
layer1.1.bn2.bias density is 1.0
layer1.1.conv3.weight density is 1.0
layer1.1.bn3.weight density is 1.0
layer1.1.bn3.bias density is 1.0
layer1.2.conv1.weight density is 1.0
layer1.2.bn1.weight density is 1.0
layer1.2.bn1.bias density is 1.0
layer1.2.conv2.weight density is 1.0
layer1.2.bn2.weight density is 1.0
layer1.2.bn2.bias density is 1.0
layer1.2.conv3.weight density is 1.0
layer1.2.bn3.weight density is 1.0
layer1.2.bn3.bias density is 1.0
layer2.0.conv1.weight density is 1.0
layer2.0.bn1.weight density is 1.0
layer2.0.bn1.bias density is 1.0
layer2.0.conv2.weight density is 1.0
layer2.0.bn2.weight density is 1.0
layer2.0.bn2.bias density is 1.0
layer2.0.conv3.weight density is 1.0
layer2.0.bn3.weight density is 1.0
layer2.0.bn3.bias density is 1.0
layer2.0.downsample.0.weight density is 1.0
layer2.0.downsample.1.weight density is 1.0
layer2.0.downsample.1.bias density is 1.0
layer2.1.conv1.weight density is 1.0
layer2.1.bn1.weight density is 1.0
layer2.1.bn1.bias density is 1.0
layer2.1.conv2.weight density is 1.0
layer2.1.bn2.weight density is 1.0
layer2.1.bn2.bias density is 1.0
layer2.1.conv3.weight density is 1.0
layer2.1.bn3.weight density is 1.0
layer2.1.bn3.bias density is 1.0
layer2.2.conv1.weight density is 1.0
layer2.2.bn1.weight density is 1.0
layer2.2.bn1.bias density is 1.0
layer2.2.conv2.weight density is 1.0
layer2.2.bn2.weight density is 1.0
layer2.2.bn2.bias density is 1.0
layer2.2.conv3.weight density is 1.0
layer2.2.bn3.weight density is 1.0
layer2.2.bn3.bias density is 1.0
layer2.3.conv1.weight density is 1.0
layer2.3.bn1.weight density is 1.0
layer2.3.bn1.bias density is 1.0
layer2.3.conv2.weight density is 1.0
layer2.3.bn2.weight density is 1.0
layer2.3.bn2.bias density is 1.0
layer2.3.conv3.weight density is 1.0
layer2.3.bn3.weight density is 1.0
layer2.3.bn3.bias density is 1.0
layer3.0.conv1.weight density is 1.0
layer3.0.bn1.weight density is 1.0
layer3.0.bn1.bias density is 1.0
layer3.0.conv2.weight density is 1.0
layer3.0.bn2.weight density is 1.0
layer3.0.bn2.bias density is 1.0
layer3.0.conv3.weight density is 1.0
layer3.0.bn3.weight density is 1.0
layer3.0.bn3.bias density is 1.0
layer3.0.downsample.0.weight density is 1.0
layer3.0.downsample.1.weight density is 1.0
layer3.0.downsample.1.bias density is 1.0
layer3.1.conv1.weight density is 1.0
layer3.1.bn1.weight density is 1.0
layer3.1.bn1.bias density is 1.0
layer3.1.conv2.weight density is 1.0
layer3.1.bn2.weight density is 1.0
layer3.1.bn2.bias density is 1.0
layer3.1.conv3.weight density is 1.0
layer3.1.bn3.weight density is 1.0
layer3.1.bn3.bias density is 1.0
layer3.2.conv1.weight density is 1.0
layer3.2.bn1.weight density is 1.0
layer3.2.bn1.bias density is 1.0
layer3.2.conv2.weight density is 1.0
layer3.2.bn2.weight density is 1.0
layer3.2.bn2.bias density is 1.0
layer3.2.conv3.weight density is 1.0
layer3.2.bn3.weight density is 1.0
layer3.2.bn3.bias density is 1.0
layer3.3.conv1.weight density is 1.0
layer3.3.bn1.weight density is 1.0
layer3.3.bn1.bias density is 1.0
layer3.3.conv2.weight density is 1.0
layer3.3.bn2.weight density is 1.0
layer3.3.bn2.bias density is 1.0
layer3.3.conv3.weight density is 1.0
layer3.3.bn3.weight density is 1.0
layer3.3.bn3.bias density is 1.0
layer3.4.conv1.weight density is 1.0
layer3.4.bn1.weight density is 1.0
layer3.4.bn1.bias density is 1.0
layer3.4.conv2.weight density is 1.0
layer3.4.bn2.weight density is 1.0
layer3.4.bn2.bias density is 1.0
layer3.4.conv3.weight density is 1.0
layer3.4.bn3.weight density is 1.0
layer3.4.bn3.bias density is 1.0
layer3.5.conv1.weight density is 1.0
layer3.5.bn1.weight density is 1.0
layer3.5.bn1.bias density is 1.0
layer3.5.conv2.weight density is 1.0
layer3.5.bn2.weight density is 1.0
layer3.5.bn2.bias density is 1.0
layer3.5.conv3.weight density is 1.0
layer3.5.bn3.weight density is 1.0
layer3.5.bn3.bias density is 1.0
layer4.0.conv1.weight density is 1.0
layer4.0.bn1.weight density is 1.0
layer4.0.bn1.bias density is 1.0
layer4.0.conv2.weight density is 1.0
layer4.0.bn2.weight density is 1.0
layer4.0.bn2.bias density is 1.0
layer4.0.conv3.weight density is 1.0
layer4.0.bn3.weight density is 1.0
layer4.0.bn3.bias density is 1.0
layer4.0.downsample.0.weight density is 1.0
layer4.0.downsample.1.weight density is 1.0
layer4.0.downsample.1.bias density is 1.0
layer4.1.conv1.weight density is 1.0
layer4.1.bn1.weight density is 1.0
layer4.1.bn1.bias density is 1.0
layer4.1.conv2.weight density is 1.0
layer4.1.bn2.weight density is 1.0
layer4.1.bn2.bias density is 1.0
layer4.1.conv3.weight density is 1.0
layer4.1.bn3.weight density is 1.0
layer4.1.bn3.bias density is 1.0
layer4.2.conv1.weight density is 1.0
layer4.2.bn1.weight density is 1.0
layer4.2.bn1.bias density is 1.0
layer4.2.conv2.weight density is 1.0
layer4.2.bn2.weight density is 1.0
layer4.2.bn2.bias density is 1.0
layer4.2.conv3.weight density is 1.0
layer4.2.bn3.weight density is 1.0
layer4.2.bn3.bias density is 1.0
fc.weight density is 1.0
fc.bias density is 1.0
Test:  [ 0/79]  eta: 0:06:36  loss: 4.4557 (4.4557)  acc1: 8.3333 (8.3333)  acc5: 45.8333 (45.8333)  time: 5.0134  data: 2.1940  max mem: 3726
Test:  [10/79]  eta: 0:00:34  loss: 4.4870 (4.5090)  acc1: 10.4167 (8.9015)  acc5: 36.4583 (34.3750)  time: 0.5030  data: 0.2000  max mem: 3726
Test:  [20/79]  eta: 0:00:17  loss: 4.5512 (4.5351)  acc1: 9.3750 (8.7798)  acc5: 30.2083 (33.7302)  time: 0.0520  data: 0.0003  max mem: 3726
Test:  [30/79]  eta: 0:00:11  loss: 4.5512 (4.5592)  acc1: 8.3333 (8.1653)  acc5: 30.2083 (34.8454)  time: 0.0760  data: 0.0048  max mem: 3726
Test:  [40/79]  eta: 0:00:07  loss: 4.7741 (4.6856)  acc1: 6.2500 (7.3171)  acc5: 31.2500 (32.9268)  time: 0.0979  data: 0.0193  max mem: 3726
Test:  [50/79]  eta: 0:00:05  loss: 4.8223 (4.7161)  acc1: 4.1667 (7.2712)  acc5: 27.0833 (32.0670)  time: 0.0984  data: 0.0326  max mem: 3726
Test:  [60/79]  eta: 0:00:03  loss: 4.8668 (4.7491)  acc1: 4.1667 (6.9501)  acc5: 29.1667 (32.1209)  time: 0.0942  data: 0.0181  max mem: 3726
Test:  [70/79]  eta: 0:00:01  loss: 4.8923 (4.7791)  acc1: 4.1667 (6.5141)  acc5: 32.2917 (31.8955)  time: 0.0745  data: 0.0001  max mem: 3726
Test:  [78/79]  eta: 0:00:00  loss: 4.9164 (4.8061)  acc1: 3.1250 (6.2667)  acc5: 29.1667 (31.4933)  time: 0.0898  data: 0.0001  max mem: 3726
Test: Total time: 0:00:11 (0.1473 s / it)
* Acc@1 6.267 Acc@5 31.493 loss 4.806
Accuracy of the network on 7500 test images: 6.26667%
Not using distributed mode
Namespace(Decom=False, aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=64, bn=False, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/sliu/project_space/datasets/imagenet-a', data_set='IMNET', data_type='imagenet-a', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.1, enable_wandb=False, epochs=300, eval=True, eval_data_path=None, finetune='', fix=False, growth='random', head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, kernel_size=[7, 7, 7, 7, 100], layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=-1, log_dir=None, lr=0.004, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='resnet50', model_ema=False, model_ema_decay=0.9999, model_ema_eval=False, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_workers=10, only_L=False, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', parallel=True, pin_mem=True, project='SLaK', prune='magnitude', prune_rate=0.3, recount=1, redistribution='none', remode='pixel', reprob=0.25, resplit=False, resume='/gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_ConvNeXtTeacher_STRN50_NKD/checkpoint-best.pth', save_ckpt=True, save_ckpt_freq=10, save_ckpt_num=3, seed=0, smoothing=0.1, sparse=False, sparse_init='snip', sparsity=0.4, start_epoch=0, train_interpolation='bicubic', update_freq=1, update_frequency=100, use_amp=False, verbose=False, wandb_ckpt=False, warmup_epochs=20, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, width_factor=1.0, world_size=1)
Transform = 
Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /home/sliu/project_space/datasets/imagenet-a
Number of the class = 1000
Mixup is activated!
Model = ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=True)
  (fc): Linear(in_features=2048, out_features=1000, bias=True)
)
number of params: 25557032
LR = 0.00400000
Batch size = 64
Update frequent = 1
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "conv1.weight",
      "layer1.0.conv1.weight",
      "layer1.0.conv2.weight",
      "layer1.0.conv3.weight",
      "layer1.0.downsample.0.weight",
      "layer1.1.conv1.weight",
      "layer1.1.conv2.weight",
      "layer1.1.conv3.weight",
      "layer1.2.conv1.weight",
      "layer1.2.conv2.weight",
      "layer1.2.conv3.weight",
      "layer2.0.conv1.weight",
      "layer2.0.conv2.weight",
      "layer2.0.conv3.weight",
      "layer2.0.downsample.0.weight",
      "layer2.1.conv1.weight",
      "layer2.1.conv2.weight",
      "layer2.1.conv3.weight",
      "layer2.2.conv1.weight",
      "layer2.2.conv2.weight",
      "layer2.2.conv3.weight",
      "layer2.3.conv1.weight",
      "layer2.3.conv2.weight",
      "layer2.3.conv3.weight",
      "layer3.0.conv1.weight",
      "layer3.0.conv2.weight",
      "layer3.0.conv3.weight",
      "layer3.0.downsample.0.weight",
      "layer3.1.conv1.weight",
      "layer3.1.conv2.weight",
      "layer3.1.conv3.weight",
      "layer3.2.conv1.weight",
      "layer3.2.conv2.weight",
      "layer3.2.conv3.weight",
      "layer3.3.conv1.weight",
      "layer3.3.conv2.weight",
      "layer3.3.conv3.weight",
      "layer3.4.conv1.weight",
      "layer3.4.conv2.weight",
      "layer3.4.conv3.weight",
      "layer3.5.conv1.weight",
      "layer3.5.conv2.weight",
      "layer3.5.conv3.weight",
      "layer4.0.conv1.weight",
      "layer4.0.conv2.weight",
      "layer4.0.conv3.weight",
      "layer4.0.downsample.0.weight",
      "layer4.1.conv1.weight",
      "layer4.1.conv2.weight",
      "layer4.1.conv3.weight",
      "layer4.2.conv1.weight",
      "layer4.2.conv2.weight",
      "layer4.2.conv3.weight",
      "fc.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "bn1.weight",
      "bn1.bias",
      "layer1.0.bn1.weight",
      "layer1.0.bn1.bias",
      "layer1.0.bn2.weight",
      "layer1.0.bn2.bias",
      "layer1.0.bn3.weight",
      "layer1.0.bn3.bias",
      "layer1.0.downsample.1.weight",
      "layer1.0.downsample.1.bias",
      "layer1.1.bn1.weight",
      "layer1.1.bn1.bias",
      "layer1.1.bn2.weight",
      "layer1.1.bn2.bias",
      "layer1.1.bn3.weight",
      "layer1.1.bn3.bias",
      "layer1.2.bn1.weight",
      "layer1.2.bn1.bias",
      "layer1.2.bn2.weight",
      "layer1.2.bn2.bias",
      "layer1.2.bn3.weight",
      "layer1.2.bn3.bias",
      "layer2.0.bn1.weight",
      "layer2.0.bn1.bias",
      "layer2.0.bn2.weight",
      "layer2.0.bn2.bias",
      "layer2.0.bn3.weight",
      "layer2.0.bn3.bias",
      "layer2.0.downsample.1.weight",
      "layer2.0.downsample.1.bias",
      "layer2.1.bn1.weight",
      "layer2.1.bn1.bias",
      "layer2.1.bn2.weight",
      "layer2.1.bn2.bias",
      "layer2.1.bn3.weight",
      "layer2.1.bn3.bias",
      "layer2.2.bn1.weight",
      "layer2.2.bn1.bias",
      "layer2.2.bn2.weight",
      "layer2.2.bn2.bias",
      "layer2.2.bn3.weight",
      "layer2.2.bn3.bias",
      "layer2.3.bn1.weight",
      "layer2.3.bn1.bias",
      "layer2.3.bn2.weight",
      "layer2.3.bn2.bias",
      "layer2.3.bn3.weight",
      "layer2.3.bn3.bias",
      "layer3.0.bn1.weight",
      "layer3.0.bn1.bias",
      "layer3.0.bn2.weight",
      "layer3.0.bn2.bias",
      "layer3.0.bn3.weight",
      "layer3.0.bn3.bias",
      "layer3.0.downsample.1.weight",
      "layer3.0.downsample.1.bias",
      "layer3.1.bn1.weight",
      "layer3.1.bn1.bias",
      "layer3.1.bn2.weight",
      "layer3.1.bn2.bias",
      "layer3.1.bn3.weight",
      "layer3.1.bn3.bias",
      "layer3.2.bn1.weight",
      "layer3.2.bn1.bias",
      "layer3.2.bn2.weight",
      "layer3.2.bn2.bias",
      "layer3.2.bn3.weight",
      "layer3.2.bn3.bias",
      "layer3.3.bn1.weight",
      "layer3.3.bn1.bias",
      "layer3.3.bn2.weight",
      "layer3.3.bn2.bias",
      "layer3.3.bn3.weight",
      "layer3.3.bn3.bias",
      "layer3.4.bn1.weight",
      "layer3.4.bn1.bias",
      "layer3.4.bn2.weight",
      "layer3.4.bn2.bias",
      "layer3.4.bn3.weight",
      "layer3.4.bn3.bias",
      "layer3.5.bn1.weight",
      "layer3.5.bn1.bias",
      "layer3.5.bn2.weight",
      "layer3.5.bn2.bias",
      "layer3.5.bn3.weight",
      "layer3.5.bn3.bias",
      "layer4.0.bn1.weight",
      "layer4.0.bn1.bias",
      "layer4.0.bn2.weight",
      "layer4.0.bn2.bias",
      "layer4.0.bn3.weight",
      "layer4.0.bn3.bias",
      "layer4.0.downsample.1.weight",
      "layer4.0.downsample.1.bias",
      "layer4.1.bn1.weight",
      "layer4.1.bn1.bias",
      "layer4.1.bn2.weight",
      "layer4.1.bn2.bias",
      "layer4.1.bn3.weight",
      "layer4.1.bn3.bias",
      "layer4.2.bn1.weight",
      "layer4.2.bn1.bias",
      "layer4.2.bn2.weight",
      "layer4.2.bn2.bias",
      "layer4.2.bn3.weight",
      "layer4.2.bn3.bias",
      "fc.bias"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
criterion = SoftTargetCrossEntropy()
dict_keys(['model', 'optimizer', 'epoch', 'scaler', 'args', 'model_ema'])
Resume checkpoint /gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_ConvNeXtTeacher_STRN50_NKD/checkpoint-best.pth
With optim & sched!
conv1.weight density is 1.0
bn1.weight density is 1.0
bn1.bias density is 1.0
layer1.0.conv1.weight density is 1.0
layer1.0.bn1.weight density is 1.0
layer1.0.bn1.bias density is 1.0
layer1.0.conv2.weight density is 1.0
layer1.0.bn2.weight density is 1.0
layer1.0.bn2.bias density is 1.0
layer1.0.conv3.weight density is 1.0
layer1.0.bn3.weight density is 1.0
layer1.0.bn3.bias density is 1.0
layer1.0.downsample.0.weight density is 1.0
layer1.0.downsample.1.weight density is 1.0
layer1.0.downsample.1.bias density is 1.0
layer1.1.conv1.weight density is 1.0
layer1.1.bn1.weight density is 1.0
layer1.1.bn1.bias density is 1.0
layer1.1.conv2.weight density is 1.0
layer1.1.bn2.weight density is 1.0
layer1.1.bn2.bias density is 1.0
layer1.1.conv3.weight density is 1.0
layer1.1.bn3.weight density is 1.0
layer1.1.bn3.bias density is 1.0
layer1.2.conv1.weight density is 1.0
layer1.2.bn1.weight density is 1.0
layer1.2.bn1.bias density is 1.0
layer1.2.conv2.weight density is 1.0
layer1.2.bn2.weight density is 1.0
layer1.2.bn2.bias density is 1.0
layer1.2.conv3.weight density is 1.0
layer1.2.bn3.weight density is 1.0
layer1.2.bn3.bias density is 1.0
layer2.0.conv1.weight density is 1.0
layer2.0.bn1.weight density is 1.0
layer2.0.bn1.bias density is 1.0
layer2.0.conv2.weight density is 1.0
layer2.0.bn2.weight density is 1.0
layer2.0.bn2.bias density is 1.0
layer2.0.conv3.weight density is 1.0
layer2.0.bn3.weight density is 1.0
layer2.0.bn3.bias density is 1.0
layer2.0.downsample.0.weight density is 1.0
layer2.0.downsample.1.weight density is 1.0
layer2.0.downsample.1.bias density is 1.0
layer2.1.conv1.weight density is 1.0
layer2.1.bn1.weight density is 1.0
layer2.1.bn1.bias density is 1.0
layer2.1.conv2.weight density is 1.0
layer2.1.bn2.weight density is 1.0
layer2.1.bn2.bias density is 1.0
layer2.1.conv3.weight density is 1.0
layer2.1.bn3.weight density is 1.0
layer2.1.bn3.bias density is 1.0
layer2.2.conv1.weight density is 1.0
layer2.2.bn1.weight density is 1.0
layer2.2.bn1.bias density is 1.0
layer2.2.conv2.weight density is 1.0
layer2.2.bn2.weight density is 1.0
layer2.2.bn2.bias density is 1.0
layer2.2.conv3.weight density is 1.0
layer2.2.bn3.weight density is 1.0
layer2.2.bn3.bias density is 1.0
layer2.3.conv1.weight density is 1.0
layer2.3.bn1.weight density is 1.0
layer2.3.bn1.bias density is 1.0
layer2.3.conv2.weight density is 1.0
layer2.3.bn2.weight density is 1.0
layer2.3.bn2.bias density is 1.0
layer2.3.conv3.weight density is 1.0
layer2.3.bn3.weight density is 1.0
layer2.3.bn3.bias density is 1.0
layer3.0.conv1.weight density is 1.0
layer3.0.bn1.weight density is 1.0
layer3.0.bn1.bias density is 1.0
layer3.0.conv2.weight density is 1.0
layer3.0.bn2.weight density is 1.0
layer3.0.bn2.bias density is 1.0
layer3.0.conv3.weight density is 1.0
layer3.0.bn3.weight density is 1.0
layer3.0.bn3.bias density is 1.0
layer3.0.downsample.0.weight density is 1.0
layer3.0.downsample.1.weight density is 1.0
layer3.0.downsample.1.bias density is 1.0
layer3.1.conv1.weight density is 1.0
layer3.1.bn1.weight density is 1.0
layer3.1.bn1.bias density is 1.0
layer3.1.conv2.weight density is 1.0
layer3.1.bn2.weight density is 1.0
layer3.1.bn2.bias density is 1.0
layer3.1.conv3.weight density is 1.0
layer3.1.bn3.weight density is 1.0
layer3.1.bn3.bias density is 1.0
layer3.2.conv1.weight density is 1.0
layer3.2.bn1.weight density is 1.0
layer3.2.bn1.bias density is 1.0
layer3.2.conv2.weight density is 1.0
layer3.2.bn2.weight density is 1.0
layer3.2.bn2.bias density is 1.0
layer3.2.conv3.weight density is 1.0
layer3.2.bn3.weight density is 1.0
layer3.2.bn3.bias density is 1.0
layer3.3.conv1.weight density is 1.0
layer3.3.bn1.weight density is 1.0
layer3.3.bn1.bias density is 1.0
layer3.3.conv2.weight density is 1.0
layer3.3.bn2.weight density is 1.0
layer3.3.bn2.bias density is 1.0
layer3.3.conv3.weight density is 1.0
layer3.3.bn3.weight density is 1.0
layer3.3.bn3.bias density is 1.0
layer3.4.conv1.weight density is 1.0
layer3.4.bn1.weight density is 1.0
layer3.4.bn1.bias density is 1.0
layer3.4.conv2.weight density is 1.0
layer3.4.bn2.weight density is 1.0
layer3.4.bn2.bias density is 1.0
layer3.4.conv3.weight density is 1.0
layer3.4.bn3.weight density is 1.0
layer3.4.bn3.bias density is 1.0
layer3.5.conv1.weight density is 1.0
layer3.5.bn1.weight density is 1.0
layer3.5.bn1.bias density is 1.0
layer3.5.conv2.weight density is 1.0
layer3.5.bn2.weight density is 1.0
layer3.5.bn2.bias density is 1.0
layer3.5.conv3.weight density is 1.0
layer3.5.bn3.weight density is 1.0
layer3.5.bn3.bias density is 1.0
layer4.0.conv1.weight density is 1.0
layer4.0.bn1.weight density is 1.0
layer4.0.bn1.bias density is 1.0
layer4.0.conv2.weight density is 1.0
layer4.0.bn2.weight density is 1.0
layer4.0.bn2.bias density is 1.0
layer4.0.conv3.weight density is 1.0
layer4.0.bn3.weight density is 1.0
layer4.0.bn3.bias density is 1.0
layer4.0.downsample.0.weight density is 1.0
layer4.0.downsample.1.weight density is 1.0
layer4.0.downsample.1.bias density is 1.0
layer4.1.conv1.weight density is 1.0
layer4.1.bn1.weight density is 1.0
layer4.1.bn1.bias density is 1.0
layer4.1.conv2.weight density is 1.0
layer4.1.bn2.weight density is 1.0
layer4.1.bn2.bias density is 1.0
layer4.1.conv3.weight density is 1.0
layer4.1.bn3.weight density is 1.0
layer4.1.bn3.bias density is 1.0
layer4.2.conv1.weight density is 1.0
layer4.2.bn1.weight density is 1.0
layer4.2.bn1.bias density is 1.0
layer4.2.conv2.weight density is 1.0
layer4.2.bn2.weight density is 1.0
layer4.2.bn2.bias density is 1.0
layer4.2.conv3.weight density is 1.0
layer4.2.bn3.weight density is 1.0
layer4.2.bn3.bias density is 1.0
fc.weight density is 1.0
fc.bias density is 1.0
Test:  [ 0/79]  eta: 0:05:10  loss: 4.2485 (4.2485)  acc1: 10.4167 (10.4167)  acc5: 48.9583 (48.9583)  time: 3.9315  data: 2.0596  max mem: 3726
Test:  [10/79]  eta: 0:00:29  loss: 4.3522 (4.4675)  acc1: 7.2917 (7.8598)  acc5: 33.3333 (33.1439)  time: 0.4301  data: 0.1874  max mem: 3726
Test:  [20/79]  eta: 0:00:14  loss: 4.5181 (4.4819)  acc1: 7.2917 (8.0357)  acc5: 33.3333 (33.6806)  time: 0.0615  data: 0.0001  max mem: 3726
Test:  [30/79]  eta: 0:00:08  loss: 4.5008 (4.4763)  acc1: 7.2917 (7.9637)  acc5: 36.4583 (35.7191)  time: 0.0450  data: 0.0024  max mem: 3726
Test:  [40/79]  eta: 0:00:06  loss: 4.5997 (4.6089)  acc1: 5.2083 (6.8598)  acc5: 30.2083 (33.8161)  time: 0.0661  data: 0.0192  max mem: 3726
Test:  [50/79]  eta: 0:00:04  loss: 4.8468 (4.6287)  acc1: 5.2083 (7.0057)  acc5: 29.1667 (33.0882)  time: 0.0788  data: 0.0169  max mem: 3726
Test:  [60/79]  eta: 0:00:02  loss: 4.7795 (4.6744)  acc1: 6.2500 (6.5915)  acc5: 31.2500 (33.1113)  time: 0.0603  data: 0.0001  max mem: 3726
Test:  [70/79]  eta: 0:00:01  loss: 4.8400 (4.7152)  acc1: 4.1667 (6.2353)  acc5: 30.2083 (32.5851)  time: 0.0478  data: 0.0001  max mem: 3726
Test:  [78/79]  eta: 0:00:00  loss: 4.9331 (4.7492)  acc1: 3.1250 (5.9067)  acc5: 29.1667 (32.2667)  time: 0.0629  data: 0.0001  max mem: 3726
Test: Total time: 0:00:08 (0.1130 s / it)
* Acc@1 5.907 Acc@5 32.267 loss 4.749
Accuracy of the network on 7500 test images: 5.90667%
Not using distributed mode
Namespace(Decom=False, aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=64, bn=False, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/sliu/project_space/datasets/imagenet-a', data_set='IMNET', data_type='imagenet-a', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.1, enable_wandb=False, epochs=300, eval=True, eval_data_path=None, finetune='', fix=False, growth='random', head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, kernel_size=[7, 7, 7, 7, 100], layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=-1, log_dir=None, lr=0.004, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='resnet50', model_ema=False, model_ema_decay=0.9999, model_ema_eval=False, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_workers=10, only_L=False, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', parallel=True, pin_mem=True, project='SLaK', prune='magnitude', prune_rate=0.3, recount=1, redistribution='none', remode='pixel', reprob=0.25, resplit=False, resume='/gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_SLakTTeachers_STRN50_NKD/checkpoint-best.pth', save_ckpt=True, save_ckpt_freq=10, save_ckpt_num=3, seed=0, smoothing=0.1, sparse=False, sparse_init='snip', sparsity=0.4, start_epoch=0, train_interpolation='bicubic', update_freq=1, update_frequency=100, use_amp=False, verbose=False, wandb_ckpt=False, warmup_epochs=20, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, width_factor=1.0, world_size=1)
Transform = 
Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /home/sliu/project_space/datasets/imagenet-a
Number of the class = 1000
Mixup is activated!
Model = ResNet(
  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)
  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (act1): ReLU(inplace=True)
  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)
  (layer1): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer2): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer3): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (3): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (4): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (5): Bottleneck(
      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (layer4): Sequential(
    (0): Bottleneck(
      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
      (downsample): Sequential(
        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)
        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      )
    )
    (1): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
    (2): Bottleneck(
      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act1): ReLU(inplace=True)
      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act2): ReLU(inplace=True)
      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act3): ReLU(inplace=True)
    )
  )
  (global_pool): SelectAdaptivePool2d (pool_type=avg, flatten=True)
  (fc): Linear(in_features=2048, out_features=1000, bias=True)
)
number of params: 25557032
LR = 0.00400000
Batch size = 64
Update frequent = 1
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "conv1.weight",
      "layer1.0.conv1.weight",
      "layer1.0.conv2.weight",
      "layer1.0.conv3.weight",
      "layer1.0.downsample.0.weight",
      "layer1.1.conv1.weight",
      "layer1.1.conv2.weight",
      "layer1.1.conv3.weight",
      "layer1.2.conv1.weight",
      "layer1.2.conv2.weight",
      "layer1.2.conv3.weight",
      "layer2.0.conv1.weight",
      "layer2.0.conv2.weight",
      "layer2.0.conv3.weight",
      "layer2.0.downsample.0.weight",
      "layer2.1.conv1.weight",
      "layer2.1.conv2.weight",
      "layer2.1.conv3.weight",
      "layer2.2.conv1.weight",
      "layer2.2.conv2.weight",
      "layer2.2.conv3.weight",
      "layer2.3.conv1.weight",
      "layer2.3.conv2.weight",
      "layer2.3.conv3.weight",
      "layer3.0.conv1.weight",
      "layer3.0.conv2.weight",
      "layer3.0.conv3.weight",
      "layer3.0.downsample.0.weight",
      "layer3.1.conv1.weight",
      "layer3.1.conv2.weight",
      "layer3.1.conv3.weight",
      "layer3.2.conv1.weight",
      "layer3.2.conv2.weight",
      "layer3.2.conv3.weight",
      "layer3.3.conv1.weight",
      "layer3.3.conv2.weight",
      "layer3.3.conv3.weight",
      "layer3.4.conv1.weight",
      "layer3.4.conv2.weight",
      "layer3.4.conv3.weight",
      "layer3.5.conv1.weight",
      "layer3.5.conv2.weight",
      "layer3.5.conv3.weight",
      "layer4.0.conv1.weight",
      "layer4.0.conv2.weight",
      "layer4.0.conv3.weight",
      "layer4.0.downsample.0.weight",
      "layer4.1.conv1.weight",
      "layer4.1.conv2.weight",
      "layer4.1.conv3.weight",
      "layer4.2.conv1.weight",
      "layer4.2.conv2.weight",
      "layer4.2.conv3.weight",
      "fc.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "bn1.weight",
      "bn1.bias",
      "layer1.0.bn1.weight",
      "layer1.0.bn1.bias",
      "layer1.0.bn2.weight",
      "layer1.0.bn2.bias",
      "layer1.0.bn3.weight",
      "layer1.0.bn3.bias",
      "layer1.0.downsample.1.weight",
      "layer1.0.downsample.1.bias",
      "layer1.1.bn1.weight",
      "layer1.1.bn1.bias",
      "layer1.1.bn2.weight",
      "layer1.1.bn2.bias",
      "layer1.1.bn3.weight",
      "layer1.1.bn3.bias",
      "layer1.2.bn1.weight",
      "layer1.2.bn1.bias",
      "layer1.2.bn2.weight",
      "layer1.2.bn2.bias",
      "layer1.2.bn3.weight",
      "layer1.2.bn3.bias",
      "layer2.0.bn1.weight",
      "layer2.0.bn1.bias",
      "layer2.0.bn2.weight",
      "layer2.0.bn2.bias",
      "layer2.0.bn3.weight",
      "layer2.0.bn3.bias",
      "layer2.0.downsample.1.weight",
      "layer2.0.downsample.1.bias",
      "layer2.1.bn1.weight",
      "layer2.1.bn1.bias",
      "layer2.1.bn2.weight",
      "layer2.1.bn2.bias",
      "layer2.1.bn3.weight",
      "layer2.1.bn3.bias",
      "layer2.2.bn1.weight",
      "layer2.2.bn1.bias",
      "layer2.2.bn2.weight",
      "layer2.2.bn2.bias",
      "layer2.2.bn3.weight",
      "layer2.2.bn3.bias",
      "layer2.3.bn1.weight",
      "layer2.3.bn1.bias",
      "layer2.3.bn2.weight",
      "layer2.3.bn2.bias",
      "layer2.3.bn3.weight",
      "layer2.3.bn3.bias",
      "layer3.0.bn1.weight",
      "layer3.0.bn1.bias",
      "layer3.0.bn2.weight",
      "layer3.0.bn2.bias",
      "layer3.0.bn3.weight",
      "layer3.0.bn3.bias",
      "layer3.0.downsample.1.weight",
      "layer3.0.downsample.1.bias",
      "layer3.1.bn1.weight",
      "layer3.1.bn1.bias",
      "layer3.1.bn2.weight",
      "layer3.1.bn2.bias",
      "layer3.1.bn3.weight",
      "layer3.1.bn3.bias",
      "layer3.2.bn1.weight",
      "layer3.2.bn1.bias",
      "layer3.2.bn2.weight",
      "layer3.2.bn2.bias",
      "layer3.2.bn3.weight",
      "layer3.2.bn3.bias",
      "layer3.3.bn1.weight",
      "layer3.3.bn1.bias",
      "layer3.3.bn2.weight",
      "layer3.3.bn2.bias",
      "layer3.3.bn3.weight",
      "layer3.3.bn3.bias",
      "layer3.4.bn1.weight",
      "layer3.4.bn1.bias",
      "layer3.4.bn2.weight",
      "layer3.4.bn2.bias",
      "layer3.4.bn3.weight",
      "layer3.4.bn3.bias",
      "layer3.5.bn1.weight",
      "layer3.5.bn1.bias",
      "layer3.5.bn2.weight",
      "layer3.5.bn2.bias",
      "layer3.5.bn3.weight",
      "layer3.5.bn3.bias",
      "layer4.0.bn1.weight",
      "layer4.0.bn1.bias",
      "layer4.0.bn2.weight",
      "layer4.0.bn2.bias",
      "layer4.0.bn3.weight",
      "layer4.0.bn3.bias",
      "layer4.0.downsample.1.weight",
      "layer4.0.downsample.1.bias",
      "layer4.1.bn1.weight",
      "layer4.1.bn1.bias",
      "layer4.1.bn2.weight",
      "layer4.1.bn2.bias",
      "layer4.1.bn3.weight",
      "layer4.1.bn3.bias",
      "layer4.2.bn1.weight",
      "layer4.2.bn1.bias",
      "layer4.2.bn2.weight",
      "layer4.2.bn2.bias",
      "layer4.2.bn3.weight",
      "layer4.2.bn3.bias",
      "fc.bias"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
criterion = SoftTargetCrossEntropy()
dict_keys(['model', 'optimizer', 'epoch', 'scaler', 'args', 'model_ema'])
Resume checkpoint /gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_SLakTTeachers_STRN50_NKD/checkpoint-best.pth
With optim & sched!
conv1.weight density is 1.0
bn1.weight density is 1.0
bn1.bias density is 1.0
layer1.0.conv1.weight density is 1.0
layer1.0.bn1.weight density is 1.0
layer1.0.bn1.bias density is 1.0
layer1.0.conv2.weight density is 1.0
layer1.0.bn2.weight density is 1.0
layer1.0.bn2.bias density is 1.0
layer1.0.conv3.weight density is 1.0
layer1.0.bn3.weight density is 1.0
layer1.0.bn3.bias density is 1.0
layer1.0.downsample.0.weight density is 1.0
layer1.0.downsample.1.weight density is 1.0
layer1.0.downsample.1.bias density is 1.0
layer1.1.conv1.weight density is 1.0
layer1.1.bn1.weight density is 1.0
layer1.1.bn1.bias density is 1.0
layer1.1.conv2.weight density is 1.0
layer1.1.bn2.weight density is 1.0
layer1.1.bn2.bias density is 1.0
layer1.1.conv3.weight density is 1.0
layer1.1.bn3.weight density is 1.0
layer1.1.bn3.bias density is 1.0
layer1.2.conv1.weight density is 1.0
layer1.2.bn1.weight density is 1.0
layer1.2.bn1.bias density is 1.0
layer1.2.conv2.weight density is 1.0
layer1.2.bn2.weight density is 1.0
layer1.2.bn2.bias density is 1.0
layer1.2.conv3.weight density is 1.0
layer1.2.bn3.weight density is 1.0
layer1.2.bn3.bias density is 1.0
layer2.0.conv1.weight density is 1.0
layer2.0.bn1.weight density is 1.0
layer2.0.bn1.bias density is 1.0
layer2.0.conv2.weight density is 1.0
layer2.0.bn2.weight density is 1.0
layer2.0.bn2.bias density is 1.0
layer2.0.conv3.weight density is 1.0
layer2.0.bn3.weight density is 1.0
layer2.0.bn3.bias density is 1.0
layer2.0.downsample.0.weight density is 1.0
layer2.0.downsample.1.weight density is 1.0
layer2.0.downsample.1.bias density is 1.0
layer2.1.conv1.weight density is 1.0
layer2.1.bn1.weight density is 1.0
layer2.1.bn1.bias density is 1.0
layer2.1.conv2.weight density is 1.0
layer2.1.bn2.weight density is 1.0
layer2.1.bn2.bias density is 1.0
layer2.1.conv3.weight density is 1.0
layer2.1.bn3.weight density is 1.0
layer2.1.bn3.bias density is 1.0
layer2.2.conv1.weight density is 1.0
layer2.2.bn1.weight density is 1.0
layer2.2.bn1.bias density is 1.0
layer2.2.conv2.weight density is 1.0
layer2.2.bn2.weight density is 1.0
layer2.2.bn2.bias density is 1.0
layer2.2.conv3.weight density is 1.0
layer2.2.bn3.weight density is 1.0
layer2.2.bn3.bias density is 1.0
layer2.3.conv1.weight density is 1.0
layer2.3.bn1.weight density is 1.0
layer2.3.bn1.bias density is 1.0
layer2.3.conv2.weight density is 1.0
layer2.3.bn2.weight density is 1.0
layer2.3.bn2.bias density is 1.0
layer2.3.conv3.weight density is 1.0
layer2.3.bn3.weight density is 1.0
layer2.3.bn3.bias density is 1.0
layer3.0.conv1.weight density is 1.0
layer3.0.bn1.weight density is 1.0
layer3.0.bn1.bias density is 1.0
layer3.0.conv2.weight density is 1.0
layer3.0.bn2.weight density is 1.0
layer3.0.bn2.bias density is 1.0
layer3.0.conv3.weight density is 1.0
layer3.0.bn3.weight density is 1.0
layer3.0.bn3.bias density is 1.0
layer3.0.downsample.0.weight density is 1.0
layer3.0.downsample.1.weight density is 1.0
layer3.0.downsample.1.bias density is 1.0
layer3.1.conv1.weight density is 1.0
layer3.1.bn1.weight density is 1.0
layer3.1.bn1.bias density is 1.0
layer3.1.conv2.weight density is 1.0
layer3.1.bn2.weight density is 1.0
layer3.1.bn2.bias density is 1.0
layer3.1.conv3.weight density is 1.0
layer3.1.bn3.weight density is 1.0
layer3.1.bn3.bias density is 1.0
layer3.2.conv1.weight density is 1.0
layer3.2.bn1.weight density is 1.0
layer3.2.bn1.bias density is 1.0
layer3.2.conv2.weight density is 1.0
layer3.2.bn2.weight density is 1.0
layer3.2.bn2.bias density is 1.0
layer3.2.conv3.weight density is 1.0
layer3.2.bn3.weight density is 1.0
layer3.2.bn3.bias density is 1.0
layer3.3.conv1.weight density is 1.0
layer3.3.bn1.weight density is 1.0
layer3.3.bn1.bias density is 1.0
layer3.3.conv2.weight density is 1.0
layer3.3.bn2.weight density is 1.0
layer3.3.bn2.bias density is 1.0
layer3.3.conv3.weight density is 1.0
layer3.3.bn3.weight density is 1.0
layer3.3.bn3.bias density is 1.0
layer3.4.conv1.weight density is 1.0
layer3.4.bn1.weight density is 1.0
layer3.4.bn1.bias density is 1.0
layer3.4.conv2.weight density is 1.0
layer3.4.bn2.weight density is 1.0
layer3.4.bn2.bias density is 1.0
layer3.4.conv3.weight density is 1.0
layer3.4.bn3.weight density is 1.0
layer3.4.bn3.bias density is 1.0
layer3.5.conv1.weight density is 1.0
layer3.5.bn1.weight density is 1.0
layer3.5.bn1.bias density is 1.0
layer3.5.conv2.weight density is 1.0
layer3.5.bn2.weight density is 1.0
layer3.5.bn2.bias density is 1.0
layer3.5.conv3.weight density is 1.0
layer3.5.bn3.weight density is 1.0
layer3.5.bn3.bias density is 1.0
layer4.0.conv1.weight density is 1.0
layer4.0.bn1.weight density is 1.0
layer4.0.bn1.bias density is 1.0
layer4.0.conv2.weight density is 1.0
layer4.0.bn2.weight density is 1.0
layer4.0.bn2.bias density is 1.0
layer4.0.conv3.weight density is 1.0
layer4.0.bn3.weight density is 1.0
layer4.0.bn3.bias density is 1.0
layer4.0.downsample.0.weight density is 1.0
layer4.0.downsample.1.weight density is 1.0
layer4.0.downsample.1.bias density is 1.0
layer4.1.conv1.weight density is 1.0
layer4.1.bn1.weight density is 1.0
layer4.1.bn1.bias density is 1.0
layer4.1.conv2.weight density is 1.0
layer4.1.bn2.weight density is 1.0
layer4.1.bn2.bias density is 1.0
layer4.1.conv3.weight density is 1.0
layer4.1.bn3.weight density is 1.0
layer4.1.bn3.bias density is 1.0
layer4.2.conv1.weight density is 1.0
layer4.2.bn1.weight density is 1.0
layer4.2.bn1.bias density is 1.0
layer4.2.conv2.weight density is 1.0
layer4.2.bn2.weight density is 1.0
layer4.2.bn2.bias density is 1.0
layer4.2.conv3.weight density is 1.0
layer4.2.bn3.weight density is 1.0
layer4.2.bn3.bias density is 1.0
fc.weight density is 1.0
fc.bias density is 1.0
Test:  [ 0/79]  eta: 0:05:54  loss: 4.2345 (4.2345)  acc1: 11.4583 (11.4583)  acc5: 44.7917 (44.7917)  time: 4.4844  data: 1.9352  max mem: 3726
Test:  [10/79]  eta: 0:00:30  loss: 4.4165 (4.4462)  acc1: 9.3750 (9.0909)  acc5: 37.5000 (36.1742)  time: 0.4467  data: 0.1760  max mem: 3726
Test:  [20/79]  eta: 0:00:14  loss: 4.4165 (4.4805)  acc1: 8.3333 (9.1766)  acc5: 32.2917 (35.0694)  time: 0.0426  data: 0.0001  max mem: 3726
Test:  [30/79]  eta: 0:00:10  loss: 4.5015 (4.4649)  acc1: 8.3333 (9.3750)  acc5: 33.3333 (36.8952)  time: 0.0851  data: 0.0150  max mem: 3726
Test:  [40/79]  eta: 0:00:06  loss: 4.5397 (4.5733)  acc1: 6.2500 (8.3333)  acc5: 33.3333 (35.2388)  time: 0.0950  data: 0.0150  max mem: 3726
Test:  [50/79]  eta: 0:00:04  loss: 4.6562 (4.5903)  acc1: 6.2500 (8.3946)  acc5: 29.1667 (34.3342)  time: 0.0737  data: 0.0129  max mem: 3726
Test:  [60/79]  eta: 0:00:02  loss: 4.7865 (4.6354)  acc1: 6.2500 (7.8552)  acc5: 32.2917 (34.2896)  time: 0.0834  data: 0.0193  max mem: 3726
Test:  [70/79]  eta: 0:00:01  loss: 4.8772 (4.6744)  acc1: 5.2083 (7.4090)  acc5: 30.2083 (33.6708)  time: 0.0793  data: 0.0065  max mem: 3726
Test:  [78/79]  eta: 0:00:00  loss: 4.8823 (4.7096)  acc1: 4.1667 (7.1200)  acc5: 29.1667 (33.3733)  time: 0.0800  data: 0.0001  max mem: 3726
Test: Total time: 0:00:10 (0.1319 s / it)
* Acc@1 7.120 Acc@5 33.373 loss 4.710
Accuracy of the network on 7500 test images: 7.12000%
Not using distributed mode
Namespace(Decom=False, aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=64, bn=True, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/sliu/project_space/datasets/imagenet-a', data_set='IMNET', data_type='imagenet-a', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.1, enable_wandb=False, epochs=300, eval=True, eval_data_path=None, finetune='', fix=False, growth='random', head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, kernel_size=[7, 7, 7, 7, 100], layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=-1, log_dir=None, lr=0.004, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='SLaK_tiny', model_ema=False, model_ema_decay=0.9999, model_ema_eval=False, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_workers=10, only_L=False, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', parallel=True, pin_mem=True, project='SLaK', prune='magnitude', prune_rate=0.3, recount=1, redistribution='none', remode='pixel', reprob=0.25, resplit=False, resume='/gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T3_bnTrue_Teacherswin_STConvNext_NKD/checkpoint-best.pth', save_ckpt=True, save_ckpt_freq=10, save_ckpt_num=3, seed=0, smoothing=0.1, sparse=False, sparse_init='snip', sparsity=0.4, start_epoch=0, train_interpolation='bicubic', update_freq=1, update_frequency=100, use_amp=False, verbose=False, wandb_ckpt=False, warmup_epochs=20, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, width_factor=1.0, world_size=1)
Transform = 
Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /home/sliu/project_space/datasets/imagenet-a
Number of the class = 1000
Mixup is activated!
Model = SLaK(
  (downsample_layers): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (1): LayerNorm()
    )
    (1): Sequential(
      (0): LayerNorm()
      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))
    )
    (2): Sequential(
      (0): LayerNorm()
      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))
    )
    (3): Sequential(
      (0): LayerNorm()
      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (stages): ModuleList(
    (0): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): Identity()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): DropPath()
      )
    )
    (1): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
    )
    (2): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (3): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (4): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (5): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (6): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (7): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (8): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
    )
    (3): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
number of params: 28595752
LR = 0.00400000
Batch size = 64
Update frequent = 1
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "downsample_layers.0.0.weight",
      "downsample_layers.1.1.weight",
      "downsample_layers.2.1.weight",
      "downsample_layers.3.1.weight",
      "stages.0.0.large_kernel.lkb_origin.conv.weight",
      "stages.0.0.pwconv1.weight",
      "stages.0.0.pwconv2.weight",
      "stages.0.1.large_kernel.lkb_origin.conv.weight",
      "stages.0.1.pwconv1.weight",
      "stages.0.1.pwconv2.weight",
      "stages.0.2.large_kernel.lkb_origin.conv.weight",
      "stages.0.2.pwconv1.weight",
      "stages.0.2.pwconv2.weight",
      "stages.1.0.large_kernel.lkb_origin.conv.weight",
      "stages.1.0.pwconv1.weight",
      "stages.1.0.pwconv2.weight",
      "stages.1.1.large_kernel.lkb_origin.conv.weight",
      "stages.1.1.pwconv1.weight",
      "stages.1.1.pwconv2.weight",
      "stages.1.2.large_kernel.lkb_origin.conv.weight",
      "stages.1.2.pwconv1.weight",
      "stages.1.2.pwconv2.weight",
      "stages.2.0.large_kernel.lkb_origin.conv.weight",
      "stages.2.0.pwconv1.weight",
      "stages.2.0.pwconv2.weight",
      "stages.2.1.large_kernel.lkb_origin.conv.weight",
      "stages.2.1.pwconv1.weight",
      "stages.2.1.pwconv2.weight",
      "stages.2.2.large_kernel.lkb_origin.conv.weight",
      "stages.2.2.pwconv1.weight",
      "stages.2.2.pwconv2.weight",
      "stages.2.3.large_kernel.lkb_origin.conv.weight",
      "stages.2.3.pwconv1.weight",
      "stages.2.3.pwconv2.weight",
      "stages.2.4.large_kernel.lkb_origin.conv.weight",
      "stages.2.4.pwconv1.weight",
      "stages.2.4.pwconv2.weight",
      "stages.2.5.large_kernel.lkb_origin.conv.weight",
      "stages.2.5.pwconv1.weight",
      "stages.2.5.pwconv2.weight",
      "stages.2.6.large_kernel.lkb_origin.conv.weight",
      "stages.2.6.pwconv1.weight",
      "stages.2.6.pwconv2.weight",
      "stages.2.7.large_kernel.lkb_origin.conv.weight",
      "stages.2.7.pwconv1.weight",
      "stages.2.7.pwconv2.weight",
      "stages.2.8.large_kernel.lkb_origin.conv.weight",
      "stages.2.8.pwconv1.weight",
      "stages.2.8.pwconv2.weight",
      "stages.3.0.large_kernel.lkb_origin.conv.weight",
      "stages.3.0.pwconv1.weight",
      "stages.3.0.pwconv2.weight",
      "stages.3.1.large_kernel.lkb_origin.conv.weight",
      "stages.3.1.pwconv1.weight",
      "stages.3.1.pwconv2.weight",
      "stages.3.2.large_kernel.lkb_origin.conv.weight",
      "stages.3.2.pwconv1.weight",
      "stages.3.2.pwconv2.weight",
      "head.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "downsample_layers.0.0.bias",
      "downsample_layers.0.1.weight",
      "downsample_layers.0.1.bias",
      "downsample_layers.1.0.weight",
      "downsample_layers.1.0.bias",
      "downsample_layers.1.1.bias",
      "downsample_layers.2.0.weight",
      "downsample_layers.2.0.bias",
      "downsample_layers.2.1.bias",
      "downsample_layers.3.0.weight",
      "downsample_layers.3.0.bias",
      "downsample_layers.3.1.bias",
      "stages.0.0.gamma",
      "stages.0.0.large_kernel.lkb_origin.bn.weight",
      "stages.0.0.large_kernel.lkb_origin.bn.bias",
      "stages.0.0.norm.weight",
      "stages.0.0.norm.bias",
      "stages.0.0.pwconv1.bias",
      "stages.0.0.pwconv2.bias",
      "stages.0.1.gamma",
      "stages.0.1.large_kernel.lkb_origin.bn.weight",
      "stages.0.1.large_kernel.lkb_origin.bn.bias",
      "stages.0.1.norm.weight",
      "stages.0.1.norm.bias",
      "stages.0.1.pwconv1.bias",
      "stages.0.1.pwconv2.bias",
      "stages.0.2.gamma",
      "stages.0.2.large_kernel.lkb_origin.bn.weight",
      "stages.0.2.large_kernel.lkb_origin.bn.bias",
      "stages.0.2.norm.weight",
      "stages.0.2.norm.bias",
      "stages.0.2.pwconv1.bias",
      "stages.0.2.pwconv2.bias",
      "stages.1.0.gamma",
      "stages.1.0.large_kernel.lkb_origin.bn.weight",
      "stages.1.0.large_kernel.lkb_origin.bn.bias",
      "stages.1.0.norm.weight",
      "stages.1.0.norm.bias",
      "stages.1.0.pwconv1.bias",
      "stages.1.0.pwconv2.bias",
      "stages.1.1.gamma",
      "stages.1.1.large_kernel.lkb_origin.bn.weight",
      "stages.1.1.large_kernel.lkb_origin.bn.bias",
      "stages.1.1.norm.weight",
      "stages.1.1.norm.bias",
      "stages.1.1.pwconv1.bias",
      "stages.1.1.pwconv2.bias",
      "stages.1.2.gamma",
      "stages.1.2.large_kernel.lkb_origin.bn.weight",
      "stages.1.2.large_kernel.lkb_origin.bn.bias",
      "stages.1.2.norm.weight",
      "stages.1.2.norm.bias",
      "stages.1.2.pwconv1.bias",
      "stages.1.2.pwconv2.bias",
      "stages.2.0.gamma",
      "stages.2.0.large_kernel.lkb_origin.bn.weight",
      "stages.2.0.large_kernel.lkb_origin.bn.bias",
      "stages.2.0.norm.weight",
      "stages.2.0.norm.bias",
      "stages.2.0.pwconv1.bias",
      "stages.2.0.pwconv2.bias",
      "stages.2.1.gamma",
      "stages.2.1.large_kernel.lkb_origin.bn.weight",
      "stages.2.1.large_kernel.lkb_origin.bn.bias",
      "stages.2.1.norm.weight",
      "stages.2.1.norm.bias",
      "stages.2.1.pwconv1.bias",
      "stages.2.1.pwconv2.bias",
      "stages.2.2.gamma",
      "stages.2.2.large_kernel.lkb_origin.bn.weight",
      "stages.2.2.large_kernel.lkb_origin.bn.bias",
      "stages.2.2.norm.weight",
      "stages.2.2.norm.bias",
      "stages.2.2.pwconv1.bias",
      "stages.2.2.pwconv2.bias",
      "stages.2.3.gamma",
      "stages.2.3.large_kernel.lkb_origin.bn.weight",
      "stages.2.3.large_kernel.lkb_origin.bn.bias",
      "stages.2.3.norm.weight",
      "stages.2.3.norm.bias",
      "stages.2.3.pwconv1.bias",
      "stages.2.3.pwconv2.bias",
      "stages.2.4.gamma",
      "stages.2.4.large_kernel.lkb_origin.bn.weight",
      "stages.2.4.large_kernel.lkb_origin.bn.bias",
      "stages.2.4.norm.weight",
      "stages.2.4.norm.bias",
      "stages.2.4.pwconv1.bias",
      "stages.2.4.pwconv2.bias",
      "stages.2.5.gamma",
      "stages.2.5.large_kernel.lkb_origin.bn.weight",
      "stages.2.5.large_kernel.lkb_origin.bn.bias",
      "stages.2.5.norm.weight",
      "stages.2.5.norm.bias",
      "stages.2.5.pwconv1.bias",
      "stages.2.5.pwconv2.bias",
      "stages.2.6.gamma",
      "stages.2.6.large_kernel.lkb_origin.bn.weight",
      "stages.2.6.large_kernel.lkb_origin.bn.bias",
      "stages.2.6.norm.weight",
      "stages.2.6.norm.bias",
      "stages.2.6.pwconv1.bias",
      "stages.2.6.pwconv2.bias",
      "stages.2.7.gamma",
      "stages.2.7.large_kernel.lkb_origin.bn.weight",
      "stages.2.7.large_kernel.lkb_origin.bn.bias",
      "stages.2.7.norm.weight",
      "stages.2.7.norm.bias",
      "stages.2.7.pwconv1.bias",
      "stages.2.7.pwconv2.bias",
      "stages.2.8.gamma",
      "stages.2.8.large_kernel.lkb_origin.bn.weight",
      "stages.2.8.large_kernel.lkb_origin.bn.bias",
      "stages.2.8.norm.weight",
      "stages.2.8.norm.bias",
      "stages.2.8.pwconv1.bias",
      "stages.2.8.pwconv2.bias",
      "stages.3.0.gamma",
      "stages.3.0.large_kernel.lkb_origin.bn.weight",
      "stages.3.0.large_kernel.lkb_origin.bn.bias",
      "stages.3.0.norm.weight",
      "stages.3.0.norm.bias",
      "stages.3.0.pwconv1.bias",
      "stages.3.0.pwconv2.bias",
      "stages.3.1.gamma",
      "stages.3.1.large_kernel.lkb_origin.bn.weight",
      "stages.3.1.large_kernel.lkb_origin.bn.bias",
      "stages.3.1.norm.weight",
      "stages.3.1.norm.bias",
      "stages.3.1.pwconv1.bias",
      "stages.3.1.pwconv2.bias",
      "stages.3.2.gamma",
      "stages.3.2.large_kernel.lkb_origin.bn.weight",
      "stages.3.2.large_kernel.lkb_origin.bn.bias",
      "stages.3.2.norm.weight",
      "stages.3.2.norm.bias",
      "stages.3.2.pwconv1.bias",
      "stages.3.2.pwconv2.bias",
      "norm.weight",
      "norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
criterion = SoftTargetCrossEntropy()
dict_keys(['model', 'optimizer', 'epoch', 'scaler', 'args', 'model_ema'])
Resume checkpoint /gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T3_bnTrue_Teacherswin_STConvNext_NKD/checkpoint-best.pth
With optim & sched!
downsample_layers.0.0.weight density is 1.0
downsample_layers.0.0.bias density is 1.0
downsample_layers.0.1.weight density is 1.0
downsample_layers.0.1.bias density is 1.0
downsample_layers.1.0.weight density is 1.0
downsample_layers.1.0.bias density is 1.0
downsample_layers.1.1.weight density is 1.0
downsample_layers.1.1.bias density is 1.0
downsample_layers.2.0.weight density is 1.0
downsample_layers.2.0.bias density is 1.0
downsample_layers.2.1.weight density is 1.0
downsample_layers.2.1.bias density is 1.0
downsample_layers.3.0.weight density is 1.0
downsample_layers.3.0.bias density is 1.0
downsample_layers.3.1.weight density is 1.0
downsample_layers.3.1.bias density is 1.0
stages.0.0.gamma density is 1.0
stages.0.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.0.norm.weight density is 1.0
stages.0.0.norm.bias density is 1.0
stages.0.0.pwconv1.weight density is 1.0
stages.0.0.pwconv1.bias density is 1.0
stages.0.0.pwconv2.weight density is 1.0
stages.0.0.pwconv2.bias density is 1.0
stages.0.1.gamma density is 1.0
stages.0.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.1.norm.weight density is 1.0
stages.0.1.norm.bias density is 1.0
stages.0.1.pwconv1.weight density is 1.0
stages.0.1.pwconv1.bias density is 1.0
stages.0.1.pwconv2.weight density is 1.0
stages.0.1.pwconv2.bias density is 1.0
stages.0.2.gamma density is 1.0
stages.0.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.2.norm.weight density is 1.0
stages.0.2.norm.bias density is 1.0
stages.0.2.pwconv1.weight density is 1.0
stages.0.2.pwconv1.bias density is 1.0
stages.0.2.pwconv2.weight density is 1.0
stages.0.2.pwconv2.bias density is 1.0
stages.1.0.gamma density is 1.0
stages.1.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.0.norm.weight density is 1.0
stages.1.0.norm.bias density is 1.0
stages.1.0.pwconv1.weight density is 1.0
stages.1.0.pwconv1.bias density is 1.0
stages.1.0.pwconv2.weight density is 1.0
stages.1.0.pwconv2.bias density is 1.0
stages.1.1.gamma density is 1.0
stages.1.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.1.norm.weight density is 1.0
stages.1.1.norm.bias density is 1.0
stages.1.1.pwconv1.weight density is 1.0
stages.1.1.pwconv1.bias density is 1.0
stages.1.1.pwconv2.weight density is 1.0
stages.1.1.pwconv2.bias density is 1.0
stages.1.2.gamma density is 1.0
stages.1.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.2.norm.weight density is 1.0
stages.1.2.norm.bias density is 1.0
stages.1.2.pwconv1.weight density is 1.0
stages.1.2.pwconv1.bias density is 1.0
stages.1.2.pwconv2.weight density is 1.0
stages.1.2.pwconv2.bias density is 1.0
stages.2.0.gamma density is 1.0
stages.2.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.0.norm.weight density is 1.0
stages.2.0.norm.bias density is 1.0
stages.2.0.pwconv1.weight density is 1.0
stages.2.0.pwconv1.bias density is 1.0
stages.2.0.pwconv2.weight density is 1.0
stages.2.0.pwconv2.bias density is 1.0
stages.2.1.gamma density is 1.0
stages.2.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.1.norm.weight density is 1.0
stages.2.1.norm.bias density is 1.0
stages.2.1.pwconv1.weight density is 1.0
stages.2.1.pwconv1.bias density is 1.0
stages.2.1.pwconv2.weight density is 1.0
stages.2.1.pwconv2.bias density is 1.0
stages.2.2.gamma density is 1.0
stages.2.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.2.norm.weight density is 1.0
stages.2.2.norm.bias density is 1.0
stages.2.2.pwconv1.weight density is 1.0
stages.2.2.pwconv1.bias density is 1.0
stages.2.2.pwconv2.weight density is 1.0
stages.2.2.pwconv2.bias density is 1.0
stages.2.3.gamma density is 1.0
stages.2.3.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.3.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.3.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.3.norm.weight density is 1.0
stages.2.3.norm.bias density is 1.0
stages.2.3.pwconv1.weight density is 1.0
stages.2.3.pwconv1.bias density is 1.0
stages.2.3.pwconv2.weight density is 1.0
stages.2.3.pwconv2.bias density is 1.0
stages.2.4.gamma density is 1.0
stages.2.4.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.4.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.4.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.4.norm.weight density is 1.0
stages.2.4.norm.bias density is 1.0
stages.2.4.pwconv1.weight density is 1.0
stages.2.4.pwconv1.bias density is 1.0
stages.2.4.pwconv2.weight density is 1.0
stages.2.4.pwconv2.bias density is 1.0
stages.2.5.gamma density is 1.0
stages.2.5.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.5.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.5.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.5.norm.weight density is 1.0
stages.2.5.norm.bias density is 1.0
stages.2.5.pwconv1.weight density is 1.0
stages.2.5.pwconv1.bias density is 1.0
stages.2.5.pwconv2.weight density is 1.0
stages.2.5.pwconv2.bias density is 1.0
stages.2.6.gamma density is 1.0
stages.2.6.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.6.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.6.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.6.norm.weight density is 1.0
stages.2.6.norm.bias density is 1.0
stages.2.6.pwconv1.weight density is 1.0
stages.2.6.pwconv1.bias density is 1.0
stages.2.6.pwconv2.weight density is 1.0
stages.2.6.pwconv2.bias density is 1.0
stages.2.7.gamma density is 1.0
stages.2.7.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.7.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.7.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.7.norm.weight density is 1.0
stages.2.7.norm.bias density is 1.0
stages.2.7.pwconv1.weight density is 1.0
stages.2.7.pwconv1.bias density is 1.0
stages.2.7.pwconv2.weight density is 1.0
stages.2.7.pwconv2.bias density is 1.0
stages.2.8.gamma density is 1.0
stages.2.8.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.8.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.8.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.8.norm.weight density is 1.0
stages.2.8.norm.bias density is 1.0
stages.2.8.pwconv1.weight density is 1.0
stages.2.8.pwconv1.bias density is 1.0
stages.2.8.pwconv2.weight density is 1.0
stages.2.8.pwconv2.bias density is 1.0
stages.3.0.gamma density is 1.0
stages.3.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.0.norm.weight density is 1.0
stages.3.0.norm.bias density is 1.0
stages.3.0.pwconv1.weight density is 1.0
stages.3.0.pwconv1.bias density is 1.0
stages.3.0.pwconv2.weight density is 1.0
stages.3.0.pwconv2.bias density is 1.0
stages.3.1.gamma density is 1.0
stages.3.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.1.norm.weight density is 1.0
stages.3.1.norm.bias density is 1.0
stages.3.1.pwconv1.weight density is 1.0
stages.3.1.pwconv1.bias density is 1.0
stages.3.1.pwconv2.weight density is 1.0
stages.3.1.pwconv2.bias density is 1.0
stages.3.2.gamma density is 1.0
stages.3.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.2.norm.weight density is 1.0
stages.3.2.norm.bias density is 1.0
stages.3.2.pwconv1.weight density is 1.0
stages.3.2.pwconv1.bias density is 1.0
stages.3.2.pwconv2.weight density is 1.0
stages.3.2.pwconv2.bias density is 1.0
norm.weight density is 1.0
norm.bias density is 1.0
head.weight density is 1.0
head.bias density is 1.0
Test:  [ 0/79]  eta: 0:04:38  loss: 3.5112 (3.5112)  acc1: 25.0000 (25.0000)  acc5: 66.6667 (66.6667)  time: 3.5203  data: 1.8434  max mem: 2139
Test:  [10/79]  eta: 0:00:26  loss: 3.3976 (3.3805)  acc1: 26.0417 (24.5265)  acc5: 61.4583 (60.4167)  time: 0.3781  data: 0.1677  max mem: 2139
Test:  [20/79]  eta: 0:00:13  loss: 3.3347 (3.3639)  acc1: 26.0417 (25.7440)  acc5: 61.4583 (59.7718)  time: 0.0640  data: 0.0001  max mem: 2139
Test:  [30/79]  eta: 0:00:08  loss: 3.3633 (3.4101)  acc1: 26.0417 (24.1263)  acc5: 62.5000 (60.6183)  time: 0.0683  data: 0.0001  max mem: 2139
Test:  [40/79]  eta: 0:00:06  loss: 3.5372 (3.5654)  acc1: 15.6250 (22.1291)  acc5: 58.3333 (57.8506)  time: 0.0767  data: 0.0001  max mem: 2139
Test:  [50/79]  eta: 0:00:04  loss: 3.6576 (3.6009)  acc1: 17.7083 (21.8342)  acc5: 47.9167 (56.1683)  time: 0.0764  data: 0.0007  max mem: 2139
Test:  [60/79]  eta: 0:00:02  loss: 3.9570 (3.7264)  acc1: 16.6667 (20.2015)  acc5: 47.9167 (54.8156)  time: 0.0750  data: 0.0048  max mem: 2139
Test:  [70/79]  eta: 0:00:01  loss: 4.4185 (3.8294)  acc1: 9.3750 (18.7647)  acc5: 46.8750 (53.5211)  time: 0.0738  data: 0.0042  max mem: 2139
Test:  [78/79]  eta: 0:00:00  loss: 4.4808 (3.8832)  acc1: 9.3750 (17.8933)  acc5: 45.8333 (52.7067)  time: 0.0699  data: 0.0001  max mem: 2139
Test: Total time: 0:00:09 (0.1161 s / it)
* Acc@1 17.893 Acc@5 52.707 loss 3.883
Accuracy of the network on 7500 test images: 17.89333%
Not using distributed mode
Namespace(Decom=False, aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=64, bn=True, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/sliu/project_space/datasets/imagenet-a', data_set='IMNET', data_type='imagenet-a', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.1, enable_wandb=False, epochs=300, eval=True, eval_data_path=None, finetune='', fix=False, growth='random', head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, kernel_size=[7, 7, 7, 7, 100], layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=-1, log_dir=None, lr=0.004, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='SLaK_tiny', model_ema=False, model_ema_decay=0.9999, model_ema_eval=False, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_workers=10, only_L=False, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', parallel=True, pin_mem=True, project='SLaK', prune='magnitude', prune_rate=0.3, recount=1, redistribution='none', remode='pixel', reprob=0.25, resplit=False, resume='/gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T3_bnTrue_Teachercswin_STConvNext_NKD/checkpoint-best.pth', save_ckpt=True, save_ckpt_freq=10, save_ckpt_num=3, seed=0, smoothing=0.1, sparse=False, sparse_init='snip', sparsity=0.4, start_epoch=0, train_interpolation='bicubic', update_freq=1, update_frequency=100, use_amp=False, verbose=False, wandb_ckpt=False, warmup_epochs=20, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, width_factor=1.0, world_size=1)
Transform = 
Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /home/sliu/project_space/datasets/imagenet-a
Number of the class = 1000
Mixup is activated!
Model = SLaK(
  (downsample_layers): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (1): LayerNorm()
    )
    (1): Sequential(
      (0): LayerNorm()
      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))
    )
    (2): Sequential(
      (0): LayerNorm()
      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))
    )
    (3): Sequential(
      (0): LayerNorm()
      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (stages): ModuleList(
    (0): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): Identity()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): DropPath()
      )
    )
    (1): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
    )
    (2): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (3): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (4): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (5): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (6): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (7): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (8): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
    )
    (3): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
number of params: 28595752
LR = 0.00400000
Batch size = 64
Update frequent = 1
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "downsample_layers.0.0.weight",
      "downsample_layers.1.1.weight",
      "downsample_layers.2.1.weight",
      "downsample_layers.3.1.weight",
      "stages.0.0.large_kernel.lkb_origin.conv.weight",
      "stages.0.0.pwconv1.weight",
      "stages.0.0.pwconv2.weight",
      "stages.0.1.large_kernel.lkb_origin.conv.weight",
      "stages.0.1.pwconv1.weight",
      "stages.0.1.pwconv2.weight",
      "stages.0.2.large_kernel.lkb_origin.conv.weight",
      "stages.0.2.pwconv1.weight",
      "stages.0.2.pwconv2.weight",
      "stages.1.0.large_kernel.lkb_origin.conv.weight",
      "stages.1.0.pwconv1.weight",
      "stages.1.0.pwconv2.weight",
      "stages.1.1.large_kernel.lkb_origin.conv.weight",
      "stages.1.1.pwconv1.weight",
      "stages.1.1.pwconv2.weight",
      "stages.1.2.large_kernel.lkb_origin.conv.weight",
      "stages.1.2.pwconv1.weight",
      "stages.1.2.pwconv2.weight",
      "stages.2.0.large_kernel.lkb_origin.conv.weight",
      "stages.2.0.pwconv1.weight",
      "stages.2.0.pwconv2.weight",
      "stages.2.1.large_kernel.lkb_origin.conv.weight",
      "stages.2.1.pwconv1.weight",
      "stages.2.1.pwconv2.weight",
      "stages.2.2.large_kernel.lkb_origin.conv.weight",
      "stages.2.2.pwconv1.weight",
      "stages.2.2.pwconv2.weight",
      "stages.2.3.large_kernel.lkb_origin.conv.weight",
      "stages.2.3.pwconv1.weight",
      "stages.2.3.pwconv2.weight",
      "stages.2.4.large_kernel.lkb_origin.conv.weight",
      "stages.2.4.pwconv1.weight",
      "stages.2.4.pwconv2.weight",
      "stages.2.5.large_kernel.lkb_origin.conv.weight",
      "stages.2.5.pwconv1.weight",
      "stages.2.5.pwconv2.weight",
      "stages.2.6.large_kernel.lkb_origin.conv.weight",
      "stages.2.6.pwconv1.weight",
      "stages.2.6.pwconv2.weight",
      "stages.2.7.large_kernel.lkb_origin.conv.weight",
      "stages.2.7.pwconv1.weight",
      "stages.2.7.pwconv2.weight",
      "stages.2.8.large_kernel.lkb_origin.conv.weight",
      "stages.2.8.pwconv1.weight",
      "stages.2.8.pwconv2.weight",
      "stages.3.0.large_kernel.lkb_origin.conv.weight",
      "stages.3.0.pwconv1.weight",
      "stages.3.0.pwconv2.weight",
      "stages.3.1.large_kernel.lkb_origin.conv.weight",
      "stages.3.1.pwconv1.weight",
      "stages.3.1.pwconv2.weight",
      "stages.3.2.large_kernel.lkb_origin.conv.weight",
      "stages.3.2.pwconv1.weight",
      "stages.3.2.pwconv2.weight",
      "head.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "downsample_layers.0.0.bias",
      "downsample_layers.0.1.weight",
      "downsample_layers.0.1.bias",
      "downsample_layers.1.0.weight",
      "downsample_layers.1.0.bias",
      "downsample_layers.1.1.bias",
      "downsample_layers.2.0.weight",
      "downsample_layers.2.0.bias",
      "downsample_layers.2.1.bias",
      "downsample_layers.3.0.weight",
      "downsample_layers.3.0.bias",
      "downsample_layers.3.1.bias",
      "stages.0.0.gamma",
      "stages.0.0.large_kernel.lkb_origin.bn.weight",
      "stages.0.0.large_kernel.lkb_origin.bn.bias",
      "stages.0.0.norm.weight",
      "stages.0.0.norm.bias",
      "stages.0.0.pwconv1.bias",
      "stages.0.0.pwconv2.bias",
      "stages.0.1.gamma",
      "stages.0.1.large_kernel.lkb_origin.bn.weight",
      "stages.0.1.large_kernel.lkb_origin.bn.bias",
      "stages.0.1.norm.weight",
      "stages.0.1.norm.bias",
      "stages.0.1.pwconv1.bias",
      "stages.0.1.pwconv2.bias",
      "stages.0.2.gamma",
      "stages.0.2.large_kernel.lkb_origin.bn.weight",
      "stages.0.2.large_kernel.lkb_origin.bn.bias",
      "stages.0.2.norm.weight",
      "stages.0.2.norm.bias",
      "stages.0.2.pwconv1.bias",
      "stages.0.2.pwconv2.bias",
      "stages.1.0.gamma",
      "stages.1.0.large_kernel.lkb_origin.bn.weight",
      "stages.1.0.large_kernel.lkb_origin.bn.bias",
      "stages.1.0.norm.weight",
      "stages.1.0.norm.bias",
      "stages.1.0.pwconv1.bias",
      "stages.1.0.pwconv2.bias",
      "stages.1.1.gamma",
      "stages.1.1.large_kernel.lkb_origin.bn.weight",
      "stages.1.1.large_kernel.lkb_origin.bn.bias",
      "stages.1.1.norm.weight",
      "stages.1.1.norm.bias",
      "stages.1.1.pwconv1.bias",
      "stages.1.1.pwconv2.bias",
      "stages.1.2.gamma",
      "stages.1.2.large_kernel.lkb_origin.bn.weight",
      "stages.1.2.large_kernel.lkb_origin.bn.bias",
      "stages.1.2.norm.weight",
      "stages.1.2.norm.bias",
      "stages.1.2.pwconv1.bias",
      "stages.1.2.pwconv2.bias",
      "stages.2.0.gamma",
      "stages.2.0.large_kernel.lkb_origin.bn.weight",
      "stages.2.0.large_kernel.lkb_origin.bn.bias",
      "stages.2.0.norm.weight",
      "stages.2.0.norm.bias",
      "stages.2.0.pwconv1.bias",
      "stages.2.0.pwconv2.bias",
      "stages.2.1.gamma",
      "stages.2.1.large_kernel.lkb_origin.bn.weight",
      "stages.2.1.large_kernel.lkb_origin.bn.bias",
      "stages.2.1.norm.weight",
      "stages.2.1.norm.bias",
      "stages.2.1.pwconv1.bias",
      "stages.2.1.pwconv2.bias",
      "stages.2.2.gamma",
      "stages.2.2.large_kernel.lkb_origin.bn.weight",
      "stages.2.2.large_kernel.lkb_origin.bn.bias",
      "stages.2.2.norm.weight",
      "stages.2.2.norm.bias",
      "stages.2.2.pwconv1.bias",
      "stages.2.2.pwconv2.bias",
      "stages.2.3.gamma",
      "stages.2.3.large_kernel.lkb_origin.bn.weight",
      "stages.2.3.large_kernel.lkb_origin.bn.bias",
      "stages.2.3.norm.weight",
      "stages.2.3.norm.bias",
      "stages.2.3.pwconv1.bias",
      "stages.2.3.pwconv2.bias",
      "stages.2.4.gamma",
      "stages.2.4.large_kernel.lkb_origin.bn.weight",
      "stages.2.4.large_kernel.lkb_origin.bn.bias",
      "stages.2.4.norm.weight",
      "stages.2.4.norm.bias",
      "stages.2.4.pwconv1.bias",
      "stages.2.4.pwconv2.bias",
      "stages.2.5.gamma",
      "stages.2.5.large_kernel.lkb_origin.bn.weight",
      "stages.2.5.large_kernel.lkb_origin.bn.bias",
      "stages.2.5.norm.weight",
      "stages.2.5.norm.bias",
      "stages.2.5.pwconv1.bias",
      "stages.2.5.pwconv2.bias",
      "stages.2.6.gamma",
      "stages.2.6.large_kernel.lkb_origin.bn.weight",
      "stages.2.6.large_kernel.lkb_origin.bn.bias",
      "stages.2.6.norm.weight",
      "stages.2.6.norm.bias",
      "stages.2.6.pwconv1.bias",
      "stages.2.6.pwconv2.bias",
      "stages.2.7.gamma",
      "stages.2.7.large_kernel.lkb_origin.bn.weight",
      "stages.2.7.large_kernel.lkb_origin.bn.bias",
      "stages.2.7.norm.weight",
      "stages.2.7.norm.bias",
      "stages.2.7.pwconv1.bias",
      "stages.2.7.pwconv2.bias",
      "stages.2.8.gamma",
      "stages.2.8.large_kernel.lkb_origin.bn.weight",
      "stages.2.8.large_kernel.lkb_origin.bn.bias",
      "stages.2.8.norm.weight",
      "stages.2.8.norm.bias",
      "stages.2.8.pwconv1.bias",
      "stages.2.8.pwconv2.bias",
      "stages.3.0.gamma",
      "stages.3.0.large_kernel.lkb_origin.bn.weight",
      "stages.3.0.large_kernel.lkb_origin.bn.bias",
      "stages.3.0.norm.weight",
      "stages.3.0.norm.bias",
      "stages.3.0.pwconv1.bias",
      "stages.3.0.pwconv2.bias",
      "stages.3.1.gamma",
      "stages.3.1.large_kernel.lkb_origin.bn.weight",
      "stages.3.1.large_kernel.lkb_origin.bn.bias",
      "stages.3.1.norm.weight",
      "stages.3.1.norm.bias",
      "stages.3.1.pwconv1.bias",
      "stages.3.1.pwconv2.bias",
      "stages.3.2.gamma",
      "stages.3.2.large_kernel.lkb_origin.bn.weight",
      "stages.3.2.large_kernel.lkb_origin.bn.bias",
      "stages.3.2.norm.weight",
      "stages.3.2.norm.bias",
      "stages.3.2.pwconv1.bias",
      "stages.3.2.pwconv2.bias",
      "norm.weight",
      "norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
criterion = SoftTargetCrossEntropy()
dict_keys(['model', 'optimizer', 'epoch', 'scaler', 'args', 'model_ema'])
Resume checkpoint /gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T3_bnTrue_Teachercswin_STConvNext_NKD/checkpoint-best.pth
With optim & sched!
downsample_layers.0.0.weight density is 1.0
downsample_layers.0.0.bias density is 1.0
downsample_layers.0.1.weight density is 1.0
downsample_layers.0.1.bias density is 1.0
downsample_layers.1.0.weight density is 1.0
downsample_layers.1.0.bias density is 1.0
downsample_layers.1.1.weight density is 1.0
downsample_layers.1.1.bias density is 1.0
downsample_layers.2.0.weight density is 1.0
downsample_layers.2.0.bias density is 1.0
downsample_layers.2.1.weight density is 1.0
downsample_layers.2.1.bias density is 1.0
downsample_layers.3.0.weight density is 1.0
downsample_layers.3.0.bias density is 1.0
downsample_layers.3.1.weight density is 1.0
downsample_layers.3.1.bias density is 1.0
stages.0.0.gamma density is 1.0
stages.0.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.0.norm.weight density is 1.0
stages.0.0.norm.bias density is 1.0
stages.0.0.pwconv1.weight density is 1.0
stages.0.0.pwconv1.bias density is 1.0
stages.0.0.pwconv2.weight density is 1.0
stages.0.0.pwconv2.bias density is 1.0
stages.0.1.gamma density is 1.0
stages.0.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.1.norm.weight density is 1.0
stages.0.1.norm.bias density is 1.0
stages.0.1.pwconv1.weight density is 1.0
stages.0.1.pwconv1.bias density is 1.0
stages.0.1.pwconv2.weight density is 1.0
stages.0.1.pwconv2.bias density is 1.0
stages.0.2.gamma density is 1.0
stages.0.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.2.norm.weight density is 1.0
stages.0.2.norm.bias density is 1.0
stages.0.2.pwconv1.weight density is 1.0
stages.0.2.pwconv1.bias density is 1.0
stages.0.2.pwconv2.weight density is 1.0
stages.0.2.pwconv2.bias density is 1.0
stages.1.0.gamma density is 1.0
stages.1.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.0.norm.weight density is 1.0
stages.1.0.norm.bias density is 1.0
stages.1.0.pwconv1.weight density is 1.0
stages.1.0.pwconv1.bias density is 1.0
stages.1.0.pwconv2.weight density is 1.0
stages.1.0.pwconv2.bias density is 1.0
stages.1.1.gamma density is 1.0
stages.1.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.1.norm.weight density is 1.0
stages.1.1.norm.bias density is 1.0
stages.1.1.pwconv1.weight density is 1.0
stages.1.1.pwconv1.bias density is 1.0
stages.1.1.pwconv2.weight density is 1.0
stages.1.1.pwconv2.bias density is 1.0
stages.1.2.gamma density is 1.0
stages.1.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.2.norm.weight density is 1.0
stages.1.2.norm.bias density is 1.0
stages.1.2.pwconv1.weight density is 1.0
stages.1.2.pwconv1.bias density is 1.0
stages.1.2.pwconv2.weight density is 1.0
stages.1.2.pwconv2.bias density is 1.0
stages.2.0.gamma density is 1.0
stages.2.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.0.norm.weight density is 1.0
stages.2.0.norm.bias density is 1.0
stages.2.0.pwconv1.weight density is 1.0
stages.2.0.pwconv1.bias density is 1.0
stages.2.0.pwconv2.weight density is 1.0
stages.2.0.pwconv2.bias density is 1.0
stages.2.1.gamma density is 1.0
stages.2.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.1.norm.weight density is 1.0
stages.2.1.norm.bias density is 1.0
stages.2.1.pwconv1.weight density is 1.0
stages.2.1.pwconv1.bias density is 1.0
stages.2.1.pwconv2.weight density is 1.0
stages.2.1.pwconv2.bias density is 1.0
stages.2.2.gamma density is 1.0
stages.2.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.2.norm.weight density is 1.0
stages.2.2.norm.bias density is 1.0
stages.2.2.pwconv1.weight density is 1.0
stages.2.2.pwconv1.bias density is 1.0
stages.2.2.pwconv2.weight density is 1.0
stages.2.2.pwconv2.bias density is 1.0
stages.2.3.gamma density is 1.0
stages.2.3.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.3.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.3.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.3.norm.weight density is 1.0
stages.2.3.norm.bias density is 1.0
stages.2.3.pwconv1.weight density is 1.0
stages.2.3.pwconv1.bias density is 1.0
stages.2.3.pwconv2.weight density is 1.0
stages.2.3.pwconv2.bias density is 1.0
stages.2.4.gamma density is 1.0
stages.2.4.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.4.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.4.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.4.norm.weight density is 1.0
stages.2.4.norm.bias density is 1.0
stages.2.4.pwconv1.weight density is 1.0
stages.2.4.pwconv1.bias density is 1.0
stages.2.4.pwconv2.weight density is 1.0
stages.2.4.pwconv2.bias density is 1.0
stages.2.5.gamma density is 1.0
stages.2.5.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.5.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.5.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.5.norm.weight density is 1.0
stages.2.5.norm.bias density is 1.0
stages.2.5.pwconv1.weight density is 1.0
stages.2.5.pwconv1.bias density is 1.0
stages.2.5.pwconv2.weight density is 1.0
stages.2.5.pwconv2.bias density is 1.0
stages.2.6.gamma density is 1.0
stages.2.6.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.6.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.6.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.6.norm.weight density is 1.0
stages.2.6.norm.bias density is 1.0
stages.2.6.pwconv1.weight density is 1.0
stages.2.6.pwconv1.bias density is 1.0
stages.2.6.pwconv2.weight density is 1.0
stages.2.6.pwconv2.bias density is 1.0
stages.2.7.gamma density is 1.0
stages.2.7.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.7.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.7.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.7.norm.weight density is 1.0
stages.2.7.norm.bias density is 1.0
stages.2.7.pwconv1.weight density is 1.0
stages.2.7.pwconv1.bias density is 1.0
stages.2.7.pwconv2.weight density is 1.0
stages.2.7.pwconv2.bias density is 1.0
stages.2.8.gamma density is 1.0
stages.2.8.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.8.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.8.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.8.norm.weight density is 1.0
stages.2.8.norm.bias density is 1.0
stages.2.8.pwconv1.weight density is 1.0
stages.2.8.pwconv1.bias density is 1.0
stages.2.8.pwconv2.weight density is 1.0
stages.2.8.pwconv2.bias density is 1.0
stages.3.0.gamma density is 1.0
stages.3.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.0.norm.weight density is 1.0
stages.3.0.norm.bias density is 1.0
stages.3.0.pwconv1.weight density is 1.0
stages.3.0.pwconv1.bias density is 1.0
stages.3.0.pwconv2.weight density is 1.0
stages.3.0.pwconv2.bias density is 1.0
stages.3.1.gamma density is 1.0
stages.3.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.1.norm.weight density is 1.0
stages.3.1.norm.bias density is 1.0
stages.3.1.pwconv1.weight density is 1.0
stages.3.1.pwconv1.bias density is 1.0
stages.3.1.pwconv2.weight density is 1.0
stages.3.1.pwconv2.bias density is 1.0
stages.3.2.gamma density is 1.0
stages.3.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.2.norm.weight density is 1.0
stages.3.2.norm.bias density is 1.0
stages.3.2.pwconv1.weight density is 1.0
stages.3.2.pwconv1.bias density is 1.0
stages.3.2.pwconv2.weight density is 1.0
stages.3.2.pwconv2.bias density is 1.0
norm.weight density is 1.0
norm.bias density is 1.0
head.weight density is 1.0
head.bias density is 1.0
Test:  [ 0/79]  eta: 0:06:00  loss: 3.2230 (3.2230)  acc1: 36.4583 (36.4583)  acc5: 66.6667 (66.6667)  time: 4.5673  data: 2.1224  max mem: 2139
Test:  [10/79]  eta: 0:00:34  loss: 3.2230 (3.3035)  acc1: 28.1250 (28.5985)  acc5: 62.5000 (61.2689)  time: 0.4966  data: 0.1931  max mem: 2139
Test:  [20/79]  eta: 0:00:20  loss: 3.1822 (3.2859)  acc1: 28.1250 (28.4722)  acc5: 60.4167 (62.2520)  time: 0.1330  data: 0.0001  max mem: 2139
Test:  [30/79]  eta: 0:00:13  loss: 3.2807 (3.3310)  acc1: 28.1250 (27.6210)  acc5: 66.6667 (63.3401)  time: 0.1500  data: 0.0001  max mem: 2139
Test:  [40/79]  eta: 0:00:09  loss: 3.4668 (3.4668)  acc1: 21.8750 (25.7368)  acc5: 61.4583 (60.6961)  time: 0.1263  data: 0.0001  max mem: 2139
Test:  [50/79]  eta: 0:00:06  loss: 3.7430 (3.5122)  acc1: 22.9167 (25.2859)  acc5: 54.1667 (59.3546)  time: 0.1233  data: 0.0001  max mem: 2139
Test:  [60/79]  eta: 0:00:03  loss: 3.9767 (3.6252)  acc1: 17.7083 (23.6339)  acc5: 50.0000 (58.0260)  time: 0.1004  data: 0.0001  max mem: 2139
Test:  [70/79]  eta: 0:00:01  loss: 4.2946 (3.7237)  acc1: 13.5417 (22.0951)  acc5: 46.8750 (56.6168)  time: 0.0876  data: 0.0001  max mem: 2139
Test:  [78/79]  eta: 0:00:00  loss: 4.2946 (3.7764)  acc1: 12.5000 (21.1867)  acc5: 46.8750 (55.6000)  time: 0.0807  data: 0.0001  max mem: 2139
Test: Total time: 0:00:13 (0.1688 s / it)
* Acc@1 21.187 Acc@5 55.600 loss 3.776
Accuracy of the network on 7500 test images: 21.18667%
Not using distributed mode
Namespace(Decom=False, aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=64, bn=True, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/sliu/project_space/datasets/imagenet-a', data_set='IMNET', data_type='imagenet-a', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.1, enable_wandb=False, epochs=300, eval=True, eval_data_path=None, finetune='', fix=False, growth='random', head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, kernel_size=[7, 7, 7, 7, 100], layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=-1, log_dir=None, lr=0.004, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='SLaK_tiny', model_ema=False, model_ema_decay=0.9999, model_ema_eval=False, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_workers=10, only_L=False, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', parallel=True, pin_mem=True, project='SLaK', prune='magnitude', prune_rate=0.3, recount=1, redistribution='none', remode='pixel', reprob=0.25, resplit=False, resume='/gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_ConvNeXtTeacher_STconvnextv1_NKD/checkpoint-best.pth', save_ckpt=True, save_ckpt_freq=10, save_ckpt_num=3, seed=0, smoothing=0.1, sparse=False, sparse_init='snip', sparsity=0.4, start_epoch=0, train_interpolation='bicubic', update_freq=1, update_frequency=100, use_amp=False, verbose=False, wandb_ckpt=False, warmup_epochs=20, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, width_factor=1.0, world_size=1)
Transform = 
Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /home/sliu/project_space/datasets/imagenet-a
Number of the class = 1000
Mixup is activated!
Model = SLaK(
  (downsample_layers): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (1): LayerNorm()
    )
    (1): Sequential(
      (0): LayerNorm()
      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))
    )
    (2): Sequential(
      (0): LayerNorm()
      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))
    )
    (3): Sequential(
      (0): LayerNorm()
      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (stages): ModuleList(
    (0): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): Identity()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): DropPath()
      )
    )
    (1): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
    )
    (2): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (3): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (4): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (5): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (6): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (7): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (8): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
    )
    (3): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
number of params: 28595752
LR = 0.00400000
Batch size = 64
Update frequent = 1
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "downsample_layers.0.0.weight",
      "downsample_layers.1.1.weight",
      "downsample_layers.2.1.weight",
      "downsample_layers.3.1.weight",
      "stages.0.0.large_kernel.lkb_origin.conv.weight",
      "stages.0.0.pwconv1.weight",
      "stages.0.0.pwconv2.weight",
      "stages.0.1.large_kernel.lkb_origin.conv.weight",
      "stages.0.1.pwconv1.weight",
      "stages.0.1.pwconv2.weight",
      "stages.0.2.large_kernel.lkb_origin.conv.weight",
      "stages.0.2.pwconv1.weight",
      "stages.0.2.pwconv2.weight",
      "stages.1.0.large_kernel.lkb_origin.conv.weight",
      "stages.1.0.pwconv1.weight",
      "stages.1.0.pwconv2.weight",
      "stages.1.1.large_kernel.lkb_origin.conv.weight",
      "stages.1.1.pwconv1.weight",
      "stages.1.1.pwconv2.weight",
      "stages.1.2.large_kernel.lkb_origin.conv.weight",
      "stages.1.2.pwconv1.weight",
      "stages.1.2.pwconv2.weight",
      "stages.2.0.large_kernel.lkb_origin.conv.weight",
      "stages.2.0.pwconv1.weight",
      "stages.2.0.pwconv2.weight",
      "stages.2.1.large_kernel.lkb_origin.conv.weight",
      "stages.2.1.pwconv1.weight",
      "stages.2.1.pwconv2.weight",
      "stages.2.2.large_kernel.lkb_origin.conv.weight",
      "stages.2.2.pwconv1.weight",
      "stages.2.2.pwconv2.weight",
      "stages.2.3.large_kernel.lkb_origin.conv.weight",
      "stages.2.3.pwconv1.weight",
      "stages.2.3.pwconv2.weight",
      "stages.2.4.large_kernel.lkb_origin.conv.weight",
      "stages.2.4.pwconv1.weight",
      "stages.2.4.pwconv2.weight",
      "stages.2.5.large_kernel.lkb_origin.conv.weight",
      "stages.2.5.pwconv1.weight",
      "stages.2.5.pwconv2.weight",
      "stages.2.6.large_kernel.lkb_origin.conv.weight",
      "stages.2.6.pwconv1.weight",
      "stages.2.6.pwconv2.weight",
      "stages.2.7.large_kernel.lkb_origin.conv.weight",
      "stages.2.7.pwconv1.weight",
      "stages.2.7.pwconv2.weight",
      "stages.2.8.large_kernel.lkb_origin.conv.weight",
      "stages.2.8.pwconv1.weight",
      "stages.2.8.pwconv2.weight",
      "stages.3.0.large_kernel.lkb_origin.conv.weight",
      "stages.3.0.pwconv1.weight",
      "stages.3.0.pwconv2.weight",
      "stages.3.1.large_kernel.lkb_origin.conv.weight",
      "stages.3.1.pwconv1.weight",
      "stages.3.1.pwconv2.weight",
      "stages.3.2.large_kernel.lkb_origin.conv.weight",
      "stages.3.2.pwconv1.weight",
      "stages.3.2.pwconv2.weight",
      "head.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "downsample_layers.0.0.bias",
      "downsample_layers.0.1.weight",
      "downsample_layers.0.1.bias",
      "downsample_layers.1.0.weight",
      "downsample_layers.1.0.bias",
      "downsample_layers.1.1.bias",
      "downsample_layers.2.0.weight",
      "downsample_layers.2.0.bias",
      "downsample_layers.2.1.bias",
      "downsample_layers.3.0.weight",
      "downsample_layers.3.0.bias",
      "downsample_layers.3.1.bias",
      "stages.0.0.gamma",
      "stages.0.0.large_kernel.lkb_origin.bn.weight",
      "stages.0.0.large_kernel.lkb_origin.bn.bias",
      "stages.0.0.norm.weight",
      "stages.0.0.norm.bias",
      "stages.0.0.pwconv1.bias",
      "stages.0.0.pwconv2.bias",
      "stages.0.1.gamma",
      "stages.0.1.large_kernel.lkb_origin.bn.weight",
      "stages.0.1.large_kernel.lkb_origin.bn.bias",
      "stages.0.1.norm.weight",
      "stages.0.1.norm.bias",
      "stages.0.1.pwconv1.bias",
      "stages.0.1.pwconv2.bias",
      "stages.0.2.gamma",
      "stages.0.2.large_kernel.lkb_origin.bn.weight",
      "stages.0.2.large_kernel.lkb_origin.bn.bias",
      "stages.0.2.norm.weight",
      "stages.0.2.norm.bias",
      "stages.0.2.pwconv1.bias",
      "stages.0.2.pwconv2.bias",
      "stages.1.0.gamma",
      "stages.1.0.large_kernel.lkb_origin.bn.weight",
      "stages.1.0.large_kernel.lkb_origin.bn.bias",
      "stages.1.0.norm.weight",
      "stages.1.0.norm.bias",
      "stages.1.0.pwconv1.bias",
      "stages.1.0.pwconv2.bias",
      "stages.1.1.gamma",
      "stages.1.1.large_kernel.lkb_origin.bn.weight",
      "stages.1.1.large_kernel.lkb_origin.bn.bias",
      "stages.1.1.norm.weight",
      "stages.1.1.norm.bias",
      "stages.1.1.pwconv1.bias",
      "stages.1.1.pwconv2.bias",
      "stages.1.2.gamma",
      "stages.1.2.large_kernel.lkb_origin.bn.weight",
      "stages.1.2.large_kernel.lkb_origin.bn.bias",
      "stages.1.2.norm.weight",
      "stages.1.2.norm.bias",
      "stages.1.2.pwconv1.bias",
      "stages.1.2.pwconv2.bias",
      "stages.2.0.gamma",
      "stages.2.0.large_kernel.lkb_origin.bn.weight",
      "stages.2.0.large_kernel.lkb_origin.bn.bias",
      "stages.2.0.norm.weight",
      "stages.2.0.norm.bias",
      "stages.2.0.pwconv1.bias",
      "stages.2.0.pwconv2.bias",
      "stages.2.1.gamma",
      "stages.2.1.large_kernel.lkb_origin.bn.weight",
      "stages.2.1.large_kernel.lkb_origin.bn.bias",
      "stages.2.1.norm.weight",
      "stages.2.1.norm.bias",
      "stages.2.1.pwconv1.bias",
      "stages.2.1.pwconv2.bias",
      "stages.2.2.gamma",
      "stages.2.2.large_kernel.lkb_origin.bn.weight",
      "stages.2.2.large_kernel.lkb_origin.bn.bias",
      "stages.2.2.norm.weight",
      "stages.2.2.norm.bias",
      "stages.2.2.pwconv1.bias",
      "stages.2.2.pwconv2.bias",
      "stages.2.3.gamma",
      "stages.2.3.large_kernel.lkb_origin.bn.weight",
      "stages.2.3.large_kernel.lkb_origin.bn.bias",
      "stages.2.3.norm.weight",
      "stages.2.3.norm.bias",
      "stages.2.3.pwconv1.bias",
      "stages.2.3.pwconv2.bias",
      "stages.2.4.gamma",
      "stages.2.4.large_kernel.lkb_origin.bn.weight",
      "stages.2.4.large_kernel.lkb_origin.bn.bias",
      "stages.2.4.norm.weight",
      "stages.2.4.norm.bias",
      "stages.2.4.pwconv1.bias",
      "stages.2.4.pwconv2.bias",
      "stages.2.5.gamma",
      "stages.2.5.large_kernel.lkb_origin.bn.weight",
      "stages.2.5.large_kernel.lkb_origin.bn.bias",
      "stages.2.5.norm.weight",
      "stages.2.5.norm.bias",
      "stages.2.5.pwconv1.bias",
      "stages.2.5.pwconv2.bias",
      "stages.2.6.gamma",
      "stages.2.6.large_kernel.lkb_origin.bn.weight",
      "stages.2.6.large_kernel.lkb_origin.bn.bias",
      "stages.2.6.norm.weight",
      "stages.2.6.norm.bias",
      "stages.2.6.pwconv1.bias",
      "stages.2.6.pwconv2.bias",
      "stages.2.7.gamma",
      "stages.2.7.large_kernel.lkb_origin.bn.weight",
      "stages.2.7.large_kernel.lkb_origin.bn.bias",
      "stages.2.7.norm.weight",
      "stages.2.7.norm.bias",
      "stages.2.7.pwconv1.bias",
      "stages.2.7.pwconv2.bias",
      "stages.2.8.gamma",
      "stages.2.8.large_kernel.lkb_origin.bn.weight",
      "stages.2.8.large_kernel.lkb_origin.bn.bias",
      "stages.2.8.norm.weight",
      "stages.2.8.norm.bias",
      "stages.2.8.pwconv1.bias",
      "stages.2.8.pwconv2.bias",
      "stages.3.0.gamma",
      "stages.3.0.large_kernel.lkb_origin.bn.weight",
      "stages.3.0.large_kernel.lkb_origin.bn.bias",
      "stages.3.0.norm.weight",
      "stages.3.0.norm.bias",
      "stages.3.0.pwconv1.bias",
      "stages.3.0.pwconv2.bias",
      "stages.3.1.gamma",
      "stages.3.1.large_kernel.lkb_origin.bn.weight",
      "stages.3.1.large_kernel.lkb_origin.bn.bias",
      "stages.3.1.norm.weight",
      "stages.3.1.norm.bias",
      "stages.3.1.pwconv1.bias",
      "stages.3.1.pwconv2.bias",
      "stages.3.2.gamma",
      "stages.3.2.large_kernel.lkb_origin.bn.weight",
      "stages.3.2.large_kernel.lkb_origin.bn.bias",
      "stages.3.2.norm.weight",
      "stages.3.2.norm.bias",
      "stages.3.2.pwconv1.bias",
      "stages.3.2.pwconv2.bias",
      "norm.weight",
      "norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
criterion = SoftTargetCrossEntropy()
dict_keys(['model', 'optimizer', 'epoch', 'scaler', 'args', 'model_ema'])
Resume checkpoint /gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_ConvNeXtTeacher_STconvnextv1_NKD/checkpoint-best.pth
With optim & sched!
downsample_layers.0.0.weight density is 1.0
downsample_layers.0.0.bias density is 1.0
downsample_layers.0.1.weight density is 1.0
downsample_layers.0.1.bias density is 1.0
downsample_layers.1.0.weight density is 1.0
downsample_layers.1.0.bias density is 1.0
downsample_layers.1.1.weight density is 1.0
downsample_layers.1.1.bias density is 1.0
downsample_layers.2.0.weight density is 1.0
downsample_layers.2.0.bias density is 1.0
downsample_layers.2.1.weight density is 1.0
downsample_layers.2.1.bias density is 1.0
downsample_layers.3.0.weight density is 1.0
downsample_layers.3.0.bias density is 1.0
downsample_layers.3.1.weight density is 1.0
downsample_layers.3.1.bias density is 1.0
stages.0.0.gamma density is 1.0
stages.0.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.0.norm.weight density is 1.0
stages.0.0.norm.bias density is 1.0
stages.0.0.pwconv1.weight density is 1.0
stages.0.0.pwconv1.bias density is 1.0
stages.0.0.pwconv2.weight density is 1.0
stages.0.0.pwconv2.bias density is 1.0
stages.0.1.gamma density is 1.0
stages.0.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.1.norm.weight density is 1.0
stages.0.1.norm.bias density is 1.0
stages.0.1.pwconv1.weight density is 1.0
stages.0.1.pwconv1.bias density is 1.0
stages.0.1.pwconv2.weight density is 1.0
stages.0.1.pwconv2.bias density is 1.0
stages.0.2.gamma density is 1.0
stages.0.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.2.norm.weight density is 1.0
stages.0.2.norm.bias density is 1.0
stages.0.2.pwconv1.weight density is 1.0
stages.0.2.pwconv1.bias density is 1.0
stages.0.2.pwconv2.weight density is 1.0
stages.0.2.pwconv2.bias density is 1.0
stages.1.0.gamma density is 1.0
stages.1.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.0.norm.weight density is 1.0
stages.1.0.norm.bias density is 1.0
stages.1.0.pwconv1.weight density is 1.0
stages.1.0.pwconv1.bias density is 1.0
stages.1.0.pwconv2.weight density is 1.0
stages.1.0.pwconv2.bias density is 1.0
stages.1.1.gamma density is 1.0
stages.1.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.1.norm.weight density is 1.0
stages.1.1.norm.bias density is 1.0
stages.1.1.pwconv1.weight density is 1.0
stages.1.1.pwconv1.bias density is 1.0
stages.1.1.pwconv2.weight density is 1.0
stages.1.1.pwconv2.bias density is 1.0
stages.1.2.gamma density is 1.0
stages.1.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.2.norm.weight density is 1.0
stages.1.2.norm.bias density is 1.0
stages.1.2.pwconv1.weight density is 1.0
stages.1.2.pwconv1.bias density is 1.0
stages.1.2.pwconv2.weight density is 1.0
stages.1.2.pwconv2.bias density is 1.0
stages.2.0.gamma density is 1.0
stages.2.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.0.norm.weight density is 1.0
stages.2.0.norm.bias density is 1.0
stages.2.0.pwconv1.weight density is 1.0
stages.2.0.pwconv1.bias density is 1.0
stages.2.0.pwconv2.weight density is 1.0
stages.2.0.pwconv2.bias density is 1.0
stages.2.1.gamma density is 1.0
stages.2.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.1.norm.weight density is 1.0
stages.2.1.norm.bias density is 1.0
stages.2.1.pwconv1.weight density is 1.0
stages.2.1.pwconv1.bias density is 1.0
stages.2.1.pwconv2.weight density is 1.0
stages.2.1.pwconv2.bias density is 1.0
stages.2.2.gamma density is 1.0
stages.2.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.2.norm.weight density is 1.0
stages.2.2.norm.bias density is 1.0
stages.2.2.pwconv1.weight density is 1.0
stages.2.2.pwconv1.bias density is 1.0
stages.2.2.pwconv2.weight density is 1.0
stages.2.2.pwconv2.bias density is 1.0
stages.2.3.gamma density is 1.0
stages.2.3.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.3.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.3.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.3.norm.weight density is 1.0
stages.2.3.norm.bias density is 1.0
stages.2.3.pwconv1.weight density is 1.0
stages.2.3.pwconv1.bias density is 1.0
stages.2.3.pwconv2.weight density is 1.0
stages.2.3.pwconv2.bias density is 1.0
stages.2.4.gamma density is 1.0
stages.2.4.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.4.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.4.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.4.norm.weight density is 1.0
stages.2.4.norm.bias density is 1.0
stages.2.4.pwconv1.weight density is 1.0
stages.2.4.pwconv1.bias density is 1.0
stages.2.4.pwconv2.weight density is 1.0
stages.2.4.pwconv2.bias density is 1.0
stages.2.5.gamma density is 1.0
stages.2.5.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.5.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.5.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.5.norm.weight density is 1.0
stages.2.5.norm.bias density is 1.0
stages.2.5.pwconv1.weight density is 1.0
stages.2.5.pwconv1.bias density is 1.0
stages.2.5.pwconv2.weight density is 1.0
stages.2.5.pwconv2.bias density is 1.0
stages.2.6.gamma density is 1.0
stages.2.6.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.6.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.6.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.6.norm.weight density is 1.0
stages.2.6.norm.bias density is 1.0
stages.2.6.pwconv1.weight density is 1.0
stages.2.6.pwconv1.bias density is 1.0
stages.2.6.pwconv2.weight density is 1.0
stages.2.6.pwconv2.bias density is 1.0
stages.2.7.gamma density is 1.0
stages.2.7.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.7.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.7.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.7.norm.weight density is 1.0
stages.2.7.norm.bias density is 1.0
stages.2.7.pwconv1.weight density is 1.0
stages.2.7.pwconv1.bias density is 1.0
stages.2.7.pwconv2.weight density is 1.0
stages.2.7.pwconv2.bias density is 1.0
stages.2.8.gamma density is 1.0
stages.2.8.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.8.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.8.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.8.norm.weight density is 1.0
stages.2.8.norm.bias density is 1.0
stages.2.8.pwconv1.weight density is 1.0
stages.2.8.pwconv1.bias density is 1.0
stages.2.8.pwconv2.weight density is 1.0
stages.2.8.pwconv2.bias density is 1.0
stages.3.0.gamma density is 1.0
stages.3.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.0.norm.weight density is 1.0
stages.3.0.norm.bias density is 1.0
stages.3.0.pwconv1.weight density is 1.0
stages.3.0.pwconv1.bias density is 1.0
stages.3.0.pwconv2.weight density is 1.0
stages.3.0.pwconv2.bias density is 1.0
stages.3.1.gamma density is 1.0
stages.3.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.1.norm.weight density is 1.0
stages.3.1.norm.bias density is 1.0
stages.3.1.pwconv1.weight density is 1.0
stages.3.1.pwconv1.bias density is 1.0
stages.3.1.pwconv2.weight density is 1.0
stages.3.1.pwconv2.bias density is 1.0
stages.3.2.gamma density is 1.0
stages.3.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.2.norm.weight density is 1.0
stages.3.2.norm.bias density is 1.0
stages.3.2.pwconv1.weight density is 1.0
stages.3.2.pwconv1.bias density is 1.0
stages.3.2.pwconv2.weight density is 1.0
stages.3.2.pwconv2.bias density is 1.0
norm.weight density is 1.0
norm.bias density is 1.0
head.weight density is 1.0
head.bias density is 1.0
Test:  [ 0/79]  eta: 0:05:25  loss: 3.4314 (3.4314)  acc1: 25.0000 (25.0000)  acc5: 68.7500 (68.7500)  time: 4.1155  data: 1.9246  max mem: 2139
Test:  [10/79]  eta: 0:00:34  loss: 3.2085 (3.2149)  acc1: 27.0833 (28.3144)  acc5: 63.5417 (65.8144)  time: 0.5072  data: 0.1751  max mem: 2139
Test:  [20/79]  eta: 0:00:18  loss: 3.1429 (3.1842)  acc1: 28.1250 (29.0179)  acc5: 64.5833 (66.0714)  time: 0.1146  data: 0.0001  max mem: 2139
Test:  [30/79]  eta: 0:00:11  loss: 3.1765 (3.3016)  acc1: 27.0833 (27.0497)  acc5: 64.5833 (65.4570)  time: 0.0938  data: 0.0001  max mem: 2139
Test:  [40/79]  eta: 0:00:07  loss: 3.5775 (3.4555)  acc1: 21.8750 (25.2795)  acc5: 59.3750 (62.3984)  time: 0.0841  data: 0.0001  max mem: 2139
Test:  [50/79]  eta: 0:00:05  loss: 3.5775 (3.4795)  acc1: 23.9583 (25.2042)  acc5: 52.0833 (61.1111)  time: 0.0877  data: 0.0002  max mem: 2139
Test:  [60/79]  eta: 0:00:03  loss: 3.9151 (3.5977)  acc1: 19.7917 (23.6680)  acc5: 52.0833 (59.4775)  time: 0.1066  data: 0.0002  max mem: 2139
Test:  [70/79]  eta: 0:00:01  loss: 4.3060 (3.7006)  acc1: 13.5417 (22.0951)  acc5: 50.0000 (58.0546)  time: 0.1060  data: 0.0001  max mem: 2139
Test:  [78/79]  eta: 0:00:00  loss: 4.3498 (3.7629)  acc1: 12.5000 (21.0667)  acc5: 47.9167 (57.0267)  time: 0.1177  data: 0.0001  max mem: 2139
Test: Total time: 0:00:12 (0.1575 s / it)
* Acc@1 21.067 Acc@5 57.027 loss 3.763
Accuracy of the network on 7500 test images: 21.06667%
Not using distributed mode
Namespace(Decom=False, aa='rand-m9-mstd0.5-inc1', auto_resume=True, batch_size=64, bn=True, clip_grad=None, color_jitter=0.4, crop_pct=None, cutmix=1.0, cutmix_minmax=None, data_path='/home/sliu/project_space/datasets/imagenet-a', data_set='IMNET', data_type='imagenet-a', device='cuda', disable_eval=False, dist_eval=True, dist_on_itp=False, dist_url='env://', distributed=False, drop_path=0.1, enable_wandb=False, epochs=300, eval=True, eval_data_path=None, finetune='', fix=False, growth='random', head_init_scale=1.0, imagenet_default_mean_and_std=True, input_size=224, kernel_size=[7, 7, 7, 7, 100], layer_decay=1.0, layer_scale_init_value=1e-06, local_rank=-1, log_dir=None, lr=0.004, min_lr=1e-06, mixup=0.8, mixup_mode='batch', mixup_prob=1.0, mixup_switch_prob=0.5, model='SLaK_tiny', model_ema=False, model_ema_decay=0.9999, model_ema_eval=False, model_ema_force_cpu=False, model_key='model|module', model_prefix='', momentum=0.9, nb_classes=1000, num_workers=10, only_L=False, opt='adamw', opt_betas=None, opt_eps=1e-08, output_dir='', parallel=True, pin_mem=True, project='SLaK', prune='magnitude', prune_rate=0.3, recount=1, redistribution='none', remode='pixel', reprob=0.25, resplit=False, resume='/gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_SlakT_Teacher_STConvNext_NKD/checkpoint-best.pth', save_ckpt=True, save_ckpt_freq=10, save_ckpt_num=3, seed=0, smoothing=0.1, sparse=False, sparse_init='snip', sparsity=0.4, start_epoch=0, train_interpolation='bicubic', update_freq=1, update_frequency=100, use_amp=False, verbose=False, wandb_ckpt=False, warmup_epochs=20, warmup_steps=-1, weight_decay=0.05, weight_decay_end=None, width_factor=1.0, world_size=1)
Transform = 
Resize(size=256, interpolation=bicubic, max_size=None, antialias=None)
CenterCrop(size=(224, 224))
ToTensor()
Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))
---------------------------
reading from datapath /home/sliu/project_space/datasets/imagenet-a
Number of the class = 1000
Mixup is activated!
Model = SLaK(
  (downsample_layers): ModuleList(
    (0): Sequential(
      (0): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))
      (1): LayerNorm()
    )
    (1): Sequential(
      (0): LayerNorm()
      (1): Conv2d(96, 192, kernel_size=(2, 2), stride=(2, 2))
    )
    (2): Sequential(
      (0): LayerNorm()
      (1): Conv2d(192, 384, kernel_size=(2, 2), stride=(2, 2))
    )
    (3): Sequential(
      (0): LayerNorm()
      (1): Conv2d(384, 768, kernel_size=(2, 2), stride=(2, 2))
    )
  )
  (stages): ModuleList(
    (0): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): Identity()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(96, 96, kernel_size=(7, 7), stride=(1, 1), groups=96, bias=False)
            (bn): SyncBatchNorm(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=96, out_features=384, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=384, out_features=96, bias=True)
        (drop_path): DropPath()
      )
    )
    (1): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(192, 192, kernel_size=(7, 7), stride=(1, 1), groups=192, bias=False)
            (bn): SyncBatchNorm(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=192, out_features=768, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=768, out_features=192, bias=True)
        (drop_path): DropPath()
      )
    )
    (2): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (3): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (4): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (5): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (6): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (7): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
      (8): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(384, 384, kernel_size=(7, 7), stride=(1, 1), groups=384, bias=False)
            (bn): SyncBatchNorm(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=384, out_features=1536, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=1536, out_features=384, bias=True)
        (drop_path): DropPath()
      )
    )
    (3): Sequential(
      (0): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
      (1): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
      (2): Block(
        (large_kernel): ReparamLargeKernelConv(
          (lkb_origin): Sequential(
            (conv): DepthWiseConv2dImplicitGEMM(768, 768, kernel_size=(7, 7), stride=(1, 1), groups=768, bias=False)
            (bn): SyncBatchNorm(768, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
          )
        )
        (norm): LayerNorm()
        (pwconv1): Linear(in_features=768, out_features=3072, bias=True)
        (act): GELU()
        (pwconv2): Linear(in_features=3072, out_features=768, bias=True)
        (drop_path): DropPath()
      )
    )
  )
  (norm): LayerNorm((768,), eps=1e-06, elementwise_affine=True)
  (head): Linear(in_features=768, out_features=1000, bias=True)
)
number of params: 28595752
LR = 0.00400000
Batch size = 64
Update frequent = 1
Param groups = {
  "decay": {
    "weight_decay": 0.05,
    "params": [
      "downsample_layers.0.0.weight",
      "downsample_layers.1.1.weight",
      "downsample_layers.2.1.weight",
      "downsample_layers.3.1.weight",
      "stages.0.0.large_kernel.lkb_origin.conv.weight",
      "stages.0.0.pwconv1.weight",
      "stages.0.0.pwconv2.weight",
      "stages.0.1.large_kernel.lkb_origin.conv.weight",
      "stages.0.1.pwconv1.weight",
      "stages.0.1.pwconv2.weight",
      "stages.0.2.large_kernel.lkb_origin.conv.weight",
      "stages.0.2.pwconv1.weight",
      "stages.0.2.pwconv2.weight",
      "stages.1.0.large_kernel.lkb_origin.conv.weight",
      "stages.1.0.pwconv1.weight",
      "stages.1.0.pwconv2.weight",
      "stages.1.1.large_kernel.lkb_origin.conv.weight",
      "stages.1.1.pwconv1.weight",
      "stages.1.1.pwconv2.weight",
      "stages.1.2.large_kernel.lkb_origin.conv.weight",
      "stages.1.2.pwconv1.weight",
      "stages.1.2.pwconv2.weight",
      "stages.2.0.large_kernel.lkb_origin.conv.weight",
      "stages.2.0.pwconv1.weight",
      "stages.2.0.pwconv2.weight",
      "stages.2.1.large_kernel.lkb_origin.conv.weight",
      "stages.2.1.pwconv1.weight",
      "stages.2.1.pwconv2.weight",
      "stages.2.2.large_kernel.lkb_origin.conv.weight",
      "stages.2.2.pwconv1.weight",
      "stages.2.2.pwconv2.weight",
      "stages.2.3.large_kernel.lkb_origin.conv.weight",
      "stages.2.3.pwconv1.weight",
      "stages.2.3.pwconv2.weight",
      "stages.2.4.large_kernel.lkb_origin.conv.weight",
      "stages.2.4.pwconv1.weight",
      "stages.2.4.pwconv2.weight",
      "stages.2.5.large_kernel.lkb_origin.conv.weight",
      "stages.2.5.pwconv1.weight",
      "stages.2.5.pwconv2.weight",
      "stages.2.6.large_kernel.lkb_origin.conv.weight",
      "stages.2.6.pwconv1.weight",
      "stages.2.6.pwconv2.weight",
      "stages.2.7.large_kernel.lkb_origin.conv.weight",
      "stages.2.7.pwconv1.weight",
      "stages.2.7.pwconv2.weight",
      "stages.2.8.large_kernel.lkb_origin.conv.weight",
      "stages.2.8.pwconv1.weight",
      "stages.2.8.pwconv2.weight",
      "stages.3.0.large_kernel.lkb_origin.conv.weight",
      "stages.3.0.pwconv1.weight",
      "stages.3.0.pwconv2.weight",
      "stages.3.1.large_kernel.lkb_origin.conv.weight",
      "stages.3.1.pwconv1.weight",
      "stages.3.1.pwconv2.weight",
      "stages.3.2.large_kernel.lkb_origin.conv.weight",
      "stages.3.2.pwconv1.weight",
      "stages.3.2.pwconv2.weight",
      "head.weight"
    ],
    "lr_scale": 1.0
  },
  "no_decay": {
    "weight_decay": 0.0,
    "params": [
      "downsample_layers.0.0.bias",
      "downsample_layers.0.1.weight",
      "downsample_layers.0.1.bias",
      "downsample_layers.1.0.weight",
      "downsample_layers.1.0.bias",
      "downsample_layers.1.1.bias",
      "downsample_layers.2.0.weight",
      "downsample_layers.2.0.bias",
      "downsample_layers.2.1.bias",
      "downsample_layers.3.0.weight",
      "downsample_layers.3.0.bias",
      "downsample_layers.3.1.bias",
      "stages.0.0.gamma",
      "stages.0.0.large_kernel.lkb_origin.bn.weight",
      "stages.0.0.large_kernel.lkb_origin.bn.bias",
      "stages.0.0.norm.weight",
      "stages.0.0.norm.bias",
      "stages.0.0.pwconv1.bias",
      "stages.0.0.pwconv2.bias",
      "stages.0.1.gamma",
      "stages.0.1.large_kernel.lkb_origin.bn.weight",
      "stages.0.1.large_kernel.lkb_origin.bn.bias",
      "stages.0.1.norm.weight",
      "stages.0.1.norm.bias",
      "stages.0.1.pwconv1.bias",
      "stages.0.1.pwconv2.bias",
      "stages.0.2.gamma",
      "stages.0.2.large_kernel.lkb_origin.bn.weight",
      "stages.0.2.large_kernel.lkb_origin.bn.bias",
      "stages.0.2.norm.weight",
      "stages.0.2.norm.bias",
      "stages.0.2.pwconv1.bias",
      "stages.0.2.pwconv2.bias",
      "stages.1.0.gamma",
      "stages.1.0.large_kernel.lkb_origin.bn.weight",
      "stages.1.0.large_kernel.lkb_origin.bn.bias",
      "stages.1.0.norm.weight",
      "stages.1.0.norm.bias",
      "stages.1.0.pwconv1.bias",
      "stages.1.0.pwconv2.bias",
      "stages.1.1.gamma",
      "stages.1.1.large_kernel.lkb_origin.bn.weight",
      "stages.1.1.large_kernel.lkb_origin.bn.bias",
      "stages.1.1.norm.weight",
      "stages.1.1.norm.bias",
      "stages.1.1.pwconv1.bias",
      "stages.1.1.pwconv2.bias",
      "stages.1.2.gamma",
      "stages.1.2.large_kernel.lkb_origin.bn.weight",
      "stages.1.2.large_kernel.lkb_origin.bn.bias",
      "stages.1.2.norm.weight",
      "stages.1.2.norm.bias",
      "stages.1.2.pwconv1.bias",
      "stages.1.2.pwconv2.bias",
      "stages.2.0.gamma",
      "stages.2.0.large_kernel.lkb_origin.bn.weight",
      "stages.2.0.large_kernel.lkb_origin.bn.bias",
      "stages.2.0.norm.weight",
      "stages.2.0.norm.bias",
      "stages.2.0.pwconv1.bias",
      "stages.2.0.pwconv2.bias",
      "stages.2.1.gamma",
      "stages.2.1.large_kernel.lkb_origin.bn.weight",
      "stages.2.1.large_kernel.lkb_origin.bn.bias",
      "stages.2.1.norm.weight",
      "stages.2.1.norm.bias",
      "stages.2.1.pwconv1.bias",
      "stages.2.1.pwconv2.bias",
      "stages.2.2.gamma",
      "stages.2.2.large_kernel.lkb_origin.bn.weight",
      "stages.2.2.large_kernel.lkb_origin.bn.bias",
      "stages.2.2.norm.weight",
      "stages.2.2.norm.bias",
      "stages.2.2.pwconv1.bias",
      "stages.2.2.pwconv2.bias",
      "stages.2.3.gamma",
      "stages.2.3.large_kernel.lkb_origin.bn.weight",
      "stages.2.3.large_kernel.lkb_origin.bn.bias",
      "stages.2.3.norm.weight",
      "stages.2.3.norm.bias",
      "stages.2.3.pwconv1.bias",
      "stages.2.3.pwconv2.bias",
      "stages.2.4.gamma",
      "stages.2.4.large_kernel.lkb_origin.bn.weight",
      "stages.2.4.large_kernel.lkb_origin.bn.bias",
      "stages.2.4.norm.weight",
      "stages.2.4.norm.bias",
      "stages.2.4.pwconv1.bias",
      "stages.2.4.pwconv2.bias",
      "stages.2.5.gamma",
      "stages.2.5.large_kernel.lkb_origin.bn.weight",
      "stages.2.5.large_kernel.lkb_origin.bn.bias",
      "stages.2.5.norm.weight",
      "stages.2.5.norm.bias",
      "stages.2.5.pwconv1.bias",
      "stages.2.5.pwconv2.bias",
      "stages.2.6.gamma",
      "stages.2.6.large_kernel.lkb_origin.bn.weight",
      "stages.2.6.large_kernel.lkb_origin.bn.bias",
      "stages.2.6.norm.weight",
      "stages.2.6.norm.bias",
      "stages.2.6.pwconv1.bias",
      "stages.2.6.pwconv2.bias",
      "stages.2.7.gamma",
      "stages.2.7.large_kernel.lkb_origin.bn.weight",
      "stages.2.7.large_kernel.lkb_origin.bn.bias",
      "stages.2.7.norm.weight",
      "stages.2.7.norm.bias",
      "stages.2.7.pwconv1.bias",
      "stages.2.7.pwconv2.bias",
      "stages.2.8.gamma",
      "stages.2.8.large_kernel.lkb_origin.bn.weight",
      "stages.2.8.large_kernel.lkb_origin.bn.bias",
      "stages.2.8.norm.weight",
      "stages.2.8.norm.bias",
      "stages.2.8.pwconv1.bias",
      "stages.2.8.pwconv2.bias",
      "stages.3.0.gamma",
      "stages.3.0.large_kernel.lkb_origin.bn.weight",
      "stages.3.0.large_kernel.lkb_origin.bn.bias",
      "stages.3.0.norm.weight",
      "stages.3.0.norm.bias",
      "stages.3.0.pwconv1.bias",
      "stages.3.0.pwconv2.bias",
      "stages.3.1.gamma",
      "stages.3.1.large_kernel.lkb_origin.bn.weight",
      "stages.3.1.large_kernel.lkb_origin.bn.bias",
      "stages.3.1.norm.weight",
      "stages.3.1.norm.bias",
      "stages.3.1.pwconv1.bias",
      "stages.3.1.pwconv2.bias",
      "stages.3.2.gamma",
      "stages.3.2.large_kernel.lkb_origin.bn.weight",
      "stages.3.2.large_kernel.lkb_origin.bn.bias",
      "stages.3.2.norm.weight",
      "stages.3.2.norm.bias",
      "stages.3.2.pwconv1.bias",
      "stages.3.2.pwconv2.bias",
      "norm.weight",
      "norm.bias",
      "head.bias"
    ],
    "lr_scale": 1.0
  }
}
Use Cosine LR scheduler
criterion = SoftTargetCrossEntropy()
dict_keys(['model', 'optimizer', 'epoch', 'scaler', 'args', 'model_ema'])
Resume checkpoint /gpfs/work3/0/prjste21060/projects/datasets/Table_1_CKS/T20_bnTrue_SlakT_Teacher_STConvNext_NKD/checkpoint-best.pth
With optim & sched!
downsample_layers.0.0.weight density is 1.0
downsample_layers.0.0.bias density is 1.0
downsample_layers.0.1.weight density is 1.0
downsample_layers.0.1.bias density is 1.0
downsample_layers.1.0.weight density is 1.0
downsample_layers.1.0.bias density is 1.0
downsample_layers.1.1.weight density is 1.0
downsample_layers.1.1.bias density is 1.0
downsample_layers.2.0.weight density is 1.0
downsample_layers.2.0.bias density is 1.0
downsample_layers.2.1.weight density is 1.0
downsample_layers.2.1.bias density is 1.0
downsample_layers.3.0.weight density is 1.0
downsample_layers.3.0.bias density is 1.0
downsample_layers.3.1.weight density is 1.0
downsample_layers.3.1.bias density is 1.0
stages.0.0.gamma density is 1.0
stages.0.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.0.norm.weight density is 1.0
stages.0.0.norm.bias density is 1.0
stages.0.0.pwconv1.weight density is 1.0
stages.0.0.pwconv1.bias density is 1.0
stages.0.0.pwconv2.weight density is 1.0
stages.0.0.pwconv2.bias density is 1.0
stages.0.1.gamma density is 1.0
stages.0.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.1.norm.weight density is 1.0
stages.0.1.norm.bias density is 1.0
stages.0.1.pwconv1.weight density is 1.0
stages.0.1.pwconv1.bias density is 1.0
stages.0.1.pwconv2.weight density is 1.0
stages.0.1.pwconv2.bias density is 1.0
stages.0.2.gamma density is 1.0
stages.0.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.0.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.0.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.0.2.norm.weight density is 1.0
stages.0.2.norm.bias density is 1.0
stages.0.2.pwconv1.weight density is 1.0
stages.0.2.pwconv1.bias density is 1.0
stages.0.2.pwconv2.weight density is 1.0
stages.0.2.pwconv2.bias density is 1.0
stages.1.0.gamma density is 1.0
stages.1.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.0.norm.weight density is 1.0
stages.1.0.norm.bias density is 1.0
stages.1.0.pwconv1.weight density is 1.0
stages.1.0.pwconv1.bias density is 1.0
stages.1.0.pwconv2.weight density is 1.0
stages.1.0.pwconv2.bias density is 1.0
stages.1.1.gamma density is 1.0
stages.1.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.1.norm.weight density is 1.0
stages.1.1.norm.bias density is 1.0
stages.1.1.pwconv1.weight density is 1.0
stages.1.1.pwconv1.bias density is 1.0
stages.1.1.pwconv2.weight density is 1.0
stages.1.1.pwconv2.bias density is 1.0
stages.1.2.gamma density is 1.0
stages.1.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.1.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.1.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.1.2.norm.weight density is 1.0
stages.1.2.norm.bias density is 1.0
stages.1.2.pwconv1.weight density is 1.0
stages.1.2.pwconv1.bias density is 1.0
stages.1.2.pwconv2.weight density is 1.0
stages.1.2.pwconv2.bias density is 1.0
stages.2.0.gamma density is 1.0
stages.2.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.0.norm.weight density is 1.0
stages.2.0.norm.bias density is 1.0
stages.2.0.pwconv1.weight density is 1.0
stages.2.0.pwconv1.bias density is 1.0
stages.2.0.pwconv2.weight density is 1.0
stages.2.0.pwconv2.bias density is 1.0
stages.2.1.gamma density is 1.0
stages.2.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.1.norm.weight density is 1.0
stages.2.1.norm.bias density is 1.0
stages.2.1.pwconv1.weight density is 1.0
stages.2.1.pwconv1.bias density is 1.0
stages.2.1.pwconv2.weight density is 1.0
stages.2.1.pwconv2.bias density is 1.0
stages.2.2.gamma density is 1.0
stages.2.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.2.norm.weight density is 1.0
stages.2.2.norm.bias density is 1.0
stages.2.2.pwconv1.weight density is 1.0
stages.2.2.pwconv1.bias density is 1.0
stages.2.2.pwconv2.weight density is 1.0
stages.2.2.pwconv2.bias density is 1.0
stages.2.3.gamma density is 1.0
stages.2.3.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.3.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.3.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.3.norm.weight density is 1.0
stages.2.3.norm.bias density is 1.0
stages.2.3.pwconv1.weight density is 1.0
stages.2.3.pwconv1.bias density is 1.0
stages.2.3.pwconv2.weight density is 1.0
stages.2.3.pwconv2.bias density is 1.0
stages.2.4.gamma density is 1.0
stages.2.4.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.4.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.4.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.4.norm.weight density is 1.0
stages.2.4.norm.bias density is 1.0
stages.2.4.pwconv1.weight density is 1.0
stages.2.4.pwconv1.bias density is 1.0
stages.2.4.pwconv2.weight density is 1.0
stages.2.4.pwconv2.bias density is 1.0
stages.2.5.gamma density is 1.0
stages.2.5.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.5.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.5.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.5.norm.weight density is 1.0
stages.2.5.norm.bias density is 1.0
stages.2.5.pwconv1.weight density is 1.0
stages.2.5.pwconv1.bias density is 1.0
stages.2.5.pwconv2.weight density is 1.0
stages.2.5.pwconv2.bias density is 1.0
stages.2.6.gamma density is 1.0
stages.2.6.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.6.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.6.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.6.norm.weight density is 1.0
stages.2.6.norm.bias density is 1.0
stages.2.6.pwconv1.weight density is 1.0
stages.2.6.pwconv1.bias density is 1.0
stages.2.6.pwconv2.weight density is 1.0
stages.2.6.pwconv2.bias density is 1.0
stages.2.7.gamma density is 1.0
stages.2.7.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.7.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.7.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.7.norm.weight density is 1.0
stages.2.7.norm.bias density is 1.0
stages.2.7.pwconv1.weight density is 1.0
stages.2.7.pwconv1.bias density is 1.0
stages.2.7.pwconv2.weight density is 1.0
stages.2.7.pwconv2.bias density is 1.0
stages.2.8.gamma density is 1.0
stages.2.8.large_kernel.lkb_origin.conv.weight density is 1.0
stages.2.8.large_kernel.lkb_origin.bn.weight density is 1.0
stages.2.8.large_kernel.lkb_origin.bn.bias density is 1.0
stages.2.8.norm.weight density is 1.0
stages.2.8.norm.bias density is 1.0
stages.2.8.pwconv1.weight density is 1.0
stages.2.8.pwconv1.bias density is 1.0
stages.2.8.pwconv2.weight density is 1.0
stages.2.8.pwconv2.bias density is 1.0
stages.3.0.gamma density is 1.0
stages.3.0.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.0.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.0.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.0.norm.weight density is 1.0
stages.3.0.norm.bias density is 1.0
stages.3.0.pwconv1.weight density is 1.0
stages.3.0.pwconv1.bias density is 1.0
stages.3.0.pwconv2.weight density is 1.0
stages.3.0.pwconv2.bias density is 1.0
stages.3.1.gamma density is 1.0
stages.3.1.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.1.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.1.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.1.norm.weight density is 1.0
stages.3.1.norm.bias density is 1.0
stages.3.1.pwconv1.weight density is 1.0
stages.3.1.pwconv1.bias density is 1.0
stages.3.1.pwconv2.weight density is 1.0
stages.3.1.pwconv2.bias density is 1.0
stages.3.2.gamma density is 1.0
stages.3.2.large_kernel.lkb_origin.conv.weight density is 1.0
stages.3.2.large_kernel.lkb_origin.bn.weight density is 1.0
stages.3.2.large_kernel.lkb_origin.bn.bias density is 1.0
stages.3.2.norm.weight density is 1.0
stages.3.2.norm.bias density is 1.0
stages.3.2.pwconv1.weight density is 1.0
stages.3.2.pwconv1.bias density is 1.0
stages.3.2.pwconv2.weight density is 1.0
stages.3.2.pwconv2.bias density is 1.0
norm.weight density is 1.0
norm.bias density is 1.0
head.weight density is 1.0
head.bias density is 1.0
Test:  [ 0/79]  eta: 0:04:48  loss: 3.0464 (3.0464)  acc1: 34.3750 (34.3750)  acc5: 69.7917 (69.7917)  time: 3.6576  data: 1.5594  max mem: 2139
Test:  [10/79]  eta: 0:00:27  loss: 3.1066 (3.1616)  acc1: 31.2500 (30.3030)  acc5: 65.6250 (65.0568)  time: 0.3998  data: 0.1419  max mem: 2139
Test:  [20/79]  eta: 0:00:15  loss: 3.0550 (3.1292)  acc1: 33.3333 (32.1925)  acc5: 65.6250 (66.0218)  time: 0.0914  data: 0.0005  max mem: 2139
Test:  [30/79]  eta: 0:00:09  loss: 3.1462 (3.1649)  acc1: 32.2917 (31.4180)  acc5: 69.7917 (67.5739)  time: 0.0883  data: 0.0005  max mem: 2139
Test:  [40/79]  eta: 0:00:06  loss: 3.4204 (3.3361)  acc1: 25.0000 (28.8872)  acc5: 65.6250 (64.2276)  time: 0.0821  data: 0.0001  max mem: 2139
Test:  [50/79]  eta: 0:00:04  loss: 3.6004 (3.3674)  acc1: 26.0417 (28.6969)  acc5: 54.1667 (62.9698)  time: 0.0941  data: 0.0001  max mem: 2139
Test:  [60/79]  eta: 0:00:02  loss: 3.7303 (3.4817)  acc1: 25.0000 (26.9126)  acc5: 54.1667 (61.4242)  time: 0.0866  data: 0.0001  max mem: 2139
Test:  [70/79]  eta: 0:00:01  loss: 4.1363 (3.5854)  acc1: 15.6250 (25.3081)  acc5: 50.0000 (59.7858)  time: 0.1112  data: 0.0001  max mem: 2139
Test:  [78/79]  eta: 0:00:00  loss: 4.1395 (3.6390)  acc1: 14.5833 (24.2667)  acc5: 47.9167 (58.8667)  time: 0.1405  data: 0.0001  max mem: 2139
Test: Total time: 0:00:11 (0.1460 s / it)
* Acc@1 24.267 Acc@5 58.867 loss 3.639
Accuracy of the network on 7500 test images: 24.26667%
SLak_kernel_distill_A.sh: line 162: source: deactivate: file not found
